<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>容器开发 on 橡果笔记</title>
    <link>http://localhost:1313/kubernetes/index.html</link>
    <description>Recent content in 容器开发 on 橡果笔记</description>
    <generator>Hugo</generator>
    <language>en</language>
    <atom:link href="http://localhost:1313/kubernetes/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title></title>
      <link>http://localhost:1313/kubernetes/docker-auto-test-product.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/kubernetes/docker-auto-test-product.html</guid>
      <description>&lt;h1 id=&#34;产品自动化测试&#34;&gt;产品自动化测试&lt;a class=&#34;anchor&#34; href=&#34;#%e4%ba%a7%e5%93%81%e8%87%aa%e5%8a%a8%e5%8c%96%e6%b5%8b%e8%af%95&#34;&gt;#&lt;/a&gt;&lt;/h1&gt;&#xA;&lt;p&gt;目前公司正在开发大数据平台，在开发过程中，每当发布了新版本时，开发组通知测试组，然后测试组从 Hudson （一个持续集成工具）下载所有的 rpm 包，在测试机上进行安装部署，然后跑测试用例，出测试结果，恢复系统环境以供再次安装测试。这个过程中显然有很多重复性工作，因此，为了缩短产品迭代周期，节省劳动力，我们需要一套自动化测试方案。 本文就笔者目前条件和环境探讨一个可行的方案，不作为具体实现，触类旁通，具体实现还请参考实际情况。&lt;/p&gt;&#xA;&lt;h2 id=&#34;基本思路&#34;&gt;基本思路&lt;a class=&#34;anchor&#34; href=&#34;#%e5%9f%ba%e6%9c%ac%e6%80%9d%e8%b7%af&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;首先，我们需要一个 Linux 集群，这里我们就用 Docker ，这样我们可以快速的创建一个 Linux 集群出来，并且很好的隔离系统环境，不用每次安装完再恢复系统环境。&lt;/li&gt;&#xA;&lt;li&gt;其次是大数据集群的部署，其中每个组件都有那么多的配置项，怎么办？？笔者目前所开发的大数据平台是用 Ambari 来作为集群管理工具，Ambari 是一个主从架构的分布式管理工具，主要有 Seriver 和 Agent 两个服务组成，它的功能包括有集群的创建 、组件的安装与卸载、监控集群状态等等，它还提供了一个功能叫 Ambari Blueprint ，这个功能可以帮我们免去一切人工操作与各组件的配置，并让我们可以更容易地实现将大数据集群的部署与配置脚本化。在这里我们就用它来创建集群。 关于更多 Ambari Blueprint 介绍和使用方法请看本文尾部提供的链接。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;实现步骤&#34;&gt;实现步骤&lt;a class=&#34;anchor&#34; href=&#34;#%e5%ae%9e%e7%8e%b0%e6%ad%a5%e9%aa%a4&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;即然要部署大数据集群，那么安装包从何而来？？我们的大数据平台是用 Hudson 来实现持续集成的，而实现自动化测试的环境是另一台服务器，所以需要远程检测 Hudson 上发布的新版本并下载，这里可以用爬虫方式来检测指定页面上的内容，如果发新有的版本就下载即可。也可以在 Hudson 服务器上写一段小程序，做为一个触发点，每当有版本编译出来就远程通知给测试机，让测试机开始下载。&lt;/li&gt;&#xA;&lt;li&gt;测试机一旦下载成功，便开始创建 Docker 容器，这里我们把容器当作虚拟机来用，所以打算搭建什么规模的集群就创建几个容器，本次我们打算搭建5个节点的集群，所以创建5个容器出来，这里需要注意的是，用来创建容器的镜像最好提前配置好，比如说大数据平台依赖的一些系统配置等等。&lt;/li&gt;&#xA;&lt;li&gt;部署 Ambari，我们刚才创建了一共5个容器，在第一个容器内安装 Ambari-Server 与 Ambari-Agent，其它四个容器只安装 Ambari-Agent 就可以了&lt;/li&gt;&#xA;&lt;li&gt;用 Ambari Blueprint 安装各个组件（如：Haoop、Spark、Hive 等），这里会用到两个 JSON 文件，这两个文件是重中之重，这两个文件作用如下：&#xA;&lt;img src=&#34;img/docker-auto-test-product/docker-auto-test-product_blueprint.jpg&#34; alt=&#34;&#34; /&gt;&#xA;上图中左边是两个 JSON 文件，它们通 http 请求发送给 Ambari-Server 端，然后 Ambari-Server 会根据 JSON 信息创建出一个集群来。&#xA;那么 JSON 从哪来呢，最简单的方法是从一个已安装好的 Ambari 集群导出，像下面这条命令：&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;curl -H &amp;#34;X-Requested-By: ambari&amp;#34; -u admin:admin -X GET \ &#xA;http://$ambari_server_ip:8080/api/v1/clusters/$cluster_name?format=blueprint &amp;gt; blueprint_document.json&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这样相当于把当前的集群状态记录下来，而第二个 JSON 文件需要自己定义。下面是两个 JSON 文件中部分示例：&#xA;&lt;img src=&#34;img/docker-auto-test-product/docker-auto-test-product_json-768x456.png&#34; alt=&#34;&#34; /&gt;&#xA;下面开始创建集群，将上面第一个 JSON 文件发送给 Ambari—Server 并注册一个 Blueprint：&lt;/p&gt;</description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:1313/kubernetes/docker-bigdata-develope.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/kubernetes/docker-bigdata-develope.html</guid>
      <description>&lt;h1 id=&#34;与大数据平台开发&#34;&gt;与大数据平台开发&lt;a class=&#34;anchor&#34; href=&#34;#%e4%b8%8e%e5%a4%a7%e6%95%b0%e6%8d%ae%e5%b9%b3%e5%8f%b0%e5%bc%80%e5%8f%91&#34;&gt;#&lt;/a&gt;&lt;/h1&gt;&#xA;&lt;h2 id=&#34;发现问题&#34;&gt;发现问题&lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%91%e7%8e%b0%e9%97%ae%e9%a2%98&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;场景一 在大数据平台的开发过程中，开发人员通常需要自己有一套集群，以便反复测试自己负责的模块，难道要给每人都配几台机器？ 场景二 测试组需要反复的安装整个平台以便发现问题，而一旦安装就很难再让 Linux 系统恢复到一个干净的状态，或者说需要花费很多时间，那如何快速地恢复系统环境？ 场景三 测试人员在测试中发现了一个 Bug，需要保存现场，可测试还要继续，怎么办？ 场景四 如何把一个部署好的大数据平台快速地迁移到其它地方？&lt;/p&gt;&#xA;&lt;h2 id=&#34;传统解决方案&#34;&gt;传统解决方案&lt;a class=&#34;anchor&#34; href=&#34;#%e4%bc%a0%e7%bb%9f%e8%a7%a3%e5%86%b3%e6%96%b9%e6%a1%88&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;解决这个问题第一个想到的当然是用虚拟机了，而之前也确实用的是虚拟机，但这种方式不能完美的解决以上问题，比如：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;虽然它也可以迁移，但这并不是它所擅长的，不够灵活，很笨重&lt;/li&gt;&#xA;&lt;li&gt;虚拟机的快照可以保存当前的状态，但要恢复回去就得把当前正在运行的虚拟机关闭，并不适合频繁保存当前状态&lt;/li&gt;&#xA;&lt;li&gt;虽然可以给每个人都分配几个虚拟机用，但它是一个完整的系统，本身需要较多的资源，底层物理机的资源很快就被用完了，我们需要寻找其它方式来弥补这些不足&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;docker-技术的引入&#34;&gt;Docker 技术的引入&lt;a class=&#34;anchor&#34; href=&#34;#docker-%e6%8a%80%e6%9c%af%e7%9a%84%e5%bc%95%e5%85%a5&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;Docker 项目的目标是实现轻量级的操作系统虚拟化解决方案，换句话说，它可以让我们把一台物理机虚拟成多台来使用，而且它还可以保修改、完整的迁移到其它地方、性能损耗小等等好处，可以说很好解决了我们的问题。 那为什么不用虚拟机？ 因为它比虚拟机更轻便，Docker容器中不包含操作系统，启动一个Docker容器只要几秒种的时间，在一台物理机上可以创建几百上千个容器，而虚拟做不到。 下面是 Docker 与虚拟机的实现原理图&lt;/p&gt;&#xA;&lt;h3 id=&#34;docker-设计图&#34;&gt;Docker 设计图&lt;a class=&#34;anchor&#34; href=&#34;#docker-%e8%ae%be%e8%ae%a1%e5%9b%be&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;img src=&#34;img/docker-bigdata-develope/docker-bigdata-develope_darch-300x244.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;vm-设计图&#34;&gt;VM 设计图&lt;a class=&#34;anchor&#34; href=&#34;#vm-%e8%ae%be%e8%ae%a1%e5%9b%be&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;img src=&#34;img/docker-bigdata-develope/docker-bigdata-develope_vmarch-272x300.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;&lt;p&gt;可见容器是在操作系统层面上实现虚拟化，直接复用本地主机的操作系统，而传统方式则是在硬件层面实现。&lt;/p&gt;&#xA;&lt;h2 id=&#34;环境的搭建&#34;&gt;环境的搭建&lt;a class=&#34;anchor&#34; href=&#34;#%e7%8e%af%e5%a2%83%e7%9a%84%e6%90%ad%e5%bb%ba&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;在实践过程中，部署一套可用的 Docker 环境，必需做好以下前提工作：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;搭建私有镜像仓库，用来统一存放构建好的镜像&lt;/li&gt;&#xA;&lt;li&gt;搭建一个安装包仓库，用来存放我们发布的各种版本的大数据安装包等&lt;/li&gt;&#xA;&lt;li&gt;使多个物理机上的 Dcoker 容器可以相互通信，官方已存给出了方案&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;为平台定制基础镜像&#34;&gt;为平台定制基础镜像&lt;a class=&#34;anchor&#34; href=&#34;#%e4%b8%ba%e5%b9%b3%e5%8f%b0%e5%ae%9a%e5%88%b6%e5%9f%ba%e7%a1%80%e9%95%9c%e5%83%8f&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;即然要在Docke容器内安装我们的平台，那就需要一个统一的 Linux 系统做为我们的 Dcoker 容器，比如 Ubuntu、CentOS 等发行商都会发布自己的Docker基础镜像到 Docker Hub 上，如果 Docker Hub 上恰好没有你需要的镜像，也可以自己制作。&lt;/li&gt;&#xA;&lt;li&gt;比如用 CentOS 做为我们的基础镜像，那么先把它 pull 下来&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[user@host1 ~]$ docker pull centos:6.8 &#xA;Using default tag: latest&#xA;latest: Pulling from ubuntu&#xA;8aec416115fd: Extracting [================&amp;gt;                          ] 16.78 MB/50.31 MB&#xA;695f074e24e3: Download complete &#xA;946d6c48c2a7: Download complete &#xA;bc7277e579f0: Verifying Checksum &#xA;...&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;3&#34;&gt;&#xA;&lt;li&gt;然后我们用这个镜像创建一个容器，并在里面配一些我们的大数据平台依赖的参数，比如 ntpd、httpd 服务等等，最终生成我们平台专属的基础镜像。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[user@host1 ~]$ docker run -tid --name build -h build centos:6.8 bash      # 创建一个容器&#xA;f5e71cd81252a3563a03ad8daee81047b62ab5d892ebbfbf71cf53415f29c130&#xA;[user@host1 ~]$ docker exec -ti build bash&#xA;[root@build ~]$ yum install ntpd      # 定制自已的配置&#xA;...&#xA;[root@build ~]$ exit&#xA;[user@host1 ~]$ docker commit build my-centos      # 保存为自已的镜像&#xA;ha256:71cd81252a3563a03ad8daee81047b62ab5d892ebbfbf71cf53415f29c130950&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;4&#34;&gt;&#xA;&lt;li&gt;这是很关键的一步，有了它以后，所有人员可以随时创建一个自己需要的Linux环境出来，以便在其内进行产品的研究和实验，且每个人的环境互不相干，当容器内的环境被破坏后，可以删掉再创建，这样一来，场景一和二所遇到的问题也就迎刃面解。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;将已经部署好的集群做成镜像&#34;&gt;将已经部署好的集群做成镜像&lt;a class=&#34;anchor&#34; href=&#34;#%e5%b0%86%e5%b7%b2%e7%bb%8f%e9%83%a8%e7%bd%b2%e5%a5%bd%e7%9a%84%e9%9b%86%e7%be%a4%e5%81%9a%e6%88%90%e9%95%9c%e5%83%8f&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;我们可以把已经部署了集群的容器保存成多种镜像，如：只包含了 Hadoop 的集群、同时包含 Hadoop、Zookeeper、Hbase 的集群，或安装了所有组件的集群等等，然后上传到私有仓库，其它人需要的时候，直接启动自已需要的集群就可以了，因为免去了部暑与配置等步骤，因而大幅度提高了工作效率，也提高了产品迭代速度。&lt;/p&gt;</description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:1313/kubernetes/docker-build-centos65.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/kubernetes/docker-build-centos65.html</guid>
      <description>&lt;h1 id=&#34;手工构建基础镜像&#34;&gt;手工构建基础镜像&lt;a class=&#34;anchor&#34; href=&#34;#%e6%89%8b%e5%b7%a5%e6%9e%84%e5%bb%ba%e5%9f%ba%e7%a1%80%e9%95%9c%e5%83%8f&#34;&gt;#&lt;/a&gt;&lt;/h1&gt;&#xA;&lt;p&gt;前段时间，老大说要把目前正在开发的产品放在Docker容器中跑，而且容器的系统版本必须跟开发一致（CentOS6.5），然后我跑去Docker Hub上找一圈，发现Centos官方并没有提供这个版本！纳尼？？开什么玩笑？好吧，然后又搜了一圈，找到了下面这个链接，解释了为什么没有CentOS6.5： &lt;a href=&#34;https://github.com/CentOS/sig-cloud-instance-images/issues/13&#34;&gt;https://github.com/CentOS/sig-cloud-instance-images/issues/13&lt;/a&gt; ，大意是说，官方觉得没必要用CentOS6.5，因为有更高的版本能可以用，并且修复了低版本中的Bug，如果他们提供了6.5的版本，那可能还需要提供6.4，6.3等等，所以直接就不提供低版本了。&lt;/p&gt;&#xA;&lt;p&gt;好吧，但最终要用哪个版本不是我说了算的，然后上Docker官网看看怎么手动构建自已的基础镜像，不幸的是，它虽提供了构建CentOS的基础镜像，但并不能构建出6.5版本的，于是乎又是一顿google，最终还是找到一个比较靠谱的文章，英文，且简略，地址我写在本文最后的部分，下面是我结合自已的需求构建镜像的全部过程。&lt;/p&gt;&#xA;&lt;h2 id=&#34;需要的环境&#34;&gt;需要的环境&lt;a class=&#34;anchor&#34; href=&#34;#%e9%9c%80%e8%a6%81%e7%9a%84%e7%8e%af%e5%a2%83&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;一个正常运行的CentOS6.5系统，这里我用的是我们产品的开发环境系统&lt;/li&gt;&#xA;&lt;li&gt;可以联外网，因为要下载东西3. 一个CentOS6.5的yum源，这里我用的是一个ISO文件直接mount到本地就可以用了&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;开始构建&#34;&gt;开始构建&lt;a class=&#34;anchor&#34; href=&#34;#%e5%bc%80%e5%a7%8b%e6%9e%84%e5%bb%ba&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;安装febootstrap与xz&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;yum install -y febootstrap xz&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;挂载自己的repo镜像，以下是一个例子：&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;mount -t iso9660 -o loop /data/CentOS-6.5-x86_64-DVD.iso /data/iso&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;另注：如果你没有这样的iso文件，可以去CentOS官方下载一个，下载这个就行：&lt;code&gt;CentOS-6.5-x86_64-minimal.iso&lt;/code&gt;，然后按以上方式挂载。 当然，如果你有自已的yum源服务器也可以，那它应该长这个样子：&lt;code&gt;http://your.host.com/repodir&lt;/code&gt;，或者这样子：&lt;code&gt;file:///var/www/html/repodir&lt;/code&gt;。&#xA;开始生成镜像文件夹&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;febootstrap -i bash -i coreutils -i tar -ai bzip2 -i gzip \&#xA;-i vim-minimal -i wget -i patch -i diffutils -i iproute \&#xA;-i yum centos6.5 centos6.5-base-image file:///data/iso&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;说明：&#xA;&lt;code&gt;file:///data/iso&lt;/code&gt;：自己的repo源&#xA;&lt;code&gt;centos6.5&lt;/code&gt;：镜像版本&#xA;&lt;code&gt;centos6.5-base-image&lt;/code&gt;：文件夹名字&lt;/p&gt;&#xA;&lt;p&gt;在文件夹内创建三个文件&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;touch centos6.5-base-ks/etc/resolv.conf touch centos6.5-base-ks/sbin/init echo -e &amp;#39;NETWORKING=yes\nHOSTNAME=build&amp;#39; /etc/sysconfig/network&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;打包文件夹&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;tar --numeric-owner -Jcpf centos6.5-base.tar.xz -C centos6.5-base-image . &lt;/code&gt;&lt;/pre&gt;&lt;p&gt;把打包好的文件发送到Docker环境下，并导入到Docker&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;scp centos6.5-base.tar.xz docker.host.com:/root&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;然后去对应的机器上（docker.host.com），把包导入到Docker，使之成为一个Docker镜像&lt;/p&gt;</description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:1313/kubernetes/docker-code-create-container.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/kubernetes/docker-code-create-container.html</guid>
      <description>&lt;h1 id=&#34;docker源码-创建容器&#34;&gt;DOCKER源码-创建容器&lt;a class=&#34;anchor&#34; href=&#34;#docker%e6%ba%90%e7%a0%81-%e5%88%9b%e5%bb%ba%e5%ae%b9%e5%99%a8&#34;&gt;#&lt;/a&gt;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;daemon结构体&#34;&gt;Daemon结构体&lt;a class=&#34;anchor&#34; href=&#34;#daemon%e7%bb%93%e6%9e%84%e4%bd%93&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;code&gt;daemon/daemon.go&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;type Daemon struct {&#xA;    // ... 省略多行 ...&#xA;    containerdCli         *containerd.Client&#xA;    containerd            libcontainerdtypes.Client&#xA;    volumes           *volumesservice.VolumesService&#xA;    // ... 省略多行 ...&#xA;}&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;client结构体&#34;&gt;client结构体&lt;a class=&#34;anchor&#34; href=&#34;#client%e7%bb%93%e6%9e%84%e4%bd%93&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;p&gt;Daemon中的containerd字段负责容器相关操作，Daemon对象所有对容器的操作都是通过调用containerd对象相应函数来完成的，它是一个接口，相应的实现定义在&lt;code&gt;libcontainerd/remote/client.go&lt;/code&gt;文件中，它具有以下函数（只列出部分）：&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;func (c *client) Version(ctx context.Context) (containerd.Version, error)&#xA;&#xA;func (c *client) Restore(ctx context.Context, id string, attachStdio libcontainerdtypes.StdioCallback) (alive bool, pid int, p libcontainerdtypes.Process, err error)&#xA;&#xA;func (c *client) Create(ctx context.Context, id string, ociSpec *specs.Spec, runtimeOptions interface{}, opts ...containerd.NewContainerOpts) error&#xA;&#xA;func (c *client) Start(ctx context.Context, id, checkpointDir string, withStdin bool, attachStdio libcontainerdtypes.StdioCallback) (int, error)&#xA;&#xA;func (c *client) Exec(ctx context.Context, containerID, processID string, spec *specs.Process, withStdin bool, attachStdio libcontainerdtypes.StdioCallback) (int, error)&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;client结构体-1&#34;&gt;Client结构体&lt;a class=&#34;anchor&#34; href=&#34;#client%e7%bb%93%e6%9e%84%e4%bd%93-1&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;p&gt;client结构体是一个较高层的封装，它在实例化的时候依赖Client结构体，而Client结构体实际是一个RPC客户端，所有容器操作都是通过RPC调用containerd进程，关键代码：&#xA;&lt;code&gt;vendor/github.com/containerd/containerd/api/services/containers/v1/containers.pb.go&lt;/code&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:1313/kubernetes/docker-code-create-network.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/kubernetes/docker-code-create-network.html</guid>
      <description>&lt;h1 id=&#34;docker源码-创建网络&#34;&gt;DOCKER源码-创建网络&lt;a class=&#34;anchor&#34; href=&#34;#docker%e6%ba%90%e7%a0%81-%e5%88%9b%e5%bb%ba%e7%bd%91%e7%bb%9c&#34;&gt;#&lt;/a&gt;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;daemon结构体&#34;&gt;Daemon结构体&lt;a class=&#34;anchor&#34; href=&#34;#daemon%e7%bb%93%e6%9e%84%e4%bd%93&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;code&gt;daemon/daemon.go&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;type Daemon struct {&#xA;    // ... 省略多行 ...&#xA;    EventsService     *events.Events&#xA;    netController     libnetwork.NetworkController&#xA;    volumes           *volumesservice.VolumesService&#xA;    discoveryWatcher  discovery.Reloader&#xA;    // ... 省略多行 ...&#xA;}&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Daemon中的netController字段负责网络相关操作，它是一个接口，其接口的定义和实现都在&lt;code&gt;vendor/github.com/docker/libnetwork/controller.go&lt;/code&gt;文件中。&lt;/p&gt;&#xA;&lt;h3 id=&#34;controller结构体&#34;&gt;controller结构体&lt;a class=&#34;anchor&#34; href=&#34;#controller%e7%bb%93%e6%9e%84%e4%bd%93&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;p&gt;controller结构体实现了libnetwork.NetworkController接口，Daemon对象的对网络设备的增删改查都是调用controller对象的相关函数，其中创建网络的关键函数如下：&#xA;&lt;code&gt;vendor/github.com/docker/libnetwork/controller.go&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;func (c *controller) NewNetwork(networkType, name string, id string, options ...NetworkOption) (Network, error) {&#xA;    // ... 省略多行 ...&#xA;    network := &amp;amp;network{&#xA;        name:             name,&#xA;        networkType:      networkType,&#xA;        generic:          map[string]interface{}{netlabel.GenericData: make(map[string]string)},&#xA;        ipamType:         defaultIpam,&#xA;        id:               id,&#xA;        created:          time.Now(),&#xA;        ctrlr:            c,&#xA;        persist:          true,&#xA;        drvOnce:          &amp;amp;sync.Once{},&#xA;        loadBalancerMode: loadBalancerModeDefault,&#xA;    }&#xA;    // ... 省略多行 ...&#xA;    err = c.addNetwork(network)&#xA;    // ... 省略多行 ...&#xA;    return network, nil&#xA;}&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;根据驱动创建网络&#34;&gt;根据驱动创建网络&lt;a class=&#34;anchor&#34; href=&#34;#%e6%a0%b9%e6%8d%ae%e9%a9%b1%e5%8a%a8%e5%88%9b%e5%bb%ba%e7%bd%91%e7%bb%9c&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;p&gt;addNetwork()函数中先根据network对象的网络类型获取相应的驱动，然后调用该驱动对象的&lt;code&gt;d.CreateNetwork()&lt;/code&gt;函数：&#xA;&lt;code&gt;vendor/github.com/docker/libnetwork/controller.go&lt;/code&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:1313/kubernetes/docker-code-start-dockerd.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/kubernetes/docker-code-start-dockerd.html</guid>
      <description>&lt;h1 id=&#34;docker源码-启动流程&#34;&gt;DOCKER源码-启动流程&lt;a class=&#34;anchor&#34; href=&#34;#docker%e6%ba%90%e7%a0%81-%e5%90%af%e5%8a%a8%e6%b5%81%e7%a8%8b&#34;&gt;#&lt;/a&gt;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;dockerd启动流程&#34;&gt;dockerd启动流程&lt;a class=&#34;anchor&#34; href=&#34;#dockerd%e5%90%af%e5%8a%a8%e6%b5%81%e7%a8%8b&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;当用户执行命令&lt;code&gt;systemctl start docker&lt;/code&gt;后，systemd检查docker配置文件&lt;code&gt;/usr/lib/systemd/system/docker.service&lt;/code&gt;。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;因为docker配置文件中定义了&lt;code&gt;BindsTo=containerd.service&lt;/code&gt;，所以containerd将先启动。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;如果containerd启动失败，那么dockerd也将启动失败。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;containerd启动成功后，dockerd二进制文件开始启动，开始执行main()函数。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;dockerd程序的main函数所在文件：&lt;code&gt;cmd/dockerd/docker.go&lt;/code&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;main函数中的调用的&lt;code&gt;newDaemonCommand()&lt;/code&gt;函数用于构建一个Command对象，该对象定义如下：&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;cmd := &amp;amp;cobra.Command{&#xA;        Use:           &amp;#34;dockerd [OPTIONS]&amp;#34;,&#xA;        Short:         &amp;#34;A self-sufficient runtime for containers.&amp;#34;,&#xA;        SilenceUsage:  true,&#xA;        SilenceErrors: true,&#xA;        Args:          cli.NoArgs,&#xA;        RunE: func(cmd *cobra.Command, args []string) error {&#xA;            opts.flags = cmd.Flags()&#xA;            return runDaemon(opts)&#xA;        },&#xA;        DisableFlagsInUseLine: true,&#xA;        Version:               fmt.Sprintf(&amp;#34;%s, build %s&amp;#34;, dockerversion.Version, dockerversion.GitCommit),&#xA;    }&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Command对象中的RunE字段就是用来启动dockerd的函数，它最终调用了&lt;code&gt;DaemonCli.start()&lt;/code&gt;函数，DaemonCli定义：&#xA;&lt;code&gt;cmd/dockerd/daemon.go&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;type DaemonCli struct {&#xA;    *config.Config&#xA;    configFile *string&#xA;    flags      *pflag.FlagSet&#xA;    api             *apiserver.Server&#xA;    d               *daemon.Daemon&#xA;    authzMiddleware *authorization.Middleware&#xA;}&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;而&lt;code&gt;DaemonCli&lt;/code&gt;实例中的&lt;code&gt;start()&lt;/code&gt;函数是dockerd启动的主要逻辑，主要包含以下事项：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;从命令行中加载日志相关参数，并配置logrus日志对象&lt;/li&gt;&#xA;&lt;li&gt;根据命令行参数判断是否打开Debug模式，是否打开实验特性&lt;/li&gt;&#xA;&lt;li&gt;创建&lt;code&gt;--data-root&lt;/code&gt;目录&lt;/li&gt;&#xA;&lt;li&gt;创建PID文件&lt;/li&gt;&#xA;&lt;li&gt;是否启用无root模式&lt;/li&gt;&#xA;&lt;li&gt;加载API Server相关配置，包括证书&lt;/li&gt;&#xA;&lt;li&gt;创建API Server，但现在不启动&lt;/li&gt;&#xA;&lt;li&gt;创建一个协程与containerd保持心跳，如果连接containerd失败，则dockerd启动失败&lt;/li&gt;&#xA;&lt;li&gt;监听信号量并设置hook函数用于释放DaemonCli对象&lt;/li&gt;&#xA;&lt;li&gt;创建Daemon实例，创建Daemon的过程中创建了几个重要对象（这里只列出部分，后面会细讲）：&#xA;&lt;ul&gt;&#xA;&lt;li&gt;containerd：负责容器相关操作，且依赖一个RPC客户端，大部分容器相关操作都通过RPC调用containerd进程来完成&lt;/li&gt;&#xA;&lt;li&gt;volumes：负责volume的创建、查找、删除等&lt;/li&gt;&#xA;&lt;li&gt;imageService：负责镜像相关操作&lt;/li&gt;&#xA;&lt;li&gt;netController：负责网络设备的相关操作&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;将Daemon对象放入DaemonCli对象中&lt;/li&gt;&#xA;&lt;li&gt;启动Metrics服务，默认地址：unix:/var/run/docker/metrics.sock&lt;/li&gt;&#xA;&lt;li&gt;如果之前配置过swarm的话，则重新加入集群，并启动集群总的服务&lt;/li&gt;&#xA;&lt;li&gt;构建API路由：用Daemon对象中包含的各核心功能与API Path对应起来&lt;/li&gt;&#xA;&lt;li&gt;启动API Server，默认地址：unix:/var/run/docker.sock&lt;/li&gt;&#xA;&lt;li&gt;阻塞直到API Server协程发生错误或完成退出后，结束各个协程&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;daemon结构体&#34;&gt;Daemon结构体&lt;a class=&#34;anchor&#34; href=&#34;#daemon%e7%bb%93%e6%9e%84%e4%bd%93&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;p&gt;以下是Daemon结构体的定义，在19.03.7版本中包含40个字段，下面我们只分析几个重要字段：&#xA;&lt;code&gt;daemon/daemon.go&lt;/code&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:1313/kubernetes/docker-config-direct-lvm.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/kubernetes/docker-config-direct-lvm.html</guid>
      <description>&lt;h1 id=&#34;配置direct-lvm&#34;&gt;配置direct-lvm&lt;a class=&#34;anchor&#34; href=&#34;#%e9%85%8d%e7%bd%aedirect-lvm&#34;&gt;#&lt;/a&gt;&lt;/h1&gt;&#xA;&lt;h2 id=&#34;关于-docker-存储引擎&#34;&gt;关于 Docker 存储引擎&lt;a class=&#34;anchor&#34; href=&#34;#%e5%85%b3%e4%ba%8e-docker-%e5%ad%98%e5%82%a8%e5%bc%95%e6%93%8e&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;在 CentOS 上安装 Docker 时，默认的存储方式为 devicemapper，而 devicemapper 又有两种模式，默认为 loop-lvm，也就是挂载 loop 设备的方式，在安装 Doccker 后，它会挂载两个 loop 设备用作存储，这两个 loop 设备对应 Docker 安装目录下的两个文件，如下：&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[user@ser0 ~]$ sudo losetup -a&#xA;/dev/loop0: [64770]:2164801071 (/var/lib/docker/devicemapper/devicemapper/data)&#xA;/dev/loop1: [64770]:2164801072 (/var/lib/docker/devicemapper/devicemapper/metadata)&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;看下它们的大小：&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[user@ser0 ~]$ sudo ls -hls /var/lib/docker/devicemapper/devicemapper/&#xA;total 52G&#xA;52G -rw-------. 1 root root 100G Jan 7 07:28 data&#xA;54M -rw-------. 1 root root 2.0G Jan 7 15:28 metadata&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;它的优点是不用过多配置，开箱即用，但性能很差，默认可使用的空间是100G，在使用时有各种问题，官方不推荐这种方式用在生产中，另一种方式是direct-lvm，性能好且稳定，本文将介绍如何配置它。&lt;/p&gt;&#xA;&lt;h2 id=&#34;基本原理&#34;&gt;基本原理&lt;a class=&#34;anchor&#34; href=&#34;#%e5%9f%ba%e6%9c%ac%e5%8e%9f%e7%90%86&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;准备一块单独的磁盘给Docker存储用，逻辑物理均可，用lvm2工具将这块磁盘分为两个逻辑卷，再将这两个逻辑卷转换成thin pool类型，然后配置给Docker。&lt;/p&gt;&#xA;&lt;h2 id=&#34;开始配置&#34;&gt;开始配置&lt;a class=&#34;anchor&#34; href=&#34;#%e5%bc%80%e5%a7%8b%e9%85%8d%e7%bd%ae&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;查看当前存储引擎：&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[user@ser0 ~]$ sudo docker info&#xA;...&#xA;Server Version: 1.12.1 Storage Driver: devicemapper&#xA;Pool Name: docker-253:2-6442838537-pool     # 这里表示当前是loop-lvm模式&#xA;Pool Blocksize: 65.54 kB&#xA;Base Device Size: 53.69 GB&#xA;Backing Filesystem: xfs&#xA;Data file: /dev/loop0&#xA;Metadata file: /dev/loop1&#xA;Data Space Used: 55.5 GB&#xA;Data Space Total: 107.4 GB                  # 默认只有100G可用空间，届时将无法创建容器&#xA;Data Space Available: 51.88 GB&#xA;Metadata Space Used: 33.37 MB&#xA;Metadata Space Total: 2.147 GB&#xA;Metadata Space Available: 2.114 GB&#xA;Thin Pool Minimum Free Space: 10.74 GB&#xA;Udev Sync Supported: true&#xA;...&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;查看磁盘使用情况&lt;/p&gt;</description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:1313/kubernetes/docker-dynamic-port-map.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/kubernetes/docker-dynamic-port-map.html</guid>
      <description>&lt;h1 id=&#34;动态映射端口&#34;&gt;动态映射端口&lt;a class=&#34;anchor&#34; href=&#34;#%e5%8a%a8%e6%80%81%e6%98%a0%e5%b0%84%e7%ab%af%e5%8f%a3&#34;&gt;#&lt;/a&gt;&lt;/h1&gt;&#xA;&lt;p&gt;虽然设计者们一再地强调在使用Docker时要遵循最佳实践，但很多情况下并不能完全做到最佳实践，就比如我们现在的情况，是把Docker当做虚拟机来用，每个容器中包含了很多服务，，，当然，这也带来了很多问题，就比如：要映射很多的端口到物理机。&lt;/p&gt;&#xA;&lt;h2 id=&#34;痛苦的映射问题&#34;&gt;痛苦的映射问题&lt;a class=&#34;anchor&#34; href=&#34;#%e7%97%9b%e8%8b%a6%e7%9a%84%e6%98%a0%e5%b0%84%e9%97%ae%e9%a2%98&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;在启动一个Docker容器时，可以指定-p参数来把容器内的端口映射到宿主机的IP端口上，这样可以很方便地从外界访问容器中的服务，一开始全都是用-p这种方式来做，如果容器中有10个端口需要映射到外面，那就指定10个-p选项，实际上可能更多，如下：&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[root@blog ~]# docker run \&#xA;--name node1 \&#xA;-p 10.0.82.43:22:22 \&#xA;-p 10.0.82.43:8080:8080 \&#xA;-p 10.0.82.43:6066:6066 \&#xA;-p 10.0.82.43:7071:7071 \&#xA;-p 10.0.82.43:111:111 \&#xA;-p 10.0.82.43:5005:5005 \&#xA;-p 10.0.82.43:6265:6265 \&#xA;-p 10.0.82.43:7699:7699 \&#xA;...&#xA;-tid node-base &lt;/code&gt;&lt;/pre&gt;&lt;p&gt;然后当docker ps的时候会看到整个屏幕被-p参数给占满了，，，好吧，这个不是问题，问题是如果在使用了很久后发现有一个端口没有映射出来！或者是你需要通过Java API远程调用容器中的程序，而又恰好没有映射那个端口，那该怎么办？&lt;/p&gt;&#xA;&lt;h2 id=&#34;映射的实现&#34;&gt;映射的实现&lt;a class=&#34;anchor&#34; href=&#34;#%e6%98%a0%e5%b0%84%e7%9a%84%e5%ae%9e%e7%8e%b0&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;其实Docker run命令中的-p选项，最终是通过宿主机上的iptables来实现的（也许你早就发现了，只是没有仔细研究），在Docker的宿主机上查看一下iptables的nat表，大概会看到下面这样：&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[root@blog ~]# iptables -t nat -nvL&#xA;Chain PREROUTING (policy ACCEPT 1082K packets, 173M bytes)&#xA; pkts bytes target     prot opt in     out     source               destination         &#xA;  637 37876 DNAT       tcp  --  *      *       0.0.0.0/0            10.100.124.231       tcp dpt:80 to:10.100.124.231:5000&#xA; 452K   27M DOCKER     all  --  *      *       0.0.0.0/0            0.0.0.0/0            ADDRTYPE match dst-type LOCAL&#xA; &#xA;Chain INPUT (policy ACCEPT 267K packets, 29M bytes)&#xA; pkts bytes target     prot opt in     out     source               destination         &#xA; &#xA;Chain OUTPUT (policy ACCEPT 144K packets, 8644K bytes)&#xA; pkts bytes target     prot opt in     out     source               destination         &#xA;26032 1563K DOCKER     all  --  *      *       0.0.0.0/0           !127.0.0.0/8          ADDRTYPE match dst-type LOCAL&#xA; &#xA;Chain POSTROUTING (policy ACCEPT 144K packets, 8666K bytes)&#xA; pkts bytes target     prot opt in     out     source               destination         &#xA; 108K 7893K MASQUERADE  all  --  *      !docker0  192.11.231.0/24      0.0.0.0/0          &#xA;91402 6409K MASQUERADE  all  --  *      !docker_gwbridge  192.12.231.0/24      0.0.0.0/0          &#xA;    0     0 MASQUERADE  tcp  --  *      *       192.11.231.2         192.11.231.2         tcp dpt:5000&#xA;    0     0 MASQUERADE  tcp  --  *      *       192.11.231.8         192.11.231.8         tcp dpt:8080&#xA;    0     0 MASQUERADE  tcp  --  *      *       192.11.231.8         192.11.231.8         tcp dpt:5005&#xA; &#xA;Chain DOCKER (2 references)&#xA; pkts bytes target     prot opt in     out     source               destination         &#xA;    4   264 RETURN     all  --  docker0 *       0.0.0.0/0            0.0.0.0/0          &#xA;    9   540 RETURN     all  --  docker_gwbridge *       0.0.0.0/0            0.0.0.0/0          &#xA;26315 1579K DNAT       tcp  --  !docker0 *       0.0.0.0/0            10.100.124.231       tcp dpt:5000 to:192.11.231.2:5000&#xA;    0     0 DNAT       tcp  --  !docker0 *       0.0.0.0/0            10.100.124.115       tcp dpt:8080 to:192.11.231.8:8080&#xA;    0     0 DNAT       tcp  --  !docker0 *       0.0.0.0/0            10.100.124.115       tcp dpt:5005 to:192.11.231.8:5005&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;最主要最下面的DOCKER链，最后那三条规则就是在启动容器时指定了-p所导致的，那个5000端口你应该很熟悉，那是Register服务的端口号，我们可以通过它在局域网中上传和下载镜像，而且我在启动它的时候也只映射了这一个端口，如下：&lt;/p&gt;</description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:1313/kubernetes/docker-multi-host-with-zk.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/kubernetes/docker-multi-host-with-zk.html</guid>
      <description>&lt;h1 id=&#34;多主机通信之zk&#34;&gt;多主机通信之ZK&lt;a class=&#34;anchor&#34; href=&#34;#%e5%a4%9a%e4%b8%bb%e6%9c%ba%e9%80%9a%e4%bf%a1%e4%b9%8bzk&#34;&gt;#&lt;/a&gt;&lt;/h1&gt;&#xA;&lt;p&gt;目前（2016.09.15）公司正在开发一个新产品，每次测试组在部署新版本时，需要先将 Linux 环境配好，卸载后又需要重新配置 Linux 环境，显然中间有很多重复性工作，所以为了加快产品版本迭代速度，减少重复性工作，我们利用Docker来做环境隔离和快速部署。 而为了测试产品性能，又须要运行在多个物理机上，那么Docker如何在多物理机间通信，就是首先要解决的问题。 上网找了很久的资料，大概有以下几种：&lt;/p&gt;&#xA;&lt;p&gt;Swarm模式：这是官方提供的Docker集群模式，且能够跨物理机通信，使用起简单方便，先看一个简单的例子，下面的命令在Swarm集群中创建一个Service：&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;docker service create \&#xA;--constraint node.hostname==biyu-c3.com \&#xA;--name sys \&#xA;--replicas 1 \&#xA;--network ingress \&#xA;kxdmmr/centos6.8-ssh bash -c &amp;#39;/usr/sbin/sshd -D&amp;#39;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;而我们启动自已的容器可能要加一些别的参数，比如：&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;docker run --privileged=true --ip 19.19.19.19 &#xA;--name hostname -h hostname -p 8080:8080 -p 80:80 &#xA;-v /data/hadoop-data:/data --storage-opt size=30G&#xA;-tid registry.io:5000/centos6.5&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如上，其中的选项：&lt;code&gt;--privileged&lt;/code&gt;、&lt;code&gt;--ip&lt;/code&gt; 在 Swarm 模式中是不提供的，目前也没有办法实现（当然你也可以自已把 Docker 重写了&amp;hellip;），所以不得不另寻它路。&#xA;自己修改网络配置，将 Dockre 网桥与物理机设在同一网段&amp;hellip;，这种方式一看我就头大了，非官方的，太麻烦，且没有保证。&lt;/p&gt;&#xA;&lt;p&gt;用第三方的服务发现工具，Docker 支持的工具有：Consul、Etcd、Zookeeper，官方给了一个列子，是在一个物理机创建多个虚拟机来模拟的，跟在物理机上还是有区别的，而且官方也没有给多更多资料，只好自已摸索了，好在最终用Zookeeper解决了问题。这种方式的好处是，我们在创建和使用容器时，跟以前没有任何区别，启动容器后，各物理机上的容器可以直接通信，且可以使用以前使用的任何选项。下面我们就动手实现一下。&lt;/p&gt;&#xA;&lt;h2 id=&#34;基本原理&#34;&gt;基本原理&lt;a class=&#34;anchor&#34; href=&#34;#%e5%9f%ba%e6%9c%ac%e5%8e%9f%e7%90%86&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;原理就是在物理机上搭建 Zookeeper，然后每个机器上的 Docker daemon 进程将网络配置和每个容器的ip等信息存储到 Zookeeper，具体细节较复杂，笔者没有彻底明白。&lt;/p&gt;&#xA;&lt;h2 id=&#34;环境描述&#34;&gt;环境描述&lt;a class=&#34;anchor&#34; href=&#34;#%e7%8e%af%e5%a2%83%e6%8f%8f%e8%bf%b0&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;硬件环境 机器数量：5&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;100.0.8.27  node27.kxdmmr.com&#xA;100.0.8.28  node28.kxdmmr.com&#xA;100.0.8.29  node29.kxdmmr.com&#xA;100.0.8.30  node30.kxdmmr.com&#xA;100.0.8.31  node31.kxdmmr.com&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;系统环境 系统版本：官方 CentOS 7 内核版本：3.10.0-327.el7.x86_64&#xA;Docker 环境 Docker版本：1.12.1 基础镜像拉取地址：&lt;code&gt;daocloud.io/centos:6&lt;/code&gt; 基础镜像系统版本：centos 6.8&lt;/p&gt;</description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:1313/kubernetes/docker-registry-deploy-manage.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/kubernetes/docker-registry-deploy-manage.html</guid>
      <description>&lt;h1 id=&#34;私有仓库的搭建与管理&#34;&gt;私有仓库的搭建与管理&lt;a class=&#34;anchor&#34; href=&#34;#%e7%a7%81%e6%9c%89%e4%bb%93%e5%ba%93%e7%9a%84%e6%90%ad%e5%bb%ba%e4%b8%8e%e7%ae%a1%e7%90%86&#34;&gt;#&lt;/a&gt;&lt;/h1&gt;&#xA;&lt;h2 id=&#34;docker-registry-简介&#34;&gt;Docker Registry 简介&lt;a class=&#34;anchor&#34; href=&#34;#docker-registry-%e7%ae%80%e4%bb%8b&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;怎样在几个机器之间使用同一个镜像？难道要先上传到官方的仓库，另几个机器去下载吗？当然不是了，最好的方法就是搭建一个本地的仓库。 Docker Registry 就是官方提供的搭建本地仓库的方案，它有两种搭建方式， http 与 https ，如果你只简单的在本地使用，第一种就足够了，如果考虑到安全方面，就用 https ，这种方式当然就需要为你的仓库服务器申请证书了，或者自建一个证书机构，自已为自已颁发证书。本文只介绍 http 方式，并介绍仓库的上传、下载、查询、删除等常用操作。&lt;/p&gt;&#xA;&lt;h2 id=&#34;环境描述&#34;&gt;环境描述&lt;a class=&#34;anchor&#34; href=&#34;#%e7%8e%af%e5%a2%83%e6%8f%8f%e8%bf%b0&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;假设我有三台机器，他它们的/etc/hosts文件包含如下内容：&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[user@ser1 ~]$ cat /etc/hosts&#xA;10.100.100.101  ser1.node.com  registry.io&#xA;10.100.100.102  ser2.node.com&#xA;10.100.100.103  ser3.node.com&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;很显然，我要在第一台机器上部署 Docker Registry ，并且这三台机器都已安装好了Docker服务&lt;/p&gt;&#xA;&lt;h2 id=&#34;docker-registry-搭建&#34;&gt;Docker Registry 搭建&lt;a class=&#34;anchor&#34; href=&#34;#docker-registry-%e6%90%ad%e5%bb%ba&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;官方提供的私有仓库是以镜像像方式提供的，也就是说你只要把这个镜像下载下来启动就可以了，当然还要自已定制一些运行参数。 关于所有可配置项，可以去官网查看，具体地址在本文尾部。&lt;/p&gt;&#xA;&lt;p&gt;在第一台机器上运行官方提供的 Registry 容器&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;sudo mkdir -p /data/docker-registry&#xA;&#xA;docker run -dt \&#xA;--name registry \&#xA;-p 10.100.100.101:5000:5000 \&#xA;--restart=always \&#xA;-v /data/docker-registry:/var/lib/registry \&#xA;-e REGISTRY_STORAGE_DELETE_ENABLED=true \&#xA;-e REGISTRY_STORAGE_DELETE_REDIRECT=true \&#xA;registry:2&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;参数说明：&#xA;&lt;code&gt;-p 10.100.100.101:5000:5000&lt;/code&gt; 这是把容器的端口映射到物理机，这样在其它机器直接访问物理机就相当于访问仓库了&#xA;&lt;code&gt;--restart=always&lt;/code&gt; 这个参数可以让容器在发生意外的时候还可以自已启动，比如当我重启 Docker 服务的时候&#xA;&lt;code&gt;-v /data/docker-registry:/var/lib/registry&lt;/code&gt; 找一个容量大的挂载点映射给容器，用来存放上传的镜像&#xA;&lt;code&gt;-e&lt;/code&gt; 最后这两个是让我们可以远程删除已经上传的镜像&lt;/p&gt;</description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:1313/kubernetes/docker-with-glusterfs.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/kubernetes/docker-with-glusterfs.html</guid>
      <description>&lt;h1 id=&#34;数据持久化之glusterfs&#34;&gt;数据持久化之GlusterFS&lt;a class=&#34;anchor&#34; href=&#34;#%e6%95%b0%e6%8d%ae%e6%8c%81%e4%b9%85%e5%8c%96%e4%b9%8bglusterfs&#34;&gt;#&lt;/a&gt;&lt;/h1&gt;&#xA;&lt;p&gt;在用 Docker 搭建集群时，遇到一些数据持久化的问题，如果容器内的数据量大的话就不能将数据保存在容器内，否则不方便迁移，而如果使用 Volume 这样的方式挂载，那么假如容器挂掉以后，会在别的物理机上重新启起来（如 Swarm 模式），这样 Volume 方式就不生效了。所以容器内的数据的持久化是个必须解决的问题，相信很多人也遇到了这样的需求，本文就此问题介绍一些解决方法。&lt;/p&gt;&#xA;&lt;h2 id=&#34;关于-glusterfs&#34;&gt;关于 GlusterFS&lt;a class=&#34;anchor&#34; href=&#34;#%e5%85%b3%e4%ba%8e-glusterfs&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;GlusterFS 是一种开源的分布式文件系统，易于横向扩展，性能高，可直接用 Linux 系统的 mount 命令从远程挂载到本地来使用，比如下面一条命令：&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;mount -t glusterfs glusterfs.server.com:/src-volume /mnt&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;说明： &lt;code&gt;-t glusterfs&lt;/code&gt; 是指挂载格式为 &lt;code&gt;glusterfs&lt;/code&gt;&#xA;&lt;code&gt;glusterfs.server.com&lt;/code&gt; 是指 glusterfs 服务所在的机器是址，这里换成IP当然也是可以的啦&#xA;&lt;code&gt;/mnt&lt;/code&gt; 是一个本地目录&#xA;这样就将 glusterfs 服务器上的一个卷挂载到本地了，然后就可以直接对 /mnt 下的文件进行存取，是不是感觉很方便？那怎么把它跟 Docker 结合起来呢？有两种方式， Docker 的 plugin 功能和容器内部挂载方式&lt;/p&gt;&#xA;&lt;h2 id=&#34;第一种docker-的-plugin-功能&#34;&gt;第一种：Docker 的 plugin 功能&lt;a class=&#34;anchor&#34; href=&#34;#%e7%ac%ac%e4%b8%80%e7%a7%8ddocker-%e7%9a%84-plugin-%e5%8a%9f%e8%83%bd&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;Docker 本身提供了一个 plugin 的功能模块，允许用户安装各种插件来扩展 Docker 的功能，而 GlusterFS 就是被其支持的插件之一，安装此插件后，就可以在启动容器的时候挂载 GlusterFS 类型的 Volume ，那么具体怎么做呢？&lt;/p&gt;&#xA;&lt;p&gt;部署 GlusterFS 服务，教程网上多的是，这里提供一篇： &lt;a href=&#34;http://navyaijm.blog.51cto.com/4647068/1258250&#34;&gt;http://navyaijm.blog.51cto.com/4647068/1258250&lt;/a&gt; 我们假设有三台主机部署了 GlusterFS ，分别为：&lt;code&gt;gfs-1&lt;/code&gt;、&lt;code&gt;gfs-2&lt;/code&gt;、&lt;code&gt;gfs-3&lt;/code&gt;&lt;/p&gt;&#xA;&lt;p&gt;在你安装有 Docker 的主机上，安装 go 语言环境，如已安装请跳过此步，安装方法请转到： &lt;a href=&#34;https://github.com/astaxie/build-web-application-with-golang/blob/master/zh/01.1.md&#34;&gt;https://github.com/astaxie/build-web-application-with-golang/blob/master/zh/01.1.md&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:1313/kubernetes/k8s-calico.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/kubernetes/k8s-calico.html</guid>
      <description>&lt;h1 id=&#34;calico网络&#34;&gt;calico网络&lt;a class=&#34;anchor&#34; href=&#34;#calico%e7%bd%91%e7%bb%9c&#34;&gt;#&lt;/a&gt;&lt;/h1&gt;&#xA;&lt;h2 id=&#34;什么是calico&#34;&gt;什么是Calico&lt;a class=&#34;anchor&#34; href=&#34;#%e4%bb%80%e4%b9%88%e6%98%afcalico&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;Calico是一个为k8s而设计的基于路由的容器网络解决方案，它的核心思想是把Linux节点当做路由器，用边界网关协议（BGP）将当前节点的路由信息通知给其它节点，来完成各个节点上的路由自动配置。&lt;/p&gt;&#xA;&lt;p&gt;有关BGP的作用和原理请看另一篇文章，&lt;a href=&#34;http://localhost:1313/articles/kubernetes/popular-bgp-protocol&#34;&gt;白话BGP协议&lt;/a&gt;。&lt;/p&gt;&#xA;&lt;h3 id=&#34;calico的组成&#34;&gt;Calico的组成&lt;a class=&#34;anchor&#34; href=&#34;#calico%e7%9a%84%e7%bb%84%e6%88%90&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;p&gt;Calico通常以Daemonset的方式安装到k8s集群中的每个子节点上，Pod的默认名字为calico-node，Pod中包含几个组件：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;BGP Client&lt;/code&gt;：BGP客户端，负责将本机的路由信息通知给其它所有节点，同时接收其它节点发来的信息，端口：179。&lt;code&gt;BGP Client&lt;/code&gt;引用了一个开源的BGP项目，你可以在本文最后找到项目地址&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;calico-felix&lt;/code&gt;：负表将将其它节点发来的路由信息设置到物理机上&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;BGP Route Reflector&lt;/code&gt;：另一个BGP客户端，在集群节点较多时启用此模式，比&lt;code&gt;BGP Client&lt;/code&gt;节省资源&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;工作原理&#34;&gt;工作原理&lt;a class=&#34;anchor&#34; href=&#34;#%e5%b7%a5%e4%bd%9c%e5%8e%9f%e7%90%86&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;p&gt;Calico启动后，所有节点上的BGP客户端将自己所在节点的主机名、IP地址、AS号等信息注册到ETCD中，这样所有节点的BGP就可以通过ETCD中的信息找到其它BGP，这时每个BGP定期将自已节点上的路由信息通知给其它所有BGP，其它节点收到信息后进行分析和筛选，将有用的信息通过&lt;code&gt;calico-felix&lt;/code&gt;设置到本地Linux路由表中。&lt;/p&gt;&#xA;&lt;p&gt;通过以下命令查看BGP注册在ETCD中的信息：&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;curl -s localhost:2379/v2/keys/calico/bgp/v1/host | jq&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;数据流程&#34;&gt;数据流程&lt;a class=&#34;anchor&#34; href=&#34;#%e6%95%b0%e6%8d%ae%e6%b5%81%e7%a8%8b&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;p&gt;有A和B两台机器，每个机器上各有一个容器X和Y，第个容器中的虚拟网卡eth0分别对应了物理机上的一个虚拟网卡，如下图中所示，容器X中的eth0网卡对应了物理机上的103，这种成对出现的设备叫做veth设备，是Linux系统专门为Namespace（也就是容器技术）设计的，当有数据从一端进入以后会被自动转发到另一端，这样就可以在容器和物理之间方便地交换数据，当我们在k8s中创建一个Pod时，kubelet组件会调用CNI插件来为Pod添加虚拟网卡，veth设备就是在这个时候被创建的。&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;img/k8s-calico/calico-data-process.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;&lt;p&gt;当X访问Y时，报文先由容器中的&lt;code&gt;eth0@if103&lt;/code&gt;到达物理机上的&lt;code&gt;103: cali103&lt;/code&gt;，然后物理机上会有一条路由（通过命令&lt;code&gt;ip route&lt;/code&gt;查看），告诉系统将目标为2.2.0.0/16这个段的数据包全部转发到5.5.5.3，数据到了B以到，机器B上会有一条路由把目标为2.2.2.2的数据包转发到203网卡上，203同样是一个veth设备，它直接转发给容器中的&lt;code&gt;eth0@if203&lt;/code&gt;网卡，A和B上的两条路由就是由Calico来设置的。&lt;/p&gt;&#xA;&lt;h2 id=&#34;参考与引用&#34;&gt;参考与引用&lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83%e4%b8%8e%e5%bc%95%e7%94%a8&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://docs.projectcalico.org/v2.5/getting-started/kubernetes/installation/hosted/hosted&#34;&gt;Calico官方文档&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://nat.moe/technologies/nat/1256/&#34;&gt;白话BGP协议&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/osrg/gobgp&#34;&gt;使用GO语言实现的BGP项目&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:1313/kubernetes/k8s-cni.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/kubernetes/k8s-cni.html</guid>
      <description>&lt;h1 id=&#34;k8s-cni&#34;&gt;k8s CNI&lt;a class=&#34;anchor&#34; href=&#34;#k8s-cni&#34;&gt;#&lt;/a&gt;&lt;/h1&gt;&#xA;&lt;h2 id=&#34;cnicontainer-network-interface&#34;&gt;CNI（container network interface）&lt;a class=&#34;anchor&#34; href=&#34;#cnicontainer-network-interface&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;为了将网络功能插件化，k8s要1.5提出了CNI标准，现在我们常用的网络如：flannel、calico等都是基于CNI标准开发的。&lt;/p&gt;&#xA;&lt;h3 id=&#34;kubelet配置cni&#34;&gt;kubelet配置CNI&lt;a class=&#34;anchor&#34; href=&#34;#kubelet%e9%85%8d%e7%bd%aecni&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;--network-plugin=cni&#xA;--cni-bin-dir=/opt/cni/bin&#xA;--cni-conf-dir=/etc/cni/net.d&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;/opt/cni/bin&lt;/code&gt;：CNI插件的存放目录&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;/etc/cni/net.d&lt;/code&gt;：插件的配置文件的存放目录&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;接口定义&#34;&gt;接口定义&lt;a class=&#34;anchor&#34; href=&#34;#%e6%8e%a5%e5%8f%a3%e5%ae%9a%e4%b9%89&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;code&gt;vendor/github.com/containernetworking/cni/libcni/api.go&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;type CNI interface {&#xA;&#x9;AddNetworkList(net *NetworkConfigList, rt *RuntimeConf) (types.Result, error)&#xA;&#x9;DelNetworkList(net *NetworkConfigList, rt *RuntimeConf) error&#xA;&#xA;&#x9;AddNetwork(net *NetworkConfig, rt *RuntimeConf) (types.Result, error)&#xA;&#x9;DelNetwork(net *NetworkConfig, rt *RuntimeConf) error&#xA;}&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;接口实现&#34;&gt;接口实现&lt;a class=&#34;anchor&#34; href=&#34;#%e6%8e%a5%e5%8f%a3%e5%ae%9e%e7%8e%b0&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;p&gt;下面以calico-plugin为例说明CNI的实现，calico-plugin与&lt;a href=&#34;http://localhost:1313/articles/kubernetes/k8s-calico&#34;&gt;calico容器网络&lt;/a&gt;之间没有直接关系，我们通常说的calico指的是这两个东西的组合，而calico容器网络是解决不同物理机上容器之间的通信，而calico-plugin是在k8s创建Pod时为Pod设置虚拟网卡，也就是容器中的&lt;code&gt;eth0&lt;/code&gt;和&lt;code&gt;lo&lt;/code&gt;网卡，calico-plugin是由两个静态的二进制文件组成，由kubelet以命令行的形式调用，这两个二进制的作用如下：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;calico-ipam：分配IP，维护IP池，需要依赖etcd。&lt;/li&gt;&#xA;&lt;li&gt;calico：通过调用系统API来修改namespace中的网卡信息。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;文件在物理机上的位置：&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;/opt/cni/&#xA;├── bin&#xA;│   ├── calico&#xA;│   ├── calico-ipam&#xA;└── net.d&#xA;    └── 10-calico.conf&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;calico-plugin在ETCD中维护的信息：&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;curl -s localhost:2379/v2/keys/calico/ipam/v2/assignment/ipv4/block | jq&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;calico插件配置&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;vim /opt/cni/net.d/10-calico.conf&#xA;{&#xA;    &amp;#34;name&amp;#34;: &amp;#34;calico-k8s-network&amp;#34;,&#xA;    &amp;#34;cniVersion&amp;#34;: &amp;#34;0.1.0&amp;#34;,&#xA;    &amp;#34;type&amp;#34;: &amp;#34;calico&amp;#34;,&#xA;    &amp;#34;etcd_endpoints&amp;#34;: &amp;#34;http://127.0.0.1:2379&amp;#34;,&#xA;    &amp;#34;log_level&amp;#34;: &amp;#34;info&amp;#34;,&#xA;    &amp;#34;ipam&amp;#34;: {&#xA;        &amp;#34;type&amp;#34;: &amp;#34;calico-ipam&amp;#34;&#xA;    },&#xA;    &amp;#34;kubernetes&amp;#34;: {&#xA;        &amp;#34;kubeconfig&amp;#34;: &amp;#34;/opt/rainbond/kubernetes/kubecfg/admin.kubeconfig&amp;#34;&#xA;    }&#xA;}&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;calico-plugin工作原理&#34;&gt;calico-plugin工作原理&lt;a class=&#34;anchor&#34; href=&#34;#calico-plugin%e5%b7%a5%e4%bd%9c%e5%8e%9f%e7%90%86&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;p&gt;kubelet在创建一个Pod时，会先启动puase容器，然后为这个容器添加设置网络，也就是添加网卡，这里会通过CNI调起文件系统中的&lt;code&gt;/opt/cni/bin/calico&lt;/code&gt;，并将Pod信息通过标准输入(stdin)传递给calico进程，calico通过修改系统中Namespace达到为容器添加网卡的目的。&lt;/p&gt;</description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:1313/kubernetes/k8s-code-apiserver-start.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/kubernetes/k8s-code-apiserver-start.html</guid>
      <description>&lt;h1 id=&#34;k8s源码分析-apiserver&#34;&gt;k8s源码分析-apiserver&lt;a class=&#34;anchor&#34; href=&#34;#k8s%e6%ba%90%e7%a0%81%e5%88%86%e6%9e%90-apiserver&#34;&gt;#&lt;/a&gt;&lt;/h1&gt;&#xA;&lt;p&gt;本系列文章是基于kubernetes1.7版本的。&lt;/p&gt;&#xA;&lt;h2 id=&#34;main函数&#34;&gt;main函数&lt;a class=&#34;anchor&#34; href=&#34;#main%e5%87%bd%e6%95%b0&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;apiserver的入口定义在&lt;code&gt;cmd/kube-apiserver/apiserver.go&lt;/code&gt;文件中：&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;func main() {&#xA;&#x9;rand.Seed(time.Now().UTC().UnixNano())&#xA;&#xA;&#x9;// 创建一个默认配置对象&#xA;&#x9;s := options.NewServerRunOptions()&#xA;&#x9;&#xA;&#x9;// 构建命令行对象&#xA;&#x9;s.AddFlags(pflag.CommandLine)&#xA;&#xA;    // 解析命令行参数&#xA;&#x9;flag.InitFlags()&#xA;&#x9;logs.InitLogs()&#xA;&#x9;defer logs.FlushLogs()&#xA;&#xA;&#x9;// 如果指定了-version选项，则打印版本号然后退出&#xA;&#x9;verflag.PrintAndExitIfRequested()&#xA;&#xA;&#x9;// 启动服务，并从管道中监听停止信号，该通道可能永远不会写入数据&#xA;&#x9;if err := app.Run(s, wait.NeverStop); err != nil {&#xA;&#x9;&#x9;fmt.Fprintf(os.Stderr, &amp;#34;%v\n&amp;#34;, err)&#xA;&#x9;&#x9;os.Exit(1)&#xA;&#x9;}&#xA;}&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这个main函数看起来是比较直观的，中文的注释是我加上去的，便于阅读。&lt;/p&gt;&#xA;&lt;h2 id=&#34;初始化配置&#34;&gt;初始化配置&lt;a class=&#34;anchor&#34; href=&#34;#%e5%88%9d%e5%a7%8b%e5%8c%96%e9%85%8d%e7%bd%ae&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;在main函数中的第一行是为随机数产生器用当前时间提供了一个基数，避免多次启动产生相同的值，说明在这个组件中可能有些功能依赖了随数机。&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;s := options.NewServerRunOptions()&lt;/code&gt;显然是在创建一个配置对象，不过这个对象中只包含一些默认参数。&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;s.AddFlags(pflag.CommandLine)&lt;/code&gt;是构建一个命令行对象，怎么构建的呢，其实就是&lt;code&gt;s&lt;/code&gt;在&lt;code&gt;pflag.CommandLine&lt;/code&gt;这个对象中加了很多选项，并把自己的在很多字段以指针的方式注入到了&lt;code&gt;pflag.CommandLine&lt;/code&gt;中，这些选项是经过分组的，如ETCD相关配置、apiserver相关的请求超时时间等。&lt;/p&gt;&#xA;&lt;p&gt;紧接着&lt;code&gt;flag.InitFlags()&lt;/code&gt;解析所有命令行参数，并通过指针把值放到配置对象&lt;code&gt;s&lt;/code&gt;中。&lt;/p&gt;&#xA;&lt;p&gt;其中&lt;code&gt;flag&lt;/code&gt;这个包是k8s对开源项目&lt;code&gt;github.com/spf13/pflag&lt;/code&gt;的封装，而这个项目又是基于golang标准库中的flag包开发的，并且封装了一些实用的函数，可以让你快速生成自己的命令行选项。&lt;/p&gt;&#xA;&lt;h2 id=&#34;准备资源&#34;&gt;准备资源&lt;a class=&#34;anchor&#34; href=&#34;#%e5%87%86%e5%a4%87%e8%b5%84%e6%ba%90&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;然后调用到了Run()函数，定义在cmd/kube-apiserver/app/server.go文件中，这里代码较多就不全部贴出来了，为了保持文章的可读性，我会尽量减少函数展开的层数。&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;nodeTunneler, proxyTransport, err := CreateNodeDialer(runOptions)&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;上面代码判断是否安装在云主机上，如果是，并且指定了密钥文件，也是就&lt;code&gt;--ssh-keyfile&lt;/code&gt;选项，则安装key到所有云主机的实例中，然后使用该密钥文件创建一个连接器&lt;code&gt;nodeTunneler&lt;/code&gt;，用来访问其它节点。&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;kubeAPIServerConfig, sharedInformers, versionedInformers, insecureServingOptions, serviceResolver, err := CreateKubeAPIServerConfig(runOptions, nodeTunneler, proxyTransport)&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这句是利用最初的配置对象&lt;code&gt;runOptions&lt;/code&gt;创建用于启动apiserver的配置对象，与&lt;code&gt;runOptions&lt;/code&gt;不同的是，它还包括了启动apiserver所需的资源。&lt;/p&gt;&#xA;&lt;p&gt;该函数中做了以下几件事：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;根据用户指定的&lt;code&gt;--admission-control&lt;/code&gt;列表加载相应的admission插件，这些插件的作用是用来过滤API请求的，可以用来作请求的合法性检测，比如我们对所有&lt;code&gt;get csr&lt;/code&gt;的请求进行用户验证，如果该用户权限太低则拒绝该请求；也可以修改某个请求，比如为所有&lt;code&gt;create deployments&lt;/code&gt;的请求加上&lt;code&gt;scale=1&lt;/code&gt;等等。默认不加载任何规则。&lt;/li&gt;&#xA;&lt;li&gt;如果有必要的配置没有设置则为其设置默认值，如Service IP段、序列化缓存大小等。&lt;/li&gt;&#xA;&lt;li&gt;验证必要配置项，如果在此阶段有错误配置项，则所有错误信息将被打包返回。&lt;/li&gt;&#xA;&lt;li&gt;如果验证通过则开始连接ETCD，并启动同步信息的进程，该进程会一直监听apiserver的数据并同步到ETCD集群中。&lt;/li&gt;&#xA;&lt;li&gt;最后创建&lt;code&gt;master.Config&lt;/code&gt;对象，也就是&lt;code&gt;kubeAPIServerConfig&lt;/code&gt;变量，然后返回。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;kubeAPIServer, err := CreateKubeAPIServer(kubeAPIServerConfig, apiExtensionsServer.GenericAPIServer, sharedInformers, apiExtensionsConfig.CRDRESTOptionsGetter)&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这行代码是比较核心的，它的作用如下：&lt;/p&gt;</description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:1313/kubernetes/k8s-code-kubelet.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/kubernetes/k8s-code-kubelet.html</guid>
      <description>&lt;h1 id=&#34;k8s源码分析-kubelet&#34;&gt;k8s源码分析-kubelet&lt;a class=&#34;anchor&#34; href=&#34;#k8s%e6%ba%90%e7%a0%81%e5%88%86%e6%9e%90-kubelet&#34;&gt;#&lt;/a&gt;&lt;/h1&gt;&#xA;&lt;p&gt;本文以&lt;code&gt;k8s v1.10&lt;/code&gt;为例，分析kubelet组件的工作原理。&lt;/p&gt;&#xA;&lt;h2 id=&#34;入口&#34;&gt;入口&lt;a class=&#34;anchor&#34; href=&#34;#%e5%85%a5%e5%8f%a3&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;main函数定义在&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/release-1.10/pkg/kubelet/kubelet.go&#34;&gt;cmd/kubelet/kubelet.go&lt;/a&gt;，主要任务如下：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;创建命令行对象，包括解析用户指定的参数，生成配置对象等&lt;/li&gt;&#xA;&lt;li&gt;执行命令行对象，最后进入启动kubelet的逻辑，调用了kubelet的&lt;code&gt;RunKubelet()&lt;/code&gt;函数&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;创建kubelet&#34;&gt;创建kubelet&lt;a class=&#34;anchor&#34; href=&#34;#%e5%88%9b%e5%bb%bakubelet&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;以下是创建kubelet的关键函数，它创建了docker客户端、网络插件等重要组件：&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/release-1.10/pkg/kubelet/kubelet.go#L321&#34;&gt;pkg/kubelet/kubelet.go:321&lt;/a&gt;&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;func NewMainKubelet(kubeCfg *kubeletconfiginternal.KubeletConfiguration,&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;初始化容器运行时服务端cri&#34;&gt;初始化容器运行时服务端（CRI）&lt;a class=&#34;anchor&#34; href=&#34;#%e5%88%9d%e5%a7%8b%e5%8c%96%e5%ae%b9%e5%99%a8%e8%bf%90%e8%a1%8c%e6%97%b6%e6%9c%8d%e5%8a%a1%e7%ab%afcri&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;p&gt;在1.5以前的版本中，k8s依赖于dokcer，为了支持不同的容器运行时，比如rkt、containerd，kubelet从1.5开始加入了CRI标准，CRI是一组rpc接口，只要是实现了这组接口都可以作为kubelet的运行时，而且在k8s内部将之前的Pod抽象为一种更为通用的SandBox。&lt;/p&gt;&#xA;&lt;p&gt;调用过程如下：&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;kubelet -&amp;gt; remote -&amp;gt; CRI -&amp;gt; dockershim -&amp;gt; docker_client -&amp;gt; docker_daemon&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;提出了CRI标准以后，意味着在新的版本里需要使用新的连接方式与docker通信，为了兼容以前的版本，k8s提供了针对docker的CRI实现，也就是kubelet包下的dockershim包，dockershim是一个rpc服务，监听一个端口供kubelet连接，dockershim收到kubelet的请求后，将其转化为REST API请求，发送给物理机上的docker daemon，以下是创建和启动dockershim的代码：&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/release-1.10/pkg/kubelet/kubelet.go#L622&#34;&gt;pkg/kubelet/kubelet.go:NewMainKubelet():622&lt;/a&gt;&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;// 创建dockershim&#xA;ds, err := dockershim.NewDockerService(kubeDeps.DockerClientConfig, crOptions.PodSandboxImage, streamingConfig,&#xA;...&#xA;// 启动rpc服务&#xA;if err := server.Start(); err != nil {&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;创建Docker客户端的逻辑是在创建dockershim的过程中，关键代码：&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/release-1.10/pkg/kubelet/dockershim/libdocker/client.go#L100&#34;&gt;pkg/kubelet/dockershim/libdocker/client.go:100&lt;/a&gt;&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;client, err := getDockerClient(dockerEndpoint)&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;client对象就是docker的客户端，包含了我们常用的&lt;code&gt;docker  run&lt;/code&gt;，&lt;code&gt;docker images&lt;/code&gt;等所有操作，&lt;code&gt;dockerEndpoint&lt;/code&gt;就是&lt;code&gt;--container-runtime-endpoint&lt;/code&gt;选项的值，默认是&lt;code&gt;unix:///var/run/docker.sock&lt;/code&gt;。&lt;/p&gt;&#xA;&lt;h3 id=&#34;初始化容器运行时客户端&#34;&gt;初始化容器运行时客户端&lt;a class=&#34;anchor&#34; href=&#34;#%e5%88%9d%e5%a7%8b%e5%8c%96%e5%ae%b9%e5%99%a8%e8%bf%90%e8%a1%8c%e6%97%b6%e5%ae%a2%e6%88%b7%e7%ab%af&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;p&gt;dockershim是rpc的服务端，&lt;code&gt;pkg/kubelet/remote&lt;/code&gt;包是rpc客户端的实现，此包下的函数由kubelet组件调用，创建remote的代码：&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/release-1.10/pkg/kubelet/kubelet.go#L657&#34;&gt;pkg/kubelet/kubelet.go:657&lt;/a&gt;&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;runtimeService, imageService, err := getRuntimeAndImageServices(remoteRuntimeEndpoint, remoteImageEndpoint, kubeCfg.RuntimeRequestTimeout)&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;初始化网络插件cni&#34;&gt;初始化网络插件（CNI）&lt;a class=&#34;anchor&#34; href=&#34;#%e5%88%9d%e5%a7%8b%e5%8c%96%e7%bd%91%e7%bb%9c%e6%8f%92%e4%bb%b6cni&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;p&gt;创建CNI实例的逻辑是在创建dockershim的过程中，用来在容器中创建和删除网络设备，关键代码：&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/release-1.10/pkg/kubelet/dockershim/docker_service.go#L232&#34;&gt;pkg/kubelet/dockershim/docker_service.go:NewDockerService():232&lt;/a&gt;&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;cniPlugins := cni.ProbeNetworkPlugins(pluginSettings.PluginConfDir, pluginSettings.PluginBinDirs)&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;初始化卷管理器&#34;&gt;初始化卷管理器&lt;a class=&#34;anchor&#34; href=&#34;#%e5%88%9d%e5%a7%8b%e5%8c%96%e5%8d%b7%e7%ae%a1%e7%90%86%e5%99%a8&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;p&gt;卷管理器的作用是检查容器需要的卷是否已挂载，需要卸载的卷是否已经卸载，关键代码：&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/release-1.10/pkg/kubelet/kubelet.go#L817&#34;&gt;pkg/kubelet/kubelet.go:NewMainKubelet():817&lt;/a&gt;&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;klet.volumePluginMgr, err =&#xA;&#x9;&#x9;NewInitializedVolumePluginMgr(klet, secretManager, configMapManager, tokenManager, kubeDeps.VolumePlugins, kubeDeps.DynamicPluginProber)&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;初始化pod处理器&#34;&gt;初始化Pod处理器&lt;a class=&#34;anchor&#34; href=&#34;#%e5%88%9d%e5%a7%8b%e5%8c%96pod%e5%a4%84%e7%90%86%e5%99%a8&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;p&gt;创建一个叫worker的对象，它持有处理Pod的入口函数，也是就&lt;code&gt;klet.syncPod&lt;/code&gt;：&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/release-1.10/pkg/kubelet/kubelet.go#L854&#34;&gt;pkg/kubelet/kubelet.go:NewMainKubelet():854&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:1313/kubernetes/k8s-code-pod-create.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/kubernetes/k8s-code-pod-create.html</guid>
      <description>&lt;h1 id=&#34;k8s源码分析-创建pod流程&#34;&gt;k8s源码分析-创建Pod流程&lt;a class=&#34;anchor&#34; href=&#34;#k8s%e6%ba%90%e7%a0%81%e5%88%86%e6%9e%90-%e5%88%9b%e5%bb%bapod%e6%b5%81%e7%a8%8b&#34;&gt;#&lt;/a&gt;&lt;/h1&gt;&#xA;&lt;p&gt;本文从源码层面解释kubernetes从接收创建Pod的指令到实际创建Pod的整个过程。&lt;/p&gt;&#xA;&lt;h3 id=&#34;11-监听用户请求&#34;&gt;1.1 监听用户请求&lt;a class=&#34;anchor&#34; href=&#34;#11-%e7%9b%91%e5%90%ac%e7%94%a8%e6%88%b7%e8%af%b7%e6%b1%82&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;p&gt;监听的任务是由&lt;code&gt;kube-apiserver&lt;/code&gt;这个组件来完成的，它实际上是一个WEB服务，一般是以双向TLS认证方式启动的，所以在启动时需要提供证书、私钥、客户端的CA证书和CA私钥，当然也支持HTTP的方式，启动后就开始监听用户请求了。&lt;/p&gt;&#xA;&lt;h3 id=&#34;12-对请求分类&#34;&gt;1.2 对请求分类&lt;a class=&#34;anchor&#34; href=&#34;#12-%e5%af%b9%e8%af%b7%e6%b1%82%e5%88%86%e7%b1%bb&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;code&gt;kube-apiserver&lt;/code&gt;中的WEB服务在启动时注册了很多的Handler，golang中的Handler相当于Java中的servlet或者是Spring中的Controller，是对某一业务逻辑的封装，通俗点说，一个Handler负责对一个URI请求的处理，而在&lt;code&gt;kube-apiserver&lt;/code&gt;中，Handler被封装成了一个叫Store的对象，怎么封装的呢？比如&lt;code&gt;/api/v1/namespaces/{namespace}/pods&lt;/code&gt;这个URI对应了一个叫&lt;code&gt;PodStorage&lt;/code&gt;的Store，这个Store中包含了对&lt;code&gt;/api/v1/namespaces/{namespace}/pods&lt;/code&gt;的多个Handler，这些Handler有的是处理创建请求，有的是处理删除请求等等，代表了对一种资源的操作集。&lt;/p&gt;&#xA;&lt;p&gt;我们来看看这个Store的定义：&#xA;&lt;code&gt;staging/src/k8s.io/apiserver/pkg/registry/generic/registry/store.go&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;type Store struct {&#xA;    CreateStrategy rest.RESTCreateStrategy&#xA;    AfterCreate ObjectFunc&#xA;    UpdateStrategy rest.RESTUpdateStrategy&#xA;    AfterUpdate ObjectFunc&#xA;    DeleteStrategy rest.RESTDeleteStrategy&#xA;    AfterDelete ObjectFunc&#xA;...&#xA;}&#xA;&#xA;func (e *Store) Create(ctx genericapirequest.Context, obj runtime.Object, includeUninitialized bool) (runtime.Object, error) {&#xA;&#xA;func (e *Store) Update(ctx genericapirequest.Context, name string, objInfo rest.UpdatedObjectInfo) (runtime.Object, bool, error) {&#xA;...&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;因为每种资源所需要的操作不一样，所以Store中只包含了基本的通用的操作，作为一个基础类。&lt;/p&gt;&#xA;&lt;h3 id=&#34;13-处理请求&#34;&gt;1.3 处理请求&lt;a class=&#34;anchor&#34; href=&#34;#13-%e5%a4%84%e7%90%86%e8%af%b7%e6%b1%82&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;p&gt;在Store的定义中有一个&lt;code&gt;Storage   storage.Interface&lt;/code&gt;字段，Store中的创建、更新、删除等操作，比如上面的&lt;code&gt;Create()&lt;/code&gt;函数中会调用这个对象中的&lt;code&gt;Create()&lt;/code&gt;方法，也就是说这个Storage对象包含了一组更低级的操作，可以看作是数据的持久化层，这些操作都是通用的，而Store可以用Storage中的功能组合出具有不同功能的控制层对象，也就是Store对象啊，，好吧我们距离真相又进了一步，那这个&lt;code&gt;Storage  storage.Interface&lt;/code&gt;对象又是怎样实现的呢？&lt;/p&gt;&#xA;&lt;h3 id=&#34;14-数据存储到etcd&#34;&gt;1.4 数据存储到ETCD&lt;a class=&#34;anchor&#34; href=&#34;#14-%e6%95%b0%e6%8d%ae%e5%ad%98%e5%82%a8%e5%88%b0etcd&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;code&gt;Storage&lt;/code&gt;字段是一个叫&lt;code&gt;Interface&lt;/code&gt;的类型，里面定义了一些数据持久层的操作，这里就不贴出来了，我们更关心它的实现，我们先来看看Interface实例的创建吧，它的创&#xA;建工作是由一个工厂类负责的：&#xA;&lt;code&gt;staging/src/k8s.io/apiserver/pkg/storage/storagebackend/factory/factory.go&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;func Create(c storagebackend.Config) (storage.Interface, DestroyFunc, error) {&#xA;        switch c.Type {&#xA;        case storagebackend.StorageTypeETCD2:&#xA;                return newETCD2Storage(c)&#xA;        case storagebackend.StorageTypeUnset, storagebackend.StorageTypeETCD3:&#xA;                return newETCD3Storage(c)&#xA;        default:&#xA;                return nil, nil, fmt.Errorf(&amp;#34;unknown storage type: %s&amp;#34;, c.Type)&#xA;        }&#xA;}&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;好吧，看到这里就彻底明白了，根据配置中指定的存储服务创建不同的Storage对象，并且目前只支持ETCD2和ETCD3两种存储服务，持久层所做的增删改查就是对ETCD中&#xA;数据的增删改查，总结一下Storage对象也是就Interface被创建的过程：&lt;/p&gt;</description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:1313/kubernetes/k8s-code-scheduler-start.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/kubernetes/k8s-code-scheduler-start.html</guid>
      <description>&lt;h1 id=&#34;k8s源码分析-scheduler&#34;&gt;k8s源码分析-scheduler&lt;a class=&#34;anchor&#34; href=&#34;#k8s%e6%ba%90%e7%a0%81%e5%88%86%e6%9e%90-scheduler&#34;&gt;#&lt;/a&gt;&lt;/h1&gt;&#xA;&lt;p&gt;本文以kubenetes v1.7为例，说明&lt;code&gt;kube-scheduler&lt;/code&gt;组件的启动流程与工作原理。&lt;/p&gt;&#xA;&lt;h2 id=&#34;入口&#34;&gt;入口&lt;a class=&#34;anchor&#34; href=&#34;#%e5%85%a5%e5%8f%a3&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;scheduler的main函数定义在&lt;code&gt;plugin/cmd/kube-scheduler/scheduler.go&lt;/code&gt;中，main函数代码也比较清晰：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;创建默认配置对象&lt;/li&gt;&#xA;&lt;li&gt;将配置对象的指针传给命令行解析器，然后命令行解析器把解析到的各选项的值写入到配置对象中&lt;/li&gt;&#xA;&lt;li&gt;如果用户指定了&lt;code&gt;version&lt;/code&gt;选项则打印版本信息并退出&lt;/li&gt;&#xA;&lt;li&gt;将配置对象传给&lt;code&gt;Run()&lt;/code&gt;函数，然后就开始启动Scheduler了&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;创建客户端&#34;&gt;创建客户端&lt;a class=&#34;anchor&#34; href=&#34;#%e5%88%9b%e5%bb%ba%e5%ae%a2%e6%88%b7%e7%ab%af&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;kubeClient, leaderElectionClient, err := createClients(s)&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;Run()&lt;/code&gt;函数一开始先创建了一个kubernetes的客户端，用来连接&lt;code&gt;kube-apiserver&lt;/code&gt;组件以获取集群信息，这个客户端对象会被包含在Scheduler对象中。&lt;/p&gt;&#xA;&lt;h2 id=&#34;创建缓存更新器&#34;&gt;创建缓存更新器&lt;a class=&#34;anchor&#34; href=&#34;#%e5%88%9b%e5%bb%ba%e7%bc%93%e5%ad%98%e6%9b%b4%e6%96%b0%e5%99%a8&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;informerFactory := informers.NewSharedInformerFactory(kubeClient, 0)&#xA;podInformer := factory.NewPodInformer(kubeClient, 0)&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;然后又根据客户端创建出来两个&lt;code&gt;Informer&lt;/code&gt;对象，它跟客户端在一个包中，其作用是允许用户提供一些事件监听器（watcher），然后它有一个Run方法，启动以后会一直循环从&lt;code&gt;kube-apiserver&lt;/code&gt;中查询我们想要的信息，比如节点状态、新增Pod等等，如果有变化就会触发我们注册的相应的监听器对应的动作，然后本地有一个缓存对象，用来存放这些查询到的信息，这时只是创建，它们的Run方法还没有被调用。&lt;/p&gt;&#xA;&lt;h2 id=&#34;创建scheduler&#34;&gt;创建Scheduler&lt;a class=&#34;anchor&#34; href=&#34;#%e5%88%9b%e5%bb%bascheduler&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;sched, err := CreateScheduler(&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Scheduler对象的创建与另外两个对象密切相关，一个是Config，它与Scheduler定义在一个文件中：&lt;code&gt;plugin/pkg/scheduler/scheduler.go&lt;/code&gt;，另一个是ConfigFactory，定义在&lt;code&gt;plugin/pkg/scheduler/factory/factory.go&lt;/code&gt;，它们的关系大概为：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;ConfigFactory的主要工作是维护本地已缓存调度资源，比如等待调度的Pod、已调度的Pod、集群节点列表、PV/PVC列表等，并由&lt;code&gt;Informer&lt;/code&gt;循环地从&lt;code&gt;apiserver&lt;/code&gt;中把资源更新到本地，当然还包括向队列增删改查的函数，这些函数由&lt;code&gt;Informer&lt;/code&gt;提供。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Scheduler是对ConfigFactory的高级抽象，相对包含的函数少一些，因为它封装出了更高级的功能，使用起来更简单。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;而Config是Scheduler中的一个字段，Config没有函数只有一些字段，主要的作用是包含了Scheduler运行时需要的资源，这个Config对象包含的元素如下：&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;type Config struct {&#xA;&#x9;SchedulerCache schedulercache.Cache&#xA;&#x9;Ecache     *core.EquivalenceCache&#xA;&#x9;NodeLister algorithm.NodeLister&#xA;&#x9;Algorithm  algorithm.ScheduleAlgorithm&#xA;&#x9;Binder     Binder&#xA;&#x9;PodConditionUpdater PodConditionUpdater&#xA;&#x9;PodPreemptor PodPreemptor&#xA;&#x9;NextPod func() *v1.Pod&#xA;&#x9;WaitForCacheSync func() bool&#xA;&#x9;Error func(*v1.Pod, error)&#xA;&#x9;Recorder record.EventRecorder&#xA;&#x9;StopEverything chan struct{}&#xA;}&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;其中&lt;code&gt;Algorithm&lt;/code&gt;字段自然就是调度算法了，&lt;code&gt;NodeLister&lt;/code&gt;字段表示集群中所有节点的列表，&lt;code&gt;Binder&lt;/code&gt;用来将指定Pod绑定到某一主机上。&lt;/p&gt;&#xA;&lt;h2 id=&#34;默认调度算法&#34;&gt;默认调度算法&lt;a class=&#34;anchor&#34; href=&#34;#%e9%bb%98%e8%ae%a4%e8%b0%83%e5%ba%a6%e7%ae%97%e6%b3%95&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;默认的调度算法对象在&lt;code&gt;pkg/scheduler/factory/factory.go:CreateFromKeys()&lt;/code&gt;函数中被创建：&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;algo := core.NewGenericScheduler(&#xA;&#x9;c.schedulerCache,&#xA;&#x9;c.equivalencePodCache,&#xA;&#x9;c.podQueue,&#xA;&#x9;predicateFuncs,&#xA;&#x9;predicateMetaProducer,&#xA;&#x9;priorityConfigs,&#xA;&#x9;priorityMetaProducer,&#xA;&#x9;extenders,&#xA;&#x9;c.volumeBinder,&#xA;&#x9;c.pVCLister,&#xA;&#x9;c.alwaysCheckAllPredicates,&#xA;&#x9;c.disablePreemption,&#xA;)&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;所以调度一个Pod的具体实现就在这个&lt;code&gt;genericScheduler&lt;/code&gt;结构体中定义了，关键函数是它的&lt;code&gt;Schedule()&lt;/code&gt;函数了：&lt;/p&gt;</description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:1313/kubernetes/k8s-cri.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/kubernetes/k8s-cri.html</guid>
      <description>&lt;h1 id=&#34;k8s-cri&#34;&gt;k8s CRI&lt;a class=&#34;anchor&#34; href=&#34;#k8s-cri&#34;&gt;#&lt;/a&gt;&lt;/h1&gt;&#xA;&lt;h2 id=&#34;cricontainer-runtime-interface&#34;&gt;CRI（container runtime interface）&lt;a class=&#34;anchor&#34; href=&#34;#cricontainer-runtime-interface&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;在1.5以前的版本中，k8s依赖于dokcer，为了与docker解耦并支持更多的容器运行时，比如rkt、containerd，kubelet从1.5开始加入了CRI，作为k8s和容器运行时通信的标准，CRI是一组rpc接口，也就是说只要是实现了这组接口都可以作为kubelet的运行时，另外在k8s内部将之前的Pod抽象为一种更为通用的SandBox。&lt;/p&gt;&#xA;&lt;h3 id=&#34;接口定义&#34;&gt;接口定义&lt;a class=&#34;anchor&#34; href=&#34;#%e6%8e%a5%e5%8f%a3%e5%ae%9a%e4%b9%89&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;v1.7 &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/release-1.7/pkg/kubelet/apis/cri/v1alpha1/runtime&#34;&gt;pkg/kubelet/apis/cri/v1alpha1/runtime&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;v1.11 &lt;a href=&#34;https://github.com/kubernetes/kubernetes/tree/release-1.11/pkg/kubelet/apis/cri/runtime/v1alpha2&#34;&gt;pkg/kubelet/apis/cri/runtime/v1alpha2&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;接口实现&#34;&gt;接口实现&lt;a class=&#34;anchor&#34; href=&#34;#%e6%8e%a5%e5%8f%a3%e5%ae%9e%e7%8e%b0&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;p&gt;提出了CRI标准以后，意味着在新的版本里需要使用新的连接方式与docker通信，也就是说docker端需要按CRI的标准实现一个rpc的服务端，所以为了兼容以前的版本，不改变用户习惯，k8s提供了针对docker的CRI实现，也就是k8s源码中kubelet包下的dockershim包，dockershim是一个rpc服务，监听一个端口供kubelet连接，dockershim收到kubelet的请求后，将其转化为REST API请求，发送给物理机上的docker daemon，以下是创建和启动dockershim的代码（k8s v1.11版本）：&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;pkg/kubelet/kubelet.go:NewMainKubelet():617&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;// 创建dockershim&#xA;ds, err := dockershim.NewDockerService(kubeDeps.DockerClientConfig, crOptions.PodSandboxImage, streamingConfig,&#xA;...&#xA;// 启动rpc服务&#xA;if err := server.Start(); err != nil {&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;创建Docker客户端的逻辑是在创建dockershim的过程中，关键代码：&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;pkg/kubelet/dockershim/libdocker/client.go:100&lt;/code&gt;：&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;client, err := getDockerClient(dockerEndpoint)&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;client对象就是docker的客户端，包含了我们常用的&lt;code&gt;docker  run&lt;/code&gt;，&lt;code&gt;docker images&lt;/code&gt;等所有操作，其中&lt;code&gt;dockerEndpoint&lt;/code&gt;变量就是kubelet启动参数&lt;code&gt;--container-runtime-endpoint&lt;/code&gt;选项的值，默认是&lt;code&gt;unix:///var/run/docker.sock&lt;/code&gt;。&lt;/p&gt;&#xA;&lt;p&gt;k8s调用容器运行时过程如下：&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;kubelet -&amp;gt; remote -&amp;gt; CRI -&amp;gt; dockershim -&amp;gt; docker_client -&amp;gt; docker_daemon&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;在Github上阅读完整的源码：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;CRI客户端实现（remote包）： &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/release-1.11/pkg/kubelet/remote&#34;&gt;pkg/kubelet/remote&lt;/a&gt;，与容器相关的逻辑主要在&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/release-1.11/pkg/kubelet/remote/remote_runtime.go&#34;&gt;remote_runtime.go&lt;/a&gt;文件中。&lt;/li&gt;&#xA;&lt;li&gt;CRI服务端实现（dockershim包）： &lt;a href=&#34;https://github.com/kubernetes/kubernetes/tree/release-1.11/pkg/kubelet/dockershim&#34;&gt;pkg/kubelet/dockershim&lt;/a&gt;，与容器相关的逻辑主要在&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/release-1.11/pkg/kubelet/dockershim/docker_container.go&#34;&gt;docker_container.go&lt;/a&gt;文件中。&lt;/li&gt;&#xA;&lt;li&gt;dockershim调用Docker的逻辑： &lt;a href=&#34;https://github.com/kubernetes/kubernetes/tree/release-1.11/pkg/kubelet/dockershim/libdocker&#34;&gt;pkg/kubelet/dockershim/libdocker&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;kubelet相关参数&#34;&gt;kubelet相关参数&lt;a class=&#34;anchor&#34; href=&#34;#kubelet%e7%9b%b8%e5%85%b3%e5%8f%82%e6%95%b0&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;//指定资源管理驱动&#xA;--runtime-cgroups cgroupfs&#xA;//指定容器运行时&#xA;--container-runtime docker&#xA;//指定docker daemon的地址&#xA;--docker-endpoint unix:///var/run/docker.sock&#xA;//创建dockershim服务，供kubelet连接&#xA;--container-runtime-endpoint unix:///var/run/dockershim.sock&#xA;--image-service-endpoint unix:///var/run/dockershim.sock&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;相关资料&#34;&gt;相关资料&lt;a class=&#34;anchor&#34; href=&#34;#%e7%9b%b8%e5%85%b3%e8%b5%84%e6%96%99&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;http://localhost:1313/articles/kubernetes/k8s-code-kubelet&#34;&gt;kubelet源码分析&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;http://localhost:1313/articles/kubernetes/k8s-code-pod-create&#34;&gt;k8s创建Pod的流程&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:1313/kubernetes/k8s-dashboard-with-heapster.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/kubernetes/k8s-dashboard-with-heapster.html</guid>
      <description>&lt;h1 id=&#34;k8s-仪表盘与性能指标&#34;&gt;k8s 仪表盘与性能指标&lt;a class=&#34;anchor&#34; href=&#34;#k8s-%e4%bb%aa%e8%a1%a8%e7%9b%98%e4%b8%8e%e6%80%a7%e8%83%bd%e6%8c%87%e6%a0%87&#34;&gt;#&lt;/a&gt;&lt;/h1&gt;&#xA;&lt;h2 id=&#34;安装heapster&#34;&gt;安装Heapster&lt;a class=&#34;anchor&#34; href=&#34;#%e5%ae%89%e8%a3%85heapster&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;需要注意的地方就是镜像，如果官方的不能下载，可以选择国内的，共三个：&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;docker pull kxdmmr/heapster-influxdb-amd64:v1.3.3&#xA;docker pull kxdmmr/heapster-grafana-amd64:v4.4.3&#xA;docker pull kxdmmr/heapster-amd64:v1.4.2&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;然后去官方下载三个yaml文件：&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/heapster/tree/master/deploy/kube-config/influxdb&#34;&gt;https://github.com/kubernetes/heapster/tree/master/deploy/kube-config/influxdb&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;还有一个关于RBAC的：&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/heapster/tree/master/deploy/kube-config/rbac&#34;&gt;https://github.com/kubernetes/heapster/tree/master/deploy/kube-config/rbac&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;修改文件内的镜像地址，然后通过kubectl create一次性创建它们。&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[root@node10 dashboard]# kubectl -n kube-system get pods&#xA;NAME                                        READY     STATUS    RESTARTS   AGE&#xA;heapster-7f776d4686-8nfz2                   1/1       Running   0          12d&#xA;monitoring-grafana-64768ccd78-4tgmd         1/1       Running   0          12d&#xA;monitoring-influxdb-84774b9644-z9m28        1/1       Running   0          12d&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;安装dashboard&#34;&gt;安装Dashboard&lt;a class=&#34;anchor&#34; href=&#34;#%e5%ae%89%e8%a3%85dashboard&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;h3 id=&#34;下载yaml&#34;&gt;下载yaml&lt;a class=&#34;anchor&#34; href=&#34;#%e4%b8%8b%e8%bd%bdyaml&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;p&gt;下载&lt;a href=&#34;https://github.com/kubernetes/dashboard&#34;&gt;官方&lt;/a&gt;的yaml文件：&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;mkdir dashboard &amp;amp;&amp;amp; cd dashboard&#xA;curl -O https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;修改镜像地址&#34;&gt;修改镜像地址&lt;a class=&#34;anchor&#34; href=&#34;#%e4%bf%ae%e6%94%b9%e9%95%9c%e5%83%8f%e5%9c%b0%e5%9d%80&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;p&gt;如果你不能下载官方的镜像，可以选择国内镜像，将yaml文件内&lt;code&gt;image: k8s.gcr&lt;/code&gt;这一行改为&lt;code&gt;image: kxdmmr/kubernetes-dashboard-amd64:v1.8.1&lt;/code&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;挂载证书到pod中&#34;&gt;挂载证书到pod中&lt;a class=&#34;anchor&#34; href=&#34;#%e6%8c%82%e8%bd%bd%e8%af%81%e4%b9%a6%e5%88%b0pod%e4%b8%ad&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;p&gt;在默认的启动参数中指定了自动生成证书&lt;code&gt;--auto-generate-certificates&lt;/code&gt;，但实际并不能正常提供https服务，我们可以指定为自己的证书。&lt;/p&gt;&#xA;&lt;p&gt;我们打算将证书以Secret的方法挂载到Pod中，所以先将证书和私钥编码成base64，至于证书和私钥的选择，你可以专门为dashboard生成一对，也可以用已经存在的，这里我用安装集群时的一对密钥作为示例：&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;less /etc/kubernetes/ssl/kubernetes.pem | base64 -w 0&#xA;less /etc/kubernetes/ssl/kubernetes-key.pem | base64 -w 0&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;然后将得到的两个base64码放入yaml文件中的Secret中，给它增加一个data字段，如下：&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;apiVersion: v1&#xA;kind: Secret&#xA;metadata:&#xA;  labels:&#xA;    k8s-app: kubernetes-dashboard&#xA;  name: kubernetes-dashboard-certs&#xA;  namespace: kube-system&#xA;type: Opaque&#xA;data:&#xA;  certfile: EZ6VVcwZWx0NC94Z.........(省略)&#xA;  keyfile: LRVktLNCS0tLE9Qo=.........(省略)&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;引用证书&#34;&gt;引用证书&lt;a class=&#34;anchor&#34; href=&#34;#%e5%bc%95%e7%94%a8%e8%af%81%e4%b9%a6&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;p&gt;这里需要修改dashboard的启动参数，也就是Deployment部分:&#xA;&lt;code&gt;Deployment.spec.template.spec.containers.args&lt;/code&gt;：&lt;/p&gt;</description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:1313/kubernetes/k8s-expose-service.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/kubernetes/k8s-expose-service.html</guid>
      <description>&lt;h1 id=&#34;k8s-暴露服务&#34;&gt;k8s 暴露服务&lt;a class=&#34;anchor&#34; href=&#34;#k8s-%e6%9a%b4%e9%9c%b2%e6%9c%8d%e5%8a%a1&#34;&gt;#&lt;/a&gt;&lt;/h1&gt;&#xA;&lt;p&gt;在k8s集群中向外暴露服务目前官方支持两种方式：NodePort和Ingress，前者简单粗暴，而后者更高级优雅，那是不是前者就可以被抛弃了？至少目前来看不是的，它们都各有优势和不足，通常情况下需要两者配合使用来满足所有场景，必要时还需要修改自己的业务模块来支持k8s平台，下面我们从使用方法上和优缺点方面细说这两种方案。&lt;/p&gt;&#xA;&lt;h2 id=&#34;nodeport&#34;&gt;NodePort&lt;a class=&#34;anchor&#34; href=&#34;#nodeport&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;h3 id=&#34;原理&#34;&gt;原理&lt;a class=&#34;anchor&#34; href=&#34;#%e5%8e%9f%e7%90%86&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;p&gt;这种方法其实类似于&lt;code&gt;docker run&lt;/code&gt;命令中的&lt;code&gt;-p&lt;/code&gt;选项，只不过在Kubernetes中用&lt;code&gt;kube-proxy&lt;/code&gt;组件代替了Docker的&lt;code&gt;-p&lt;/code&gt;的功能，并且是定义在&lt;code&gt;service&lt;/code&gt;中，其原理是通过操作系统的&lt;code&gt;iptables&lt;/code&gt;来将物理机上指定端口的数据转发到对应的Pod内。&lt;/p&gt;&#xA;&lt;h3 id=&#34;使用&#34;&gt;使用&lt;a class=&#34;anchor&#34; href=&#34;#%e4%bd%bf%e7%94%a8&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;p&gt;下面是定义一个NodePort类型的Service：&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;apiVersion: v1&#xA;kind: Service&#xA;metadata:&#xA;  name: spark-driver&#xA;spec:&#xA;  type: NodePort&#xA;  externalIPs:&#xA;  - &amp;#34;10.100.100.100&amp;#34;&#xA;  ports:&#xA;  - port: 7070&#xA;    name: master&#xA;  - port: 4040&#xA;    name: appui&#xA;  selector:&#xA;    app: spark-driver-pod&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;10.100.100.100&lt;/code&gt;是k8s集群中某节点的物理IP地址，然后我们就可以通过&lt;code&gt;10.100.100.100:4040&lt;/code&gt;访问到&lt;code&gt;app: spark-driver-pod&lt;/code&gt;这个Pod的4040端口。&lt;/p&gt;&#xA;&lt;h3 id=&#34;限制&#34;&gt;限制&lt;a class=&#34;anchor&#34; href=&#34;#%e9%99%90%e5%88%b6&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;p&gt;很快你就会发现这种方式会有一个问题，当我们在k8s中创建了多个一样的服务，并且都需要访问它们的4040端口和7070端口时，我们需要为每个服务都绑定一个物理IP，而集群中的节点是有限的，很快就没有IP可以用了，这时候有两种变通的方法：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;把上面yaml文件定义中的4040改为其它端口，比如：4041、4042，由于每个服务映射出来的端口不一样，这时需要我们通过其它手段把这些信息记录下来，用户才知道怎样访问自己创建的服务，做这些事是需要工作量的，而且端口也并不是无限的。&lt;/li&gt;&#xA;&lt;li&gt;第二方法是增加IP，虽然节点是有限的，但如果你的集群使用的IP段是16位的话，还是有很多IP可以用的，方法就是在其中一个k8s子节点上增加子IP，比如几百个、几千个、几万个，，这个方法其实已经在我的前几篇文章中提到过了，具体操作可以参考我的&lt;a href=&#34;http://localhost:1313/articles/linux/linux-centos7-multi-ip&#34;&gt;这篇文章&lt;/a&gt;。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;ingress&#34;&gt;Ingress&lt;a class=&#34;anchor&#34; href=&#34;#ingress&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;上面说的两种变通方法其实有点“旁门左道”的味道了，而k8s官方在1.1及以上版本中提供了更优雅的方式来解决这个问题，那就是&lt;code&gt;Ingress&lt;/code&gt;，它是以插件的方式存在的，并且默认是没有安装的。&lt;/p&gt;&#xA;&lt;h3 id=&#34;原理-1&#34;&gt;原理&lt;a class=&#34;anchor&#34; href=&#34;#%e5%8e%9f%e7%90%86-1&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;p&gt;Ingress是通过在k8s集群中启动一个或多个nginx服务，然后通过k8s的REST API从&lt;code&gt;kube-apiserver&lt;/code&gt;中监控&lt;code&gt;Endpoint&lt;/code&gt;的变化来动态修改这个nginx的配置文件，将不同的请求转发给相应的Service来完成数据转发，这样说起来可能比较抽象，下面我们将用一个demo来说明它的工作原理。&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;img/k8s-expose-service/k8s-ingress-principle.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;安装&#34;&gt;安装&lt;a class=&#34;anchor&#34; href=&#34;#%e5%ae%89%e8%a3%85&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;p&gt;Ingress的安装还是比较简单的，官方的安装文档在&lt;a href=&#34;https://github.com/kubernetes/ingress-nginx/blob/master/deploy/README.md&#34;&gt;这里&lt;/a&gt;，大概可以分为三步：&lt;/p&gt;&#xA;&lt;p&gt;1、下载yaml文件&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;mkdir ingress &amp;amp;&amp;amp; cd ingress&#xA;&#xA;curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/namespace.yaml&#xA;curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/default-backend.yaml&#xA;curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/configmap.yaml&#xA;curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/tcp-services-configmap.yaml&#xA;curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/udp-services-configmap.yaml&#xA;curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/rbac.yaml&#xA;curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/with-rbac.yaml&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;上面每个命令都涉及一个yaml文件，其中&lt;code&gt;default-backend.yaml&lt;/code&gt;和&lt;code&gt;with-rbac.yaml&lt;/code&gt;两个文件中涉及到两个在国外的镜像，你可能下载不下来，这时需要改为国内镜像：&lt;/p&gt;</description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:1313/kubernetes/k8s-principle.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/kubernetes/k8s-principle.html</guid>
      <description>&lt;h1 id=&#34;k8s-基本原理&#34;&gt;k8s 基本原理&lt;a class=&#34;anchor&#34; href=&#34;#k8s-%e5%9f%ba%e6%9c%ac%e5%8e%9f%e7%90%86&#34;&gt;#&lt;/a&gt;&lt;/h1&gt;&#xA;&lt;h2 id=&#34;k8s原理解析&#34;&gt;k8s原理解析&lt;a class=&#34;anchor&#34; href=&#34;#k8s%e5%8e%9f%e7%90%86%e8%a7%a3%e6%9e%90&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;kubernetes由多个模块组成，一般情况下，除去插件和依赖我们需要部署5个组件和一个静态工具，分别为：kube-apiserver、kube-scheduler、kube-controller-manager、kubelet、kube-proxy、kubectl，我们逐一讲解它们的作用及原理。&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;img/k8s-principle/k8s-architecture.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;kube-apiserver&#34;&gt;kube-apiserver&lt;a class=&#34;anchor&#34; href=&#34;#kube-apiserver&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;p&gt;一个WEB服务，运行在主节点，它向外提供了很多的REST API，比如：新删改Pod、Service、Deployments，添加节点，提供对REST API的访问权限控制等。&lt;/p&gt;&#xA;&lt;p&gt;一般运行一个WEB服务都会依赖一个数据库，用来持久化WEB的状态，apiserver也不例外，它的运行需要依赖一个键值对存储服务，用来存储集群中所有信息，如：Pod状态、Node状态、已创建的Service、已创建的Endpoint，创建的Deployments、Statefulsets等，目前(v1.7.2)它只支持ETCD2、ETCD3两种存储服务，所以在部署apiserver前需要先部署一个ETCD集群，且默认地、它与apiserver之间的通信是TLS协议的，所以在apiserver和ETCD的启动参数中都会指定一对证书和密钥，其它所有组件与apiserver进行通信时也都是TLS方式。&lt;/p&gt;&#xA;&lt;h3 id=&#34;kube-scheduler&#34;&gt;kube-scheduler&lt;a class=&#34;anchor&#34; href=&#34;#kube-scheduler&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;p&gt;一个提供Pod调度功能和调度算法的组件，运行在主节点，它的工作原理是不断地从apiserver监听是否有新创建的Pod，或者说已创建但还没有进行调度的Pod，如果有则将该Pod放入本地队列，然后用指定的算法为集群中所有Node进行打分，将Pod绑定到得分最高的Node上，绑定其实就是将Node的Hostname写入到Pod对象对应的字段中，然后将Pod信息写回到apiserver中，至此调度就算完成了。&lt;/p&gt;&#xA;&lt;p&gt;scheduler启动时可以指定调度算法，默认的算法名为&amp;quot;default-scheduler&amp;quot;，原理是遍历所有Node，用Node的剩余内存和CPU作为权重，计算出每个Node的分数，如果有多个第一名则随机选其中一个第一名。&lt;/p&gt;&#xA;&lt;h3 id=&#34;kube-controller-menager&#34;&gt;kube-controller-menager&lt;a class=&#34;anchor&#34; href=&#34;#kube-controller-menager&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;p&gt;负责管理其它scale-controller，运行在主节点，我们创建Deployments、Statefulsets等资源的时候会有一个相应的scale-controller被创建出来，用来监控该资源的副本数是否与预期数量相同。&lt;/p&gt;&#xA;&lt;h3 id=&#34;kubelet&#34;&gt;kubelet&lt;a class=&#34;anchor&#34; href=&#34;#kubelet&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;p&gt;做实际部署工作的组件，运行在每个子节点上，原理是所有kubelet不断从apiserver检测已绑定主机但还未部署的Pod，如果这个Pod是绑这定在自己主机上的则将其部署，然后通过REST API将Pod状态更新到apiserver中。除此之外它还要发送心跳给apiserver并汇报自身状态。&lt;/p&gt;&#xA;&lt;h3 id=&#34;kube-proxy&#34;&gt;kube-proxy&lt;a class=&#34;anchor&#34; href=&#34;#kube-proxy&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;p&gt;与kubelet一起被部署在每个子节点上，负责从&lt;code&gt;api-server&lt;/code&gt;中监听service和endpoint资源，并在物理机上通过iptables或ipvs为Pod设置端口转发、负载均衡以及从service到Pod的数据转发。&lt;/p&gt;&#xA;&lt;h3 id=&#34;kubectl&#34;&gt;kubectl&lt;a class=&#34;anchor&#34; href=&#34;#kubectl&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;p&gt;一个k8s客户端，可以部署在任何地方，它的运行需要依赖一个配置文件，用来提供apiserver的地址、端口、证书等信息，以下是一个创建Deployments的示例：&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;vim spark.yaml&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;apiVersion: apps/v1beta1&#xA;kind: Deployment&#xA;metadata:&#xA;  name: spark-executor&#xA;spec:&#xA;  replicas: 10&#xA;  template:&#xA;    metadata:&#xA;      labels:&#xA;        app: spark-executor&#xA;    spec:&#xA;      containers:&#xA;      - name: spark-executor&#xA;        image: registry.io:5000/leap/spark:latest&#xA;        imagePullPolicy: Always&#xA;        resources:&#xA;          requests:&#xA;            memory: &amp;#34;2G&amp;#34;&#xA;            cpu: &amp;#34;5&amp;#34;&#xA;          limits:&#xA;            memory: &amp;#34;2G&amp;#34;&#xA;            cpu: &amp;#34;5&amp;#34;&#xA;        ports:&#xA;        - containerPort: 8081&#xA;          name: port-1&#xA;        env:&#xA;        - name: DEBUG&#xA;          value: &amp;#34;__DEBUG&amp;#34;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;kubectl -n p48-u26-jiajun2 create spark.yml&lt;/code&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:1313/kubernetes/kubeedge-code.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/kubernetes/kubeedge-code.html</guid>
      <description>&lt;h1 id=&#34;kubeedge源码分析&#34;&gt;KubeEdge源码分析&lt;a class=&#34;anchor&#34; href=&#34;#kubeedge%e6%ba%90%e7%a0%81%e5%88%86%e6%9e%90&#34;&gt;#&lt;/a&gt;&lt;/h1&gt;&#xA;&lt;p&gt;本文以1.2.1版本为例，项目结构说明：&lt;a href=&#34;https://github.com/kubeedge/kubeedge/blob/v1.2.1/README_zh.md&#34;&gt;kubeedge&lt;/a&gt;。&lt;/p&gt;&#xA;&lt;h2 id=&#34;边缘端模块&#34;&gt;边缘端模块&lt;a class=&#34;anchor&#34; href=&#34;#%e8%be%b9%e7%bc%98%e7%ab%af%e6%a8%a1%e5%9d%97&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;h3 id=&#34;edgehub&#34;&gt;EdgeHub&lt;a class=&#34;anchor&#34; href=&#34;#edgehub&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;p&gt;模块位置：&lt;code&gt;edge/pkg/edgehub&lt;/code&gt;&#xA;该模块用于与CloudHub通信，目前支持两种方式与CloudHub建立连接：QUIC协议和WebSocket协议，无论使用哪种方式都会使用TLS加密。&lt;/p&gt;&#xA;&lt;h4 id=&#34;使用quic方式&#34;&gt;使用QUIC方式&lt;a class=&#34;anchor&#34; href=&#34;#%e4%bd%bf%e7%94%a8quic%e6%96%b9%e5%bc%8f&#34;&gt;#&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;p&gt;关键代码&lt;a href=&#34;github.com/kubeedge/kubeedge/blob/v1.2.1/edge/pkg/edgehub/clients/quicclient/quicclient.go#L75&#34;&gt;quicclient.go:75&lt;/a&gt;：&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;func (qcc *QuicClient) Init() error {&#xA;    // ... 省略多行 ...&#xA;    client := qclient.NewQuicClient(option, exOpts)&#xA;    // ... 省略多行 ...&#xA;}&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;上面的&lt;code&gt;NewQuicClient()&lt;/code&gt;函数其实是调用第三方包（quic-go）建立多路复用的UDP连接与CloudHub端通信&lt;a href=&#34;github.com/lucas-clemente/quic-go/blob/v0.10.2/client.go#L86&#34;&gt;client.go:86&lt;/a&gt;：&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;func DialAddrContext(ctx context.Context, addr string, tlsConf *tls.Config, config *Config,) (Session, error) {&#xA;    // ... 省略多行 ...&#xA;    udpConn, err := net.ListenUDP(&amp;#34;udp&amp;#34;, &amp;amp;net.UDPAddr{IP: net.IPv4zero, Port: 0})&#xA;    return dialContext(ctx, udpConn, udpAddr, addr, tlsConf, config, true)&#xA;}&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;使用websocket方式&#34;&gt;使用WebSocket方式&lt;a class=&#34;anchor&#34; href=&#34;#%e4%bd%bf%e7%94%a8websocket%e6%96%b9%e5%bc%8f&#34;&gt;#&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;p&gt;关键代码&lt;a href=&#34;github.com/kubeedge/kubeedge/blob/v1.2.1/edge/pkg/edgehub/clients/wsclient/websocket.go#L47&#34;&gt;websocket.go:75&lt;/a&gt;：&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;func (qcc *QuicClient) Init() error {&#xA;    // ... 省略多行 ...&#xA;    client := &amp;amp;wsclient.Client{Options: option, ExOpts: exOpts}&#xA;    // ... 省略多行 ...&#xA;}&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:1313/kubernetes/kubernetes-quick-deloy-17.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/kubernetes/kubernetes-quick-deloy-17.html</guid>
      <description>&lt;h1 id=&#34;k8s-快速部署-17&#34;&gt;k8s 快速部署 1.7&lt;a class=&#34;anchor&#34; href=&#34;#k8s-%e5%bf%ab%e9%80%9f%e9%83%a8%e7%bd%b2-17&#34;&gt;#&lt;/a&gt;&lt;/h1&gt;&#xA;&lt;p&gt;最近（2017.07.30）k8s 又发布了新版本，这个版本中增加了两种持久化存储方案，StorygeOS 与 Local ，为了一探究竟，我部署了一套最新版的 k8s 集群，虽然我在半年前部署过一次，并写过一篇 k8s-v1.5 版本部署的文篇，但那次的经验已经不再适用于新版本，这让我又体验了一次 k8s 的部署过程，这简直是种折磨，我现在新重新整理出来，希望给读者带来帮助。&lt;/p&gt;&#xA;&lt;h2 id=&#34;环境说明&#34;&gt;环境说明&lt;a class=&#34;anchor&#34; href=&#34;#%e7%8e%af%e5%a2%83%e8%af%b4%e6%98%8e&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;这次同样是用的 k8s 官方提供的快速安装工具 &lt;code&gt;kubeadm&lt;/code&gt;，这种方式依然被官方视为实验性功能，并不建议用在生产环境之中，先说明一下我的环境。&lt;/p&gt;&#xA;&lt;p&gt;服务器三台：node1.docker.com, node2.docker.com, node3.docker.com&lt;/p&gt;&#xA;&lt;p&gt;操作系统：CentOS 7.2-1511 64位 Docker 版本：1.12.6&lt;/p&gt;&#xA;&lt;p&gt;Kubernetes 版本：1.7.0&lt;/p&gt;&#xA;&lt;p&gt;部署方式：官方提供的 kubeadm 工具&lt;/p&gt;&#xA;&lt;h2 id=&#34;准备-linux&#34;&gt;准备 Linux&lt;a class=&#34;anchor&#34; href=&#34;#%e5%87%86%e5%a4%87-linux&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;关于系统，我用的是 CentOS7.2，官方说的是只要 CentOS7 就可以，如果是 Ubuntu 的话，版本要在 Ubuntu 16.04 或以上，或者是 HypriotOS v1.0.1+ 系统，不过我没用过这个系统。。&lt;/p&gt;&#xA;&lt;h2 id=&#34;安装-docker&#34;&gt;安装 Docker&lt;a class=&#34;anchor&#34; href=&#34;#%e5%ae%89%e8%a3%85-docker&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;所有节点都需要做这个步骤，这里要提醒大家一下，如果你按照 Docker 官网给出的方式，通过官方提供的 repo 源安装了最新版的 Docker，最好卸载掉，然后重新安装 1.12.x 版本的 Docker，否则后导致后面初始化 K8s 时失败，我已经在这个环节上浪费掉很多时间了，，k8s 官方说 Docker-1.13.x 也是可以的，但我没有试过，，所以正确的安装姿势是，用 CentosOS 7.2 自带的 repo 源来安装，如果你的系统中有 /etc/yum.repos.d/docker.repo 这个文件的话，请移除，然后执行：&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;yum install docker-1.12.6 -y&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如果你的系统自带的源中没有找到 docker，那可以尝试从 Docker 官网安装，但一定要是 1.12.x 的 Docker 版本，请参考&lt;a href=&#34;https://docs.docker.com/engine/installation/linux/docker-ce/centos/#install-using-the-repository&#34;&gt;Docker 官方文档&lt;/a&gt;。&lt;/p&gt;</description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:1313/kubernetes/kubernetes-quick-deloy.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/kubernetes/kubernetes-quick-deloy.html</guid>
      <description>&lt;h1 id=&#34;k8s-快速部署-15&#34;&gt;k8s 快速部署 1.5&lt;a class=&#34;anchor&#34; href=&#34;#k8s-%e5%bf%ab%e9%80%9f%e9%83%a8%e7%bd%b2-15&#34;&gt;#&lt;/a&gt;&lt;/h1&gt;&#xA;&lt;p&gt;本篇文章可能已不再适用于最新版本的 k8s，您可以看我的另一篇文章：&lt;a href=&#34;http://localhost:1313/articles/kubernetes/kubernetes-quick-deloy-17&#34;&gt;Kubernetes - 快速部署(1.7)&lt;/a&gt;。&lt;/p&gt;&#xA;&lt;p&gt;k8s 即 kubernetes，它是一个由谷歌开源的容器管理框架，提供完善的容器管理功能，如：Docker 容器编排，服务发现，状态监视等，据说它融合了谷歌多年的容器运营经验，所以目前为止，在容器管理界它是最成熟的，但它并不容易使用，比起 Docker 自带的容器管理框架 Swarm 要复杂的多，有人称 Kubernetes 的集群部署是地狱级的，这并不夸张。 Kubernetes 在 1.5 版本以后，谷歌简化了它的部署流程，小编看了官网的介绍，只需要在各个机器上执行一两条命令就可以搭建一个 k8s 集群，所以赶紧小试了一把，然而实际上并没那么容易，今天就记录在此。下面我们就来搭建一个 k8s 集群，并安装一个 WEB UI 应用（Dashboard）做为示例。&lt;/p&gt;&#xA;&lt;h2 id=&#34;环境描述&#34;&gt;环境描述&lt;a class=&#34;anchor&#34; href=&#34;#%e7%8e%af%e5%a2%83%e6%8f%8f%e8%bf%b0&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;三台机器：node1.docker.com, node2.docker.com, node3.docker.com&lt;/li&gt;&#xA;&lt;li&gt;操作系统：CentOS 7.2&lt;/li&gt;&#xA;&lt;li&gt;k8s 版本：v1.5.1&lt;/li&gt;&#xA;&lt;li&gt;好多地方需要用 root 权限，所以笔者在这里直接用 root 用户了&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;在每台机器上安装-k8s&#34;&gt;在每台机器上安装 k8s&lt;a class=&#34;anchor&#34; href=&#34;#%e5%9c%a8%e6%af%8f%e5%8f%b0%e6%9c%ba%e5%99%a8%e4%b8%8a%e5%ae%89%e8%a3%85-k8s&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;这一步的难点在于相关资 源的下载，这些镜像都是在谷歌的服务器上，国内不能下载，这里有几个选择，一是用加速器，二是在网上找一下，看有没有其它人共享下载好的 k8s 与相关镜像，有的话最好，三是购买几台国外的服务器来部署 k8s，当然还有其它方式，相信这难不倒诸位，笔者推荐第一种。&lt;/p&gt;&#xA;&lt;p&gt;以下是本文用到的所有镜像，如果你用上述的第二种方式，请提前下载好这些镜像：&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[root@node1 ~]# docker images &#xA;REPOSITORY                                                   TAG                 IMAGE ID            CREATED             SIZE&#xA;docker.io/weaveworks/weave-npc                               1.9.2               6d47d7ef52cf        3 days ago          58.23 MB&#xA;docker.io/weaveworks/weave-kube                              1.9.2               c187d4ccbf10        3 days ago          163.2 MB&#xA;gcr.io/google_containers/kube-proxy-amd64                    v1.5.3              932ee3606ada        2 weeks ago         173.5 MB&#xA;gcr.io/google_containers/kube-scheduler-amd64                v1.5.3              cb0ce9bb60f9        2 weeks ago         54 MB&#xA;gcr.io/google_containers/kube-controller-manager-amd64       v1.5.3              25304c6f1bb2        2 weeks ago         102.8 MB&#xA;gcr.io/google_containers/kube-apiserver-amd64                v1.5.3              93d8b30a8f27        2 weeks ago         125.9 MB&#xA;gcr.io/google_containers/kubernetes-dashboard-amd64          v1.5.1              1180413103fd        7 weeks ago         103.6 MB&#xA;gcr.io/google_containers/etcd-amd64                          3.0.14-kubeadm      856e39ac7be3        3 months ago        174.9 MB&#xA;gcr.io/google_containers/kubedns-amd64                       1.9                 26cf1ed9b144        3 months ago        47 MB&#xA;gcr.io/google_containers/dnsmasq-metrics-amd64               1.0                 5271aabced07        4 months ago        14 MB&#xA;gcr.io/google_containers/kube-dnsmasq-amd64                  1.4                 3ec65756a89b        5 months ago        5.126 MB&#xA;gcr.io/google_containers/kube-discovery-amd64                1.0                 c5e0c9a457fc        5 months ago        134.2 MB&#xA;gcr.io/google_containers/exechealthz-amd64                   1.2                 93a43bfb39bf        5 months ago        8.375 MB&#xA;gcr.io/google_containers/pause-amd64                         3.0                 99e59f495ffa        10 months ago       746.9 kB&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;解决了下载问题就开始安装 k8s 各组件了，因为笔者是 CentOS 系统，执行以下命令安装：&lt;/p&gt;</description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:1313/kubernetes/popular-bgp-protocol.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/kubernetes/popular-bgp-protocol.html</guid>
      <description>&lt;h1 id=&#34;白话bgp协议&#34;&gt;白话BGP协议&lt;a class=&#34;anchor&#34; href=&#34;#%e7%99%bd%e8%af%9dbgp%e5%8d%8f%e8%ae%ae&#34;&gt;#&lt;/a&gt;&lt;/h1&gt;&#xA;&lt;h2 id=&#34;什么是bgp&#34;&gt;什么是BGP&lt;a class=&#34;anchor&#34; href=&#34;#%e4%bb%80%e4%b9%88%e6%98%afbgp&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;引用维基百科对BGP的描述:&lt;/p&gt;&#xA;&lt;blockquote class=&#39;book-hint &#39;&gt;&#xA;&lt;p&gt;边界网关协议（英文：Border Gateway Protocol, BGP）是互联网上一个核心的去中心化自治路由协议。 它通过维护IP路由表或&amp;rsquo;前缀&amp;rsquo;表来实现自治系统（AS）之间的可达性，属于矢量路由协议。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;BGP是一种通信协议、一组规范、一种解决方案，通俗的说，它用来在多个路由器之间共享彼此的路由表信息，代替了人工维护各个路由器上的路由表，这在大的网络拓扑结构中是很有用的。&lt;/p&gt;&#xA;&lt;h2 id=&#34;为什么用bgp&#34;&gt;为什么用BGP&lt;a class=&#34;anchor&#34; href=&#34;#%e4%b8%ba%e4%bb%80%e4%b9%88%e7%94%a8bgp&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;我们通过一个例子来看看BGP到底解决了什么问题。&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;img/popular-bgp-protocol/bgp-protocol.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;&lt;p&gt;图中有两个路由器R1和R2，为了方便讨论，我将每个IP写成了1.1、2.2的形式，你也可以把它想象成1.1.1.1和2.2.2.2的形式，其中5.5是一个公网IP，其它均是公司内部规划的私有IP，红色的字母代表路由器上的接口，每个路由器旁边有一张表，表示了它们当前的路由信息，并且这几条路由信息是它们默认生成的。&lt;/p&gt;&#xA;&lt;p&gt;当主机2.2访问主机1.2时，报文先到达R2，这时由于R2的路由表中没有1.2的路由信息，所以报文会被转发到默认网关1.1，然后数据包从R2的A接口发出并到达R1，由于R1中有1.2的路由，所以报文可以到过1.2主机。&lt;/p&gt;&#xA;&lt;p&gt;当主机1.2访问主机2.2时，由于R1中没有2.2的路由，报文会被送到默认网关5.1，也就是Internet网，结果可想而知，数据包最终会因为TTL耗尽而被丢弃。&lt;/p&gt;&#xA;&lt;p&gt;要解决1.2不能访问2.2的问题，我们需要在R1上加一条路由，如下：&lt;/p&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;目标&lt;/th&gt;&#xA;          &lt;th&gt;网关&lt;/th&gt;&#xA;          &lt;th&gt;接口&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;2.2&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;C&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;p&gt;这样当报文到了R1以后，就会被R1从C接口发送到R2，R2再将报文转发给2.2。&lt;/p&gt;&#xA;&lt;p&gt;但是在网络拓扑非常复杂的情况下，会有很多的路由器以及成千上万的主机（这里不限于同一个公司内部的路由器和主机，更实际的情况是多个公司、多个网络提供商、多个自治网络(AS)组合起来的复杂网络），靠人工为每台路由器配置和维护路由表就会变得不实现。&lt;/p&gt;&#xA;&lt;p&gt;那能不能自动化完成这些事情呢？答案是可以的，例如上面我们为R1增加的路由信息的操作，实际上可以在所有路由器上各自启动一个服务序程序，让它们将自己的路由表通过TCP连接共享给其它所有路由器，其它路由器收到信息后进行分析，将有用的信息添加到自己的路由表中。这样的服务程序就是BGP，它的目标就是解决大型网络中的可达性信息的共享和管理问题。&lt;/p&gt;&#xA;&lt;p&gt;实际上，BGP协议被称为最复杂的网络协议之一，实现一个可用的BGP协议需要很多知识，在这里我也只能粗略说一下它的作用和基本原理，让你快速了解到BGP到底是什么，如果需要深入BGP可以查阅相关书籍，如《BGP设计与实现》。&lt;/p&gt;&#xA;&lt;h2 id=&#34;怎样实现bgp&#34;&gt;怎样实现BGP&lt;a class=&#34;anchor&#34; href=&#34;#%e6%80%8e%e6%a0%b7%e5%ae%9e%e7%8e%b0bgp&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;我接触过一个使用BGP实现的容器网络通信方案Calico，它把Linux节点当做路由器，在多个节点之间使用BGP协议共享路由信息，这个Calico项目是开源的，这篇Calico的文章在这里：&lt;a href=&#34;http://localhost:1313/articles/kubernetes/k8s-calico&#34;&gt;Calico网络&lt;/a&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>

<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>大数据 on 橡果笔记</title>
    <link>http://localhost:1313/bigdata/index.html</link>
    <description>Recent content in 大数据 on 橡果笔记</description>
    <generator>Hugo</generator>
    <language>en</language>
    <atom:link href="http://localhost:1313/bigdata/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title></title>
      <link>http://localhost:1313/bigdata/ambari-quick-build-bigdata-platform.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/bigdata/ambari-quick-build-bigdata-platform.html</guid>
      <description>&lt;h1 id=&#34;ambari---三条命令创建集群&#34;&gt;Ambari - 三条命令创建集群&lt;a class=&#34;anchor&#34; href=&#34;#ambari---%e4%b8%89%e6%9d%a1%e5%91%bd%e4%bb%a4%e5%88%9b%e5%bb%ba%e9%9b%86%e7%be%a4&#34;&gt;#&lt;/a&gt;&lt;/h1&gt;&#xA;&lt;h2 id=&#34;什么是-ambari&#34;&gt;什么是 Ambari&lt;a class=&#34;anchor&#34; href=&#34;#%e4%bb%80%e4%b9%88%e6%98%af-ambari&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;Ambari 是大数据生态圈中的一员，但它不是一个大数据计算引擎，而是一个管理工具，用来管理其它大数据工具，如：Hadoop、Spark、Hive 等，当你用 Ambari 作为你的管理工具时，它可以帮你监视你的集群状态、每个组件状态、节点状态等，如下图（这个不是 apache 官方版，所以请原谅我打点马赛克）：&#xA;&lt;img src=&#34;img/ambari-quick-build-bigdata-platform/ambari-quick-build-bigdata-platform_blue-introduce-1024x487.png&#34; alt=&#34;&#34; /&gt;&#xA;它还可以为你的集群增加或删除一个组件，比如，你的集群没有 Storm，而现在你可以通过 Ambari 来添加它，只需动一动鼠标即可完成。&lt;/p&gt;&#xA;&lt;h2 id=&#34;原理简介&#34;&gt;原理简介&lt;a class=&#34;anchor&#34; href=&#34;#%e5%8e%9f%e7%90%86%e7%ae%80%e4%bb%8b&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;它是一个经典的主从架构的分布式的软件，主要由 Ambari-Server 与 Ambari-Agent 两部分组成，架构图如下：&#xA;&lt;img src=&#34;img/ambari-quick-build-bigdata-platform/ambari-quick-build-bigdata-platform_arch.png&#34; alt=&#34;&#34; /&gt; 以上是用户为集群添加一个组件（Hbase）的过程，用户在前端进行的所有操作都是通过 REST API 调用后台 Ambari 的，我们可以用浏览器抓包看一下：&#xA;&lt;img src=&#34;img/ambari-quick-build-bigdata-platform/ambari-quick-build-bigdata-platform_blue-api-1024x481.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;ambari-bleprint-作用&#34;&gt;Ambari Bleprint 作用&lt;a class=&#34;anchor&#34; href=&#34;#ambari-bleprint-%e4%bd%9c%e7%94%a8&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;我们先来看一下用 Ambari 创建一个集群的步骤，当在一台机器上安装好 Ambari 后，进入它的 WEB UI，这时出现一个集群创建向导，如下：&#xA;&lt;img src=&#34;img/ambari-quick-build-bigdata-platform/ambari-quick-build-bigdata-platform_guide.png&#34; alt=&#34;&#34; /&gt;&#xA;这个向导还是很方便的，其中有添加主机，选择要安装的组件等功能。 而 Ambari Bleprint 可以看作是 Ambari 本身提供的一个功能，它的作用是：可以让用户免去 Ambari 的集群安装向导操作，可以让用户把安装集群的所有步骤脚本化，这是个很有用的场景，而它的基本原理就是通两个 JSON 文件来描述一个集群，如下：&#xA;&lt;img src=&#34;img/ambari-quick-build-bigdata-platform/docker-auto-test-product_json-768x456.png&#34; alt=&#34;&#34; /&gt;&#xA;图中左边是两个 JSON 文件，它们被通过 REST API 的方式发送给 Ambari-Server ，再由 Ambari-Server 创建出一个集群。&lt;/p&gt;&#xA;&lt;h2 id=&#34;ambari-bleprint-的使用&#34;&gt;Ambari Bleprint 的使用&lt;a class=&#34;anchor&#34; href=&#34;#ambari-bleprint-%e7%9a%84%e4%bd%bf%e7%94%a8&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;用 Ambari Bleprint 创建一个集群大概分为6步：&lt;/p&gt;</description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:1313/bigdata/bigdate-on-docker.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/bigdata/bigdate-on-docker.html</guid>
      <description>&lt;h1 id=&#34;bigdata-on-docker&#34;&gt;Bigdata on Docker&lt;a class=&#34;anchor&#34; href=&#34;#bigdata-on-docker&#34;&gt;#&lt;/a&gt;&lt;/h1&gt;&#xA;&lt;p&gt;最近我们在把自己的大数据平台迁移到 Docker 上面来，最初的想法是用 Docker 技术代替我们现在的 Open Stack 平台，也就是把 Docker 容器当成虚拟来用，看到这里可能人要说了：这明显没有遵循 Docker 的佳实践啊！是的，但具我了解，很多大数据厂商都在想办法把自家的平台容器化，而且有不少厂商已经做到了，也许这就是趋势吧，尽管 Docker 的设计者并不建议这样做，到后面我们会把大数据平台中的每一个组件一个个的拆开，每一个组件都容器化。现在我们已经实践有一段时间了，并且下一步打算用 Kubernetes 来管理我们的 Docker 集群，先来整理一下在 bigdata on docker 实践中遇到的问题，这些问题都是最实际的，正所谓：真正的工作在细节之中。也希望能给读者一条捷径。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;现在我们来把一个已有的大数据集群放在 Docker 环境中，我指的是重新搭一个集群而不是完整的挪过去。。&lt;/p&gt;&#xA;&lt;h2 id=&#34;系统环境&#34;&gt;系统环境&lt;a class=&#34;anchor&#34; href=&#34;#%e7%b3%bb%e7%bb%9f%e7%8e%af%e5%a2%83&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;大数据平台在整体架构上不会有什么变化，就是把物理机换成是 Docker 容器而已，那么容器从哪来？一般情况下你需要自己构建一个出来，比如你的大数据平台是基于 CentOS6.5 的，那就要自己动手&lt;a href=&#34;http://localhost:1313/articles/docker/docker-build-centos65&#34;&gt;构建一个基础镜像&lt;/a&gt;作为系统，这个镜像中可以包含你需要的一切依赖，如 jdk 和 python 等，然后用这个镜像创建指定数据的容器，比如我打算搭建一个 5 节点的集群那就创建 5 个容器出来，关于创建容器的参数下面再讲，因为还要涉及到网络和挂载之类的问题。&lt;/p&gt;&#xA;&lt;h2 id=&#34;同步镜像&#34;&gt;同步镜像&lt;a class=&#34;anchor&#34; href=&#34;#%e5%90%8c%e6%ad%a5%e9%95%9c%e5%83%8f&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;有了基础镜像之后，我们需要把这个镜像同步到所有物理机上，如果你如有一台物理机的话，可能不需要这一步，直接用 &lt;code&gt;docker pull docker.io/...&lt;/code&gt; 这样的方式就可以了，但如果你有几十台或者更多的物理机，那么你需要&lt;a href=&#34;http://localhost:1313/articles/docker/docker-registry-deploy-manage&#34;&gt;搭建一个私有仓库&lt;/a&gt;，把你的镜像 push 进入，然后其它机器执行 &lt;code&gt;docker pull&lt;/code&gt; 即可。&lt;/p&gt;&#xA;&lt;h2 id=&#34;网络&#34;&gt;网络&lt;a class=&#34;anchor&#34; href=&#34;#%e7%bd%91%e7%bb%9c&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;解决 Docker 的跨主机通信是比较重要的一环，好在 Docker 1.9 版本中加入了 Overlay 网络，官方给出来两种方案解决跨主机的通信问题，一个是创建 Swarm 集群，创建好后它会自带一个 Overlay 网络，然后在这个集群中用 &lt;code&gt;Docker service create&lt;/code&gt; 这样的方式创建一些容器，它们相互之间就可以直接通信了，但这个网络不能用 &lt;code&gt;Docker run --network&lt;/code&gt; 的方式使用。&lt;/p&gt;</description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:1313/bigdata/bigdate-on-kubernetes.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/bigdata/bigdate-on-kubernetes.html</guid>
      <description>&lt;h1 id=&#34;bigdata-on-kubernetes&#34;&gt;Bigdata on Kubernetes&lt;a class=&#34;anchor&#34; href=&#34;#bigdata-on-kubernetes&#34;&gt;#&lt;/a&gt;&lt;/h1&gt;&#xA;&lt;p&gt;把大数据平台容器化，是我们的必经之路，一是因为容器化有诸多好处，比如：将每个组件标准化、易于管理、快速升级、快速部署等等，二是因为趋势，当其它人做了而你没做，就可能被客户抛弃，所以最好在所有人之前就开始做。&lt;/p&gt;&#xA;&lt;h2 id=&#34;基础镜像&#34;&gt;基础镜像&lt;a class=&#34;anchor&#34; href=&#34;#%e5%9f%ba%e7%a1%80%e9%95%9c%e5%83%8f&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;把大数据平台容器化，首先得先把平台中的组件一个个拆开，然后每个组件（Hadoop, Spark等）的运行环境都有差别，比如 Hadoop 依赖 DJK7 而 Spark 依赖 JDK8 等，这就需要将组件和相应的依赖打包成在一起，但所有组件应该共用一个基础镜像，这个基础镜像很小，可能只有几十到几百 MB ，构建基础镜像&lt;a href=&#34;http://localhost:1313/articles/docker/docker-build-centos65&#34;&gt;看这里&lt;/a&gt;，有了基础镜像之后就可以在写每个 Dockerfile 时指定的你 FROM 参数。&lt;/p&gt;&#xA;&lt;h2 id=&#34;构建组件镜像&#34;&gt;构建组件镜像&lt;a class=&#34;anchor&#34; href=&#34;#%e6%9e%84%e5%bb%ba%e7%bb%84%e4%bb%b6%e9%95%9c%e5%83%8f&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;我们先用 hadoop-yarn 来举例编写一个 Dockerfile，如果我有自己的 yum 源，那么可以直接使用 &lt;code&gt;yum install hadoop-yarn&lt;/code&gt; 这样的方式来安装，下面是一个例子：&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;FROM centos6.5-base&#xA;&#xA;ADD  /jdk-7u79-linux-x64.tar.gz /opt&#xA;COPY /bigdata.repo /etc/yum.repos.d/bigdata.repo&#xA;&#xA;ENV JAVA_HOME=/opt/jdk1.7.0_79&#xA;&#xA;RUN yum install hadoop* -y &#xA;&#xA;COPY /bootstrap.sh /bootstrap.sh&#xA;&#xA;CMD /bootstrap.sh&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;其中 &lt;code&gt;bigdata.repo&lt;/code&gt; 是一个 yum 源文件，里面定义了 hadoop, spark, hbase 等组件的 rpm 源；&lt;code&gt;bootstrap.sh&lt;/code&gt; 是一个服务启动脚本，里面写了很重要的逻辑。&lt;/p&gt;&#xA;&lt;h2 id=&#34;镜像仓库&#34;&gt;镜像仓库&lt;a class=&#34;anchor&#34; href=&#34;#%e9%95%9c%e5%83%8f%e4%bb%93%e5%ba%93&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;有了 Yarn 镜像之后，我们需要将它放到私有仓库中，以便 Kubernetes 集群中的所有机器都能使用它，这时你需要&lt;a href=&#34;http://localhost:1313/articles/docker/docker-registry-deploy-manage&#34;&gt;搭建一个私有仓库&lt;/a&gt;，然后把所有组件的镜像 push 进去。&lt;/p&gt;</description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:1313/bigdata/spark-performance-optimiz.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/bigdata/spark-performance-optimiz.html</guid>
      <description>&lt;h1 id=&#34;spark-on-yarn---性能调优&#34;&gt;Spark on Yarn - 性能调优&lt;a class=&#34;anchor&#34; href=&#34;#spark-on-yarn---%e6%80%a7%e8%83%bd%e8%b0%83%e4%bc%98&#34;&gt;#&lt;/a&gt;&lt;/h1&gt;&#xA;&lt;p&gt;自从领导让我接手了 Docker 方面的工作，虽然每天还是会跟大数据接触，但主要精力已转到了 Docker 上，感觉自己离大数据越来越远了，今天写一篇关于 Spark 调优的文章，也算是复习一下。&lt;/p&gt;&#xA;&lt;h2 id=&#34;环境描述&#34;&gt;环境描述&lt;a class=&#34;anchor&#34; href=&#34;#%e7%8e%af%e5%a2%83%e6%8f%8f%e8%bf%b0&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;Spark 有多种运行模式，本次我们以 yarn-cluster 模式来运行，环境如下：&#xA;Spark 版本：2.0.0&#xA;Spark 运行模式：yarn-cluster&#xA;计算节点数：10&#xA;单节点内存大小：64g&#xA;单节点 CPU 核数：16&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;yarn配置&#34;&gt;Yarn配置&lt;a class=&#34;anchor&#34; href=&#34;#yarn%e9%85%8d%e7%bd%ae&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;即然用 Yarn 来管理资源，那么 Spark 最终运行性能也会受 Yarn 的影向，我们先来说明 Yanr 中几个关键配置参数。&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;yarn.nodemanager.resource.memory-mb = 61440&lt;/code&gt;：&#xA;这个参数指每个（假设我们所有节点配置都是一样的）物理机要供献出多少内存给 Yarn，每个节点有 64G，减去给系统预留的 2G，NodeManager 和 DataNode 各占用 1G，如果没有其它服务的话，还剩下 60G，最终得出 60 * 1024 = 61440 ， 这只是个参考，实际上在系统开机以后已经没有 64G 了，而且每个节点上还不止这两个服务，如 Hbase、impala、logstash 等等，所以还要考虑这些服务所占用的资源。&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;yarn.nodemanager.resource.cpu-vcores = 15&lt;/code&gt;：&#xA;这个值指单个节点要供献出多少CPU给Ynar来调度，同样要减去系统与其它服务所占用的核数，我们就以15为例了。 另外关于这个值还有个说法，比如物理机是12核，减去其它服务占用的2个，那么在配置里面写的时候就是15 * 1.5 = 22 或者 10 * 2 = 30，而我试过以后，觉得性能并没有明显变化。 这时，Yarn的总资源： 总内存：61440M * 10个节点 = 614400M 总核数：15核 * 10个节点 = 150核&lt;/p&gt;</description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:1313/bigdata/spark-performance-test.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/bigdata/spark-performance-test.html</guid>
      <description>&lt;h1 id=&#34;spark---性能测试&#34;&gt;Spark - 性能测试&lt;a class=&#34;anchor&#34; href=&#34;#spark---%e6%80%a7%e8%83%bd%e6%b5%8b%e8%af%95&#34;&gt;#&lt;/a&gt;&lt;/h1&gt;&#xA;&lt;p&gt;本文介绍一种测试 Spark 性能的方法，也可以用来测试 Spark SQL 模块对 SQL 语句的覆盖率，主要用 TPC-DS 这个工具，希望可以帮到一些同学。&lt;/p&gt;&#xA;&lt;h2 id=&#34;什么是-tpc-ds&#34;&gt;什么是 TPC-DS&lt;a class=&#34;anchor&#34; href=&#34;#%e4%bb%80%e4%b9%88%e6%98%af-tpc-ds&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;TPC-DS 是事务性能管理委员会 (TPC) 发布的大数据测试基准&lt;/li&gt;&#xA;&lt;li&gt;它可以生成 99 个测试案例，遵循 SQL&#39;99 和 SQL 2003 的语法标准，SQL 案例比较复杂&lt;/li&gt;&#xA;&lt;li&gt;分析的数据量大，并且测试案例是在回答真实的商业问题&lt;/li&gt;&#xA;&lt;li&gt;测试案例中包含各种业务模型（如分析报告型，迭代式的联机分析型，数据挖掘型等）&lt;/li&gt;&#xA;&lt;li&gt;几乎所有的测试案例都有很高的 IO 负载和 CPU 计算需求&lt;/li&gt;&#xA;&lt;li&gt;它有常用的两个功能:&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;按指定规模生成测试数据&lt;/li&gt;&#xA;&lt;li&gt;按指定的 SQL 标准生成测试用的 99 条 SQL 语句&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;基本原理&#34;&gt;基本原理&lt;a class=&#34;anchor&#34; href=&#34;#%e5%9f%ba%e6%9c%ac%e5%8e%9f%e7%90%86&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;首先用 TPC-DS 生成测试数据与 99 条 SQL 语句，然后将数据导入 Hive，配置 Spark 使其能够访问到 Hive 元数据，这时启用 Spark 自身的 spark thrift server 服务，如果没有了解过，&lt;a href=&#34;http://localhost:1313/articles/spark/spark-thrift-mode&#34;&gt;可以参考这里&lt;/a&gt;，向这个 Server 发送 SQL 语句，最后获取其执行时间并记录下来。&lt;/p&gt;&#xA;&lt;h2 id=&#34;生成数据与sql语句&#34;&gt;生成数据与sql语句&lt;a class=&#34;anchor&#34; href=&#34;#%e7%94%9f%e6%88%90%e6%95%b0%e6%8d%ae%e4%b8%8esql%e8%af%ad%e5%8f%a5&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;下载最新版 tpc-ds-tool.zip, 放在 /root/modules 目录下，官网地址：www.tpc.org&lt;/p&gt;</description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:1313/bigdata/spark-source-code-1.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/bigdata/spark-source-code-1.html</guid>
      <description>&lt;h1 id=&#34;spark---源码分析一&#34;&gt;Spark - 源码分析（一）&lt;a class=&#34;anchor&#34; href=&#34;#spark---%e6%ba%90%e7%a0%81%e5%88%86%e6%9e%90%e4%b8%80&#34;&gt;#&lt;/a&gt;&lt;/h1&gt;&#xA;&lt;p&gt;最近抽时间读了下 Spark 源码，最初只是想看看它是怎样跟 HDFS 交互的，每个 task 怎样在它所在的机器上找到及读取数据块的，这一看就是好几天，罢了，那就从头到尾研究一下吧，准备每周更新一篇。在这里记录的只是我所理解的东西，这样一个庞大的软件，如果没有当初的设计图纸是很难梳理清楚其全部原理的，我就看到哪写到哪吧。&lt;/p&gt;&#xA;&lt;h2 id=&#34;spark简介&#34;&gt;Spark简介&lt;a class=&#34;anchor&#34; href=&#34;#spark%e7%ae%80%e4%bb%8b&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;Spark 是一个基于内存的大数据处理框架，官方称 Spark 比M apReduce 的处理速度快 10 到 100 倍。它最初在 2009 年由加州大学伯克利分校的 AMPLab 开发，并于 2010 年成为 Apache 的开源项目之一。&lt;/p&gt;&#xA;&lt;p&gt;Spark版本：2.1.0&lt;/p&gt;&#xA;&lt;p&gt;部署模式：Spark on Yarn&lt;/p&gt;&#xA;&lt;p&gt;我们就从提交一个任开始，看看Spark都做了些什么，先写一个简单的Spark任务，如下：&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;package com.example&#xA; &#xA;import org.apache.spark.{SparkContext,SparkConf}&#xA; &#xA;object SparkCode {&#xA;   &#xA;  def main(args: Array[String]): Unit = {&#xA;    val spark = SparkSession&#xA;      .builder&#xA;      .appName(&amp;#34;Spark Pi&amp;#34;)&#xA;      .getOrCreate()&#xA;     &#xA;    spark.sparkContext.textFile(&amp;#34;hdfs:///spark/input/file.txt&amp;#34;, 3)&#xA;      .map { x =&amp;gt; (x, 1)}&#xA;      .reduceByKey(_ + _)&#xA;      .saveAsTextFile(&amp;#34;hdfs:///spark/output&amp;#34;)&#xA;     &#xA;    spark.stop()&#xA;  }&#xA;   &#xA;}&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;将上面代码打成Jar包，然后通过 spark-submit 命令提交这个任务，提交以后它在干什么？&lt;/p&gt;</description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:1313/bigdata/spark-source-code-2.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/bigdata/spark-source-code-2.html</guid>
      <description>&lt;h1 id=&#34;spark---源码分析二&#34;&gt;Spark - 源码分析（二）&lt;a class=&#34;anchor&#34; href=&#34;#spark---%e6%ba%90%e7%a0%81%e5%88%86%e6%9e%90%e4%ba%8c&#34;&gt;#&lt;/a&gt;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;小节scala中的闭包&#34;&gt;小节：Scala中的闭包&lt;a class=&#34;anchor&#34; href=&#34;#%e5%b0%8f%e8%8a%82scala%e4%b8%ad%e7%9a%84%e9%97%ad%e5%8c%85&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;p&gt;本篇开始之前，先说一下闭包的概念，一会要用到，在 Scala 中的写一个闭包是很容易的，甚至比 js 还简单，如下：&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;object Test {  &#xA;   var base = 3 &#xA;   val myFunc = (i:Int) =&amp;gt; i * base  &#xA;   def main(args: Array[String]) {  &#xA;      println(myFunc(5))  &#xA;   }  &#xA;}&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如上的 myFunc 函数依赖了一个外部变量 base，也就是说 base 成为了 myFunc 函数的一部分，因为 myFunc 函数被其它地方引用，所以使得 base 变量也不会被GC回收，这样就形成了一个闭包。&lt;/p&gt;&#xA;&lt;h2 id=&#34;提交job&#34;&gt;提交Job&lt;a class=&#34;anchor&#34; href=&#34;#%e6%8f%90%e4%ba%a4job&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;上篇中讲到了，假如我们在 RDD 对象上调用了产生宽依赖的算子时，那么该算子将最终会调用 SparkContext 类中的 runJob 方法，本篇中我们接着这个 runJob 方法继续往下看，先回顾下调用 runJob 的地方，以下是RDD类中的一个算子：&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;...&#xA;  def foreach(f: T =&amp;gt; Unit): Unit = withScope {&#xA;    val cleanF = sc.clean(f)&#xA;    sc.runJob(this, (iter: Iterator[T]) =&amp;gt; iter.foreach(cleanF))&#xA;  }&#xA;...&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;参数列表中的 f，就是我们在调用 foreach 的时候，自己写的匿名函数，在这里它被另一个匿名函数引用，这样就形成了一个闭包，最后将这个闭包函数传递给了 runJob 方法。以下是 runJob 方法的原型：&lt;/p&gt;</description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:1313/bigdata/spark-source-code-3.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/bigdata/spark-source-code-3.html</guid>
      <description>&lt;h1 id=&#34;spark---源码分析三&#34;&gt;Spark - 源码分析（三）&lt;a class=&#34;anchor&#34; href=&#34;#spark---%e6%ba%90%e7%a0%81%e5%88%86%e6%9e%90%e4%b8%89&#34;&gt;#&lt;/a&gt;&lt;/h1&gt;&#xA;&lt;h2 id=&#34;生成task&#34;&gt;生成Task&lt;a class=&#34;anchor&#34; href=&#34;#%e7%94%9f%e6%88%90task&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;上篇中我们讲到Spark在提交一个Stage后，DAGScheduler中的&lt;code&gt;submitStage()&lt;/code&gt;方法会以递归的方式找到该Stage依赖的最上层的父Stage，找到后会将这个最上层的Stage传给&lt;code&gt;submitMissingTasks()&lt;/code&gt;方法，该方法定义如下：&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;...&#xA;  /** Called when stage&amp;#39;s parents are available and we can now do its task. */&#xA;  private def submitMissingTasks(stage: Stage, jobId: Int) {&#xA;    logDebug(&amp;#34;submitMissingTasks(&amp;#34; + stage + &amp;#34;)&amp;#34;)&#xA;    ...&#xA;    runningStages += stage&#xA;    stage match {&#xA;      case s: ShuffleMapStage =&amp;gt;&#xA;        outputCommitCoordinator.stageStart(stage = s.id, maxPartitionId = s.numPartitions - 1)&#xA;      case s: ResultStage =&amp;gt;&#xA;        outputCommitCoordinator.stageStart(&#xA;          stage = s.id, maxPartitionId = s.rdd.partitions.length - 1)&#xA;    }&#xA;    val taskIdToLocations: Map[Int, Seq[TaskLocation]] = try {&#xA;      stage match {&#xA;        case s: ShuffleMapStage =&amp;gt;&#xA;          partitionsToCompute.map { id =&amp;gt; (id, getPreferredLocs(stage.rdd, id))}.toMap&#xA;        case s: ResultStage =&amp;gt;&#xA;          partitionsToCompute.map { id =&amp;gt;&#xA;            val p = s.partitions(id)&#xA;            (id, getPreferredLocs(stage.rdd, p))&#xA;          }.toMap&#xA;      }&#xA;    ...&#xA;    taskBinary = sc.broadcast(taskBinaryBytes)&#xA;...&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;上面只贴出了该方法的一部分，我们刚才说的父Stage被传进来后，做了几件事情，我们分步骤说一下：&lt;/p&gt;</description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:1313/bigdata/spark-source-code-4.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/bigdata/spark-source-code-4.html</guid>
      <description>&lt;h1 id=&#34;spark--源码分析四&#34;&gt;Spark – 源码分析（四）&lt;a class=&#34;anchor&#34; href=&#34;#spark--%e6%ba%90%e7%a0%81%e5%88%86%e6%9e%90%e5%9b%9b&#34;&gt;#&lt;/a&gt;&lt;/h1&gt;&#xA;&lt;h2 id=&#34;task-的执行&#34;&gt;Task 的执行&lt;a class=&#34;anchor&#34; href=&#34;#task-%e7%9a%84%e6%89%a7%e8%a1%8c&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;上篇讲到TaskRunner类，这个类是定义在Executor类中，实现了Java的Runnable接口，Spark运行过程中的一个Task就是一个TaskRunner实例，下面是它的 &lt;code&gt;run()&lt;/code&gt; 方法&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;  override def run(): Unit = {&#xA;    threadId = Thread.currentThread.getId&#xA;    Thread.currentThread.setName(threadName)&#xA;    val threadMXBean = ManagementFactory.getThreadMXBean&#xA;    val taskMemoryManager = new TaskMemoryManager(env.memoryManager, taskId)&#xA;    val deserializeStartTime = System.currentTimeMillis()&#xA;    val deserializeStartCpuTime = if (threadMXBean.isCurrentThreadCpuTimeSupported) {&#xA;      threadMXBean.getCurrentThreadCpuTime&#xA;    } else 0L&#xA;    Thread.currentThread.setContextClassLoader(replClassLoader)&#xA;    val ser = env.closureSerializer.newInstance()&#xA;    logInfo(s&amp;#34;Running $taskName (TID $taskId)&amp;#34;)&#xA;    execBackend.statusUpdate(taskId, TaskState.RUNNING, EMPTY_BYTE_BUFFER)&#xA;    var taskStart: Long = 0&#xA;    var taskStartCpu: Long = 0&#xA;    startGCTime = computeTotalGcTime()&#xA; &#xA;    try {&#xA;      // Must be set before updateDependencies() is called, in case fetching dependencies&#xA;      // requires access to properties contained within (e.g. for access control).&#xA;      Executor.taskDeserializationProps.set(taskDescription.properties)&#xA; &#xA;      updateDependencies(taskDescription.addedFiles, taskDescription.addedJars)&#xA;      task = ser.deserialize[Task[Any]](&#xA;        taskDescription.serializedTask, Thread.currentThread.getContextClassLoader)&#xA;      task.localProperties = taskDescription.properties&#xA;      task.setTaskMemoryManager(taskMemoryManager)&#xA;...&#xA;      // Run the actual task and measure its runtime.&#xA;      taskStart = System.currentTimeMillis()&#xA;      taskStartCpu = if (threadMXBean.isCurrentThreadCpuTimeSupported) {&#xA;        threadMXBean.getCurrentThreadCpuTime&#xA;      } else 0L&#xA;      var threwException = true&#xA;      val value = try {&#xA;        val res = task.run(&#xA;          taskAttemptId = taskId,&#xA;          attemptNumber = taskDescription.attemptNumber,&#xA;          metricsSystem = env.metricsSystem)&#xA;        threwException = false&#xA;        res&#xA;      } finally {&#xA;        val releasedLocks = env.blockManager.releaseAllLocksForTask(taskId)&#xA;        val freedMemory = taskMemoryManager.cleanUpAllAllocatedMemory()&#xA; &#xA;        if (freedMemory &amp;gt; 0 &amp;amp;&amp;amp; !threwException) {&#xA;          val errMsg = s&amp;#34;Managed memory leak detected; size = $freedMemory bytes, TID = $taskId&amp;#34;&#xA;          if (conf.getBoolean(&amp;#34;spark.unsafe.exceptionOnMemoryLeak&amp;#34;, false)) {&#xA;            throw new SparkException(errMsg)&#xA;          } else {&#xA;            logWarning(errMsg)&#xA;          }&#xA;        }&#xA;...&#xA;      val resultSer = env.serializer.newInstance()&#xA;      val beforeSerialization = System.currentTimeMillis()&#xA;      val valueBytes = resultSer.serialize(value)&#xA;      val afterSerialization = System.currentTimeMillis()&#xA;...&#xA;      execBackend.statusUpdate(taskId, TaskState.FINISHED, serializedResult)&#xA;...&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;看起来有点长，只贴出部分，在这里分步总结一下：&lt;/p&gt;</description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:1313/bigdata/spark-source-code-5.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/bigdata/spark-source-code-5.html</guid>
      <description>&lt;h1 id=&#34;spark---源码分析图&#34;&gt;Spark - 源码分析（图）&lt;a class=&#34;anchor&#34; href=&#34;#spark---%e6%ba%90%e7%a0%81%e5%88%86%e6%9e%90%e5%9b%be&#34;&gt;#&lt;/a&gt;&lt;/h1&gt;&#xA;&lt;p&gt;最近抽时间画了张Spark的流程图，帮助理解Spark架构，供参考，点击图片可以放大。&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;img/spark-source-code-5/spark-source-code-5_flow_chart.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:1313/bigdata/spark-thrift-mode.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/bigdata/spark-thrift-mode.html</guid>
      <description>&lt;h1 id=&#34;spark---thrift-server模式&#34;&gt;Spark - Thrift Server模式&lt;a class=&#34;anchor&#34; href=&#34;#spark---thrift-server%e6%a8%a1%e5%bc%8f&#34;&gt;#&lt;/a&gt;&lt;/h1&gt;&#xA;&lt;p&gt;Spark Thrift Server 其实是 Spark SQL 的一种运行方式，笔者前段时间用它来做测试了 Spark 的性能，以及 Spark SQL 对 SQL 语句的支持情况，用起来还是十分方便的，后来在网上一搜，除了官网很少有写 Spark Thrift Server 的文章，那么本文将让你了解并且会使用它。&lt;/p&gt;&#xA;&lt;h2 id=&#34;spark-thrift-server-运行原理&#34;&gt;Spark Thrift Server 运行原理&lt;a class=&#34;anchor&#34; href=&#34;#spark-thrift-server-%e8%bf%90%e8%a1%8c%e5%8e%9f%e7%90%86&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;即然它被称为 Server，那它就是提供某种服务的，它提供的服务就是解析 SQL 语句，然后将解析完的 SQL 转换为 Spark 任务，并提交给 Spark 集群去执行（这里要取决于 Spark 的运行方式，如：Spark on Yarn、Spark Standalone 等），最后捕获运行结果。&lt;/p&gt;&#xA;&lt;h2 id=&#34;配置一个可用的-spark-thrift-server&#34;&gt;配置一个可用的 Spark Thrift Server&lt;a class=&#34;anchor&#34; href=&#34;#%e9%85%8d%e7%bd%ae%e4%b8%80%e4%b8%aa%e5%8f%af%e7%94%a8%e7%9a%84-spark-thrift-server&#34;&gt;#&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;h3 id=&#34;前提准备&#34;&gt;前提准备&lt;a class=&#34;anchor&#34; href=&#34;#%e5%89%8d%e6%8f%90%e5%87%86%e5%a4%87&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;我们为以三台机器的作为示例，分别为：host1、host2、host3&lt;/li&gt;&#xA;&lt;li&gt;在三个节点上部署好 Hadoop2.7 集群，并启动 Hdfs、Yarn&lt;/li&gt;&#xA;&lt;li&gt;在 host1 上部署好 Hive1.2.1，并且可以运行&lt;/li&gt;&#xA;&lt;li&gt;在 host1 上安装 Spark2.0，先不用配置&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;修改-spark-envsh&#34;&gt;修改 spark-env.sh&lt;a class=&#34;anchor&#34; href=&#34;#%e4%bf%ae%e6%94%b9-spark-envsh&#34;&gt;#&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;p&gt;要使用 Spark Thrift Server，需要用到三个配置文件，我们一个一个来配置 首先是 Spark 安装目录下的 &lt;code&gt;conf/spark-env.sh&lt;/code&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>

[{"id":0,"href":"/algorithm/algorithm-collaborative-filtering.html","title":"推荐算法 - 协同过滤","section":"数据结构与算法","content":"推荐算法 - 协同过滤# 协同过滤是一种推荐算法，它是一种解决特定问题的思路，而非一种固定算法，所以它可以有多种实现，各种实现略有差异，本文将用通俗的方式帮助你了解它的原理，并给出一个应用的实例。\n理解协同过滤# 假如我有一个跟我品味相似的朋友，大多数时候，我喜欢听的歌他也喜欢，这时就可以把那些我喜欢的而他没有听过的歌，推荐给他，反之，他也可以推荐给我。 这里有两个关键指标，一个是需要知道我对每首歌的喜欢程度，还有就是哪些人与我品味相似。 图解一下：\n上表有ABC三人，与XYZ三首歌，数字表示用户给歌曲的评分（满分为10），也表示用户对歌曲的喜欢程度，例如：用户A对歌曲X的喜欢程度为9。而用户A与用户B可以认为是品味相似，因为用户AB同时喜欢歌曲X，且同时不太喜欢歌曲Y。这时就可以将用户A喜欢的歌曲Z推荐给用户B。\n计算用户相关性# 关于这个问题，网上大多都提到了“欧几里得距离”以及“皮尔逊相关系数”这两个方法，相信这两种方法的相应实现，网上也能找到一大把，不过我想以另一种方法来说明一下，还是以上面的表为例，我们以每首歌为单为，求出所有用户对每首歌的评价的相似性，如下：\n所有用户对歌曲X评价的相似性： 计算规则：用户A对X的评分为9，用户B对X的评分为8，求出它们的差的绝对值，也就是1，值越小相关性越强。依此类推求出剩下人的相关性。灰色部分的值是重覆的，因此略过。\n所有用户对歌曲Y评价的相似性： 将上面两张表合并，也就是将他们的值合并起来： 这张表上的值表示：A与B的相关性最强，B与C其次，A与C相关性则最差。 当然，我们可以将上表中的结果提取出来做个排序，这样当A有了喜欢的新歌时，根据每个用户与A的相关性排序，应优先推荐给B，其次是C。而当C有了喜欢的新歌时，优先推荐给B，其次才是A。\n使用Spark MLLib中的ALS算法# 上面只是帮助你理解协同过滤的核心思想，而实际的实现有多种，而且也比较复杂，不过在Spark的机器学习库中已经有相应的实现，也就是ALS算法，我们只需提供数据集，可以很容易的使用，下面就来演示一下它的用法。\n生成测试数据# package com.algorithm.matrix import scala.util.Random import java.io.PrintWriter /** * 此类用来生成一些测试数据并写入到指定的文件中 */ object GenFile { //模拟歌手列表 val artists = Array(\u0026#34;李健\u0026#34;,\u0026#34;吉克隽逸\u0026#34;,\u0026#34;吴莫愁\u0026#34;,\u0026#34;杨坤\u0026#34;,\u0026#34;宋祖英\u0026#34;,\u0026#34;罗大佑\u0026#34;,\u0026#34;龙梅子\u0026#34;, \u0026#34;水木年华\u0026#34;,\u0026#34;小沈阳\u0026#34;,\u0026#34;谭晶\u0026#34;,\u0026#34;蔡健雅\u0026#34;,\u0026#34;刘佳\u0026#34;,\u0026#34;王蓉\u0026#34;,\u0026#34;黄龄\u0026#34;,\u0026#34;庞麦郎\u0026#34;,\u0026#34;钟汉良\u0026#34;) //模拟用户列表 val user = Array(\u0026#34;q\u0026#34;,\u0026#34;w\u0026#34;,\u0026#34;e\u0026#34;,\u0026#34;r\u0026#34;,\u0026#34;t\u0026#34;,\u0026#34;y\u0026#34;,\u0026#34;u\u0026#34;,\u0026#34;i\u0026#34;,\u0026#34;o\u0026#34;,\u0026#34;p\u0026#34;,\u0026#34;a\u0026#34;,\u0026#34;s\u0026#34;) //创建一个随机对象 val random = Random /** * 定义一个函数，用随机的的方式模拟出某用户听了某歌手的歌，并给出评分 * 生成数据格式为：用户ID，歌手ID，评分 */ def genLine() = { val ad = artists(random.nextInt(artists.length)).hashCode() val ud = user(random.nextInt(user.length)).hashCode() val count = random.nextInt(30) + 1 s\u0026#34;$ud,$ad,$count\u0026#34; } def main(args: Array[String]): Unit = { //创建文件对象 val file = new PrintWriter(\u0026#34;D:/Downloads/user_artists.log\u0026#34;) //生成1000条评价，每条为一行，写入文件中 for(i \u0026lt;- 0 to 1000){ val str = genLine file.println(str) } file.close() } }上面代码用来生成一些模拟数据，这个不是必须的，可以用自己的方法得到数据，生成完以后就可以上传到HDFS中，比如： hdfs:///test/user_artists.log 这个位置。当然只是测试的话也可以放在本地，然后以本地模式启动Spark来运行。\n构建模型# package com.algorithm.matrix import org.apache.spark.mllib.recommendation._ import org.apache.spark.SparkConf import org.apache.spark.SparkContext object MusicReferee { def main(args: Array[String]): Unit = { //初始化Spark应用基本对象 val sc = new SparkContext(new SparkConf()) //模拟歌手列表，在这里起到字典作用，下面会根据ID取出对应的歌手 val artists = Array(\u0026#34;李健\u0026#34;,\u0026#34;吉克隽逸\u0026#34;,\u0026#34;吴莫愁\u0026#34;,\u0026#34;杨坤\u0026#34;,\u0026#34;宋祖英\u0026#34;,\u0026#34;罗大佑\u0026#34;,\u0026#34;龙梅子\u0026#34;, \u0026#34;水木年华\u0026#34;,\u0026#34;小沈阳\u0026#34;,\u0026#34;谭晶\u0026#34;,\u0026#34;蔡健雅\u0026#34;,\u0026#34;刘佳\u0026#34;,\u0026#34;王蓉\u0026#34;,\u0026#34;黄龄\u0026#34;,\u0026#34;庞麦郎\u0026#34;,\u0026#34;钟汉良\u0026#34;) //作用同上 val user = Array(\u0026#34;q\u0026#34;,\u0026#34;w\u0026#34;,\u0026#34;e\u0026#34;,\u0026#34;r\u0026#34;,\u0026#34;t\u0026#34;,\u0026#34;y\u0026#34;,\u0026#34;u\u0026#34;,\u0026#34;i\u0026#34;,\u0026#34;o\u0026#34;,\u0026#34;p\u0026#34;,\u0026#34;a\u0026#34;,\u0026#34;s\u0026#34;) //通过歌手id取出对应名字 def getArti(id: Int) = { artists.find { x =\u0026gt; x.hashCode() == id }.get } //定义一个函数，可以通过用户id取出对应名字 def getUser(id: Int) = {user.find { x =\u0026gt; x.hashCode() == id }.get} //从HDFS上加载测试数据文件 val arr = sc.textFile(\u0026#34;hdfs:///test/user_artists.log\u0026#34;) //将文件中的每一行数据封装成一个评定对象(用户,产品,评分) val trainData = arr.map { x =\u0026gt; val Array(userId, artistsId, count) = x.split(\u0026#34;,\u0026#34;).map { x =\u0026gt; x.toInt } Rating(userId, artistsId, count) }.cache() //用Spark机器学习库中的ALS算法分解并生成我们想要的模型 //后面是生成模型时需要的参数，具体含义请看Spark官方文档 val model = ALS.trainImplicit(trainData, 10, 5, 0.01, 1.0) //打印出其中一个用户A收听过的所有歌手 arr.filter { x =\u0026gt; x.split(\u0026#34;,\u0026#34;)(0).equals(\u0026#34;98\u0026#34;) }.map { x =\u0026gt; val s = x.split(\u0026#34;,\u0026#34;) (\u0026#34;b\u0026#34;,getArti(s(1).toInt),s(2)) }.repartition(1).sortBy(f =\u0026gt; f._3).foreach(println) //得知用户A听谁的歌较多,然后向他推荐5个歌手 val artiOfUser = model.recommendProducts(98, 5) //转换成可读形式并打印出来 artiOfUser.map(x =\u0026gt; println((x.user,getArti(x.product),x.rating))) } }然后打成JAR包后向Spark提交就可以了，这个模型有很多参数可以微调，不同参数对最终结果有较大影响，具体的使用细节请参考Spark官方：https://spark.apache.org/docs/latest/mllib-collaborative-filtering.html#collaborative-filtering\n-End-\n关于# 作者：张佳军\n阅读：43\n点赞：1\n创建：2017-06-11\n"},{"id":1,"href":"/algorithm/algorithm-naive-bayes.html","title":"分类算法 - 朴素贝叶斯","section":"数据结构与算法","content":"分类算法 - 朴素贝叶斯# 朴素贝叶斯是种简单而有效的分类算法，被应用在很多二元分类器中，那什么叫二元分类？也就是非A即B，假设我现在用朴素贝叶斯算法写一个分类器，然后输入一封邮件，它可以根据特征库来判断这封邮件是不是垃圾邮件。当然，它还可以用来处理多元分类的问题，比如：文章分类、拼写纠正等等。\n基本原理# 在网上已经有很多关于朴素贝叶斯的介绍了，其中不乏一些讲得深刻而又通俗易懂的文章，本文不准备长篇大论探讨它的数学原理及深层含义，不过基本原理还是得讲一下，先说一下条件概率模型，以下摘抄自wiki：\n公式：p(C|F1,\u0026hellip;,Fn) 解释：独立的类别变量C有若干类别，条件依赖于若干特征变量。\n意思是说，在F1,F2,F3\u0026hellip;这些特征发生的情况下，类别C的发生概率，很容易理解。 那么，如果我事先知道类别C发生的情况下，F1,F2,F3\u0026hellip;出现的概率，也就是p(F1,\u0026hellip;,Fn|C)，那么也可以反过来求出p(C|F1,\u0026hellip;,Fn)，这其实就是它基本原理了，以下是贝叶斯公式：\np(类别1|特征1,特征2,特征3...) = p(特征1,特征2,特征3...|类别1) * p(类别1) / p(特征1,特征2,特征3...)解释： p(类别1|特征1,特征2,特征3\u0026hellip;) 表示在特征1、特征2、特征3\u0026hellip;发生的情况下，类别1发生的概率。 p(特征1,特征2,特征3\u0026hellip;|类别1) 表示在类别1发生的情况下，特征1、特征2、特征3\u0026hellip;发生的概率。 p(类别1) 表示类别1在所有类别中出的概率 p(特征1,特征2,特征3\u0026hellip;) 表示特征1,特征2,特征3\u0026hellip;在特征库中出现的概率 重复使用链式法则后，最终可以将公式转换为这样：\np(类别1|特征1,特征2,特征3...) = p(特征1|类别1) * p(特征2|类别1) * p(特征3|类别1) ...这就是朴素贝叶斯公式了。\n实现邮件分类器# 之前在研究Spark的MLLib时第一次接触到朴素贝叶斯算法（实际上已经是2016年的事了），并动手实现了一个简单的垃圾邮件识别程序，今天重新整理一下，我会在代码中一步一步讲明原理，先说一下实现邮件分类器的步骤：\n收集数据，准备一些正常的邮件和垃圾邮件，将它们放在两个不同的目录中 特征提取，要让程序判断一封邮件是不是垃圾邮件，就需要告诉程序，什么是正常邮件，什么是垃圾邮件，这一步叫特征提取，也可以叫做训练集，提取的方法很简单，就是分别计算出正常邮件中与垃圾邮件中所有单词出现的数次及概率，这样我们就有了两个训练集 识别邮件，这时需要写一个函数用来接收并识别给定的邮件，其原理也很简单，先将指定的邮件中的所有单词提取出来，然后计算这些单词在两个训练集出现的概率之积即可，这样就得到两个数字，从哪个训练集中计算出的数字大，那这封邮件就属于哪种类型。 参数调整，当程序基本实现后，需要对各步骤中的参数进行调整，比如在特征提取时，我去掉了只出现一次或两次的单词，又比如，在识别新邮件时，如果这封邮件中出现了训练集中没有的单词时，应该给定它一个默认的概率值 下面给出一个实际的例子，在本例中我大概用了三千封历史邮件，分别放在了两个目录中，例子中的数据实在是想不起来在哪下载的，所以请同学们自行收集吧。 首先是准备数据：\npackage com.algorithm.bayes import org.apache.spark.SparkConf import org.apache.spark.SparkContext import scala.io.Source import java.io.File import scala.sys.process.ProcessBuilder.Source import scala.collection.mutable.ArrayBuffer import scala.collection.mutable.HashMap /** * 利用贝叶斯实现的邮件分类器，提交给Spark执行 */ object Classification { def main(args: Array[String]): Unit = { //初始化Spark val conf = new SparkConf().setAppName(this.getClass.getName) val sc = new SparkContext(conf) //该目录内均为正常邮件 var easy = \u0026#34;/home/kxdmmr/src/ml-data/Email-data/easy_ham\u0026#34; //该目录内均为垃圾邮件 var spam = \u0026#34;/home/kxdmmr/src/ml-data/Email-data/spam\u0026#34; ...然后就是创建两个特征库，这个是重中之重：\n//将所有正常邮件的文本都提取出来 var easyArr = sc.parallelize(getEmailText(easy)) //将所有垃圾邮件的文件都提取出来 var spamArr = sc.parallelize(getEmailText(spam)) //正常邮件中所有的单词,不去重 var easySplit = easyArr.flatMap { x =\u0026gt; x.split(\u0026#34;[^a-zA-Z]+\u0026#34;) } var easyLen = easySplit.count().toDouble //垃圾邮件中所有的单词,不去重 var spamSplit = spamArr.flatMap { x =\u0026gt; x.split(\u0026#34;[^a-zA-Z]+\u0026#34;) } var spamLen = spamSplit.count().toDouble //正常邮件中每个单词占所有单词的百分比及出现次数 //结果数据的格式为：HashMap[单词: String,(出现次数: Double,百分比: Double)] var easyWordProb = new HashMap[String,(Double,Double)]() easySplit.map { x =\u0026gt; (x,1.0) }.reduceByKey(_+_).filter(x =\u0026gt; x._2 \u0026gt; 2).collect().foreach(f =\u0026gt; easyWordProb.put(f._1,(f._2,f._2/easyLen))) //垃圾邮件中每个单词占所有单词的百分比及出现次数 //结果数据的格式为：HashMap[单词: String,(出现次数: Double,百分比: Double)] var spamWordProb = new HashMap[String,(Double,Double)]() spamSplit.map { x =\u0026gt; (x,1.0) }.reduceByKey(_+_).filter(x =\u0026gt; x._2 \u0026gt; 2).collect().foreach(f =\u0026gt; spamWordProb.put(f._1,(f._2,f._2/spamLen)))上面用到了一个函数，定义如下：\n/** * 处理指定目录内的500封邮件，提取邮件内出现的所有单词 * 并封结果封装成一个数组，将其返回 */ def getEmailText(path: String):ArrayBuffer[String] ={ var count = 0; var textArr = new ArrayBuffer[String]() var files = new File(path).listFiles() var temp = new StringBuilder() var isAdd = false for(f \u0026lt;- files){ if(count ==500) return textArr; count += 1 isAdd = false temp.delete(0, temp.length) var lines = Source.fromFile(f) try{ for(line \u0026lt;- lines.getLines()){ if(line.equals(\u0026#34;\u0026#34;)) isAdd = true else temp.append(line+\u0026#34;,\u0026#34;) } }catch{ case e:Exception =\u0026gt; println(\u0026#34;************** \u0026#34;+e.toString()) } lines.close() textArr.append(temp.toString) } textArr }最后写一个函数，用来处理指定的邮件：\n/** * 识别指定邮件的类型，将结果打印到控制台 */ def getType(file: File){ var textArr = new ArrayBuffer[String]() var isAdd = false try{ for(x \u0026lt;- Source.fromFile(file).getLines()){ if(x.equals(\u0026#34;\u0026#34;)) isAdd = true else textArr.append(x+\u0026#34;,\u0026#34;) } }catch{case e:Exception =\u0026gt; None} var result = textArr.flatMap { x =\u0026gt; x.split(\u0026#34;[^a-zA-Z]+\u0026#34;) } var a = 1.0 var b = 1.0 for(i \u0026lt;- result ){ var easyPro = easyWordProb.getOrElse(i,(1.0,0.0001))._2 var spamPro = spamWordProb.getOrElse(i,(1.0,0.0001))._2 a = easyPro * a b = spamPro * b } if(a \u0026gt; b) println(\u0026#34;此邮件为正常邮件 \u0026#34;+a+\u0026#34; \u0026#34;+b) else println(\u0026#34;此邮件为垃圾邮件 \u0026#34;+a+\u0026#34; \u0026#34;+b) } } }结语# 以上就是实现一个邮件分类器的所有步骤，单独解释贝叶斯可能比较难懂，但实际用的时候发现它的理原极其简单而又直接，这也许就是它名字的由来吧，刚开始接触朴素贝叶斯时查阅了很多资料，其中有几篇让我印象非常深刻，也推荐大家看一看：\n参考资料# 首先是wiki，理所当然，它的专业性很强，如果你的数学功底比较扎实的话，有这一篇就够了： 朴素贝叶斯分类器 这篇文章写的很好，举了很多例子，探讨了朴素贝叶斯更为深层的意义，且专业程度比起wiki有过之而无不及： 数学之美番外篇：平凡而又神奇的贝叶斯方法 如果你像我一样，数学课没有好好听，那就看看下面这篇文章，相当之通俗易懂，而又很容易看清朴素贝叶斯的本质： 自己动手写贝叶斯分类器给图书分类 关于# 作者：张佳军\n阅读：45\n点赞：2\n创建：2017-06-24\n"},{"id":2,"href":"/algorithm/algorithm-quick-sort.html","title":"排序算法 - 快速排序","section":"数据结构与算法","content":"排序算法 - 快速排序# 作为排序之王的快速排序法，要理解它很容易，但实现起来又是另外一回事，主要是因为快速排序在实现上是很苛刻的，差之毫厘，谬以千里，如果不是专业研究算法的话，突然哪天要用到还真不一定写的出来，主要是平时用的很少。。\n基本原理# 给定一个数据集，先从中选出一个元素作为枢钮元，选取枢钮元有多种策略，它对算法的性能影响颇大，目前效果最好的应该是三数中值分割法，三数指的是数组中第一个元素，最后一个元素，和中间那个元素，然后从这三个元素中找出中位数并作为枢钮元 将大于枢钮元的元素移动到枢钮元的左边，将小于枢钮元的元素移动到枢钮元的右边 这时在枢钮元两边可以看作是两个子数据集，然后对这两个数据集分别进行快速排序，直到子数据集长度为1 时间复杂度# 平均时间为O(N log N) 最坏情况为O(N^2) 实例# static void swap(int [] a,int i,int j){ int temp = a[i]; a[i] = a[j]; a[j] = temp; } static int partition(int a[],int start,int end){ int m = start+(end-start)/2; if(a[start] \u0026gt; a[end]) swap(a,start,end); if(a[start] \u0026gt; a[m]) swap(a,start,m); if(a[m] \u0026lt; a[end]) swap(a,m,end); return a[end]; } static void quickSort(int a[],int start,int end){ if(!(start\u0026lt;end)) return; int temp = partition(a,start,end); int i = start, j = end; while(i \u0026lt; j){ while(i \u0026lt; j \u0026amp;\u0026amp; a[i] \u0026lt;= temp)i++; a[j] = a[i]; while(i \u0026lt; j \u0026amp;\u0026amp; a[j] \u0026gt;= temp)j--; a[i] = a[j]; } a[j] = temp; quickSort(a,start,j-1); quickSort(a,j+1,end); } public static void quickSort(int[] arr){ quickSort(arr, 0, arr.length - 1); }关于# 作者：张佳军\n阅读：17\n点赞：3\n创建：2017-06-24\n"},{"id":3,"href":"/architecture/cloud-plateform-loadbalancer.html","title":"云平台 - 负载均衡器","section":"架构设计","content":"云平台 - 负载均衡器# 负载均衡是云平台的重要组成部分，本文介绍开源项目好雨云帮平台(以下简称云帮)中负载均衡模块的具体实现，以及它出于什么样的考虑，希望能给有需要的同学带来一些参考和思路。\n为什么需要负载均衡# 首先，云帮平台的内部网络划分是支持多租户的，每个租户有一个私有的IP段，不同租户的网络是相互不可见的，当我们把一个容器化的应用部署到云帮后，云帮平台会为这个容器分配一个内部IP，用于同一租户中的不同应用在集群内部通信，而从集群外部是不能直接访问的。所以我们需要有一个集群入口控制器让用户能方便地访问这些应用。\n其次，云帮中部署的每个应用都可以有多个实例，假设我们为一个WEB应用部署了三个实例，然后每个实例分担一部分流量，这时我每就需要在它们前面加一个负载均衡器来分发流量给三个实例。\n除了上述的基本功能以外，我们的负载均衡器还必须支持更多的功能，比如：\n入口控制器能够根据数据包信息（如协议、端口号、主机名等）将请求转发给指定的应用。 实时地发现集群中应用的变化（如添加自定义域名、添加证书、添加端口等）并动态更自身的转发规则。 要同时支持HTTP、TLS、TCP和UDP协议，因为有时不只WEB应用需要向外提供服条，像RPC、MySQL等也需要对外开放。 最后一点也很重要，那就是支持高可用。 综上所述，我们需要一个同时支持L4、L7的负载均衡器集群，还必须能够自动发现集群中的应用变化以更新自己的转发规则。\n云帮中的负载均衡# 整体架构# 以下是云帮负载均衡模块的架构示意图：\nweb：表示云帮中的一个应用，并且有三个实例。 api-server：表示kubeneters的kube-apiserver组件。 entrance：表示云帮的负载均衡器通用接口，支持多种负载均衡器插件。 Entrance实现# 云帮中的负载均衡是面向应用的，不同的应用可以使用不同的负载均衡，所以我们设计了Entrance组件，它可以集成多种负载均衡插件，OpenResty就是其中之一，这意味着云帮不仅支持OpenResty，还可以方支持其它负载均衡插件。\n它的主要工作是从kube-apiserver中通过Restful API监听应用容器的IP、端口，service和endpoint等的资源变化，然后把这些资源抽象为通用的负载均衡资源并缓存在ETCD中，这些通用资源有：\nPool：表示一个负载均衡池，其中包括多个节点，对应上图中的三个WEB实例。 Node：表示Pool中的一个节点，对应上图中的其中一个WEB实例。 Domain：表示一个域名，负载均衡器可以识别一个数据包中的域名信息然后将数据转发给对应的Pool。 VirtualService：表示监听了某个端口的虚拟主机，还指明了端口的协议名称，主要用来处理L4入口控制和负载均衡。 Rule：表示一条转发规则，用来描述域名跟Pool的对应关系，还指明了端口的协议名称与证书信息，主要用来处理L7入口控制和负载均衡。 当有资源发生变化时，Entrance会将通用资源转化为相应插件的资源并调用该插件的Restful API。\n从上图中可以看到，有两个Entrance和两个OpenResty实例，它们的关系是：每个Entrance中持有所有OpenResty的地址，当有信息需要更新时，Entrance会将信息更新到所有的OpenResty。那两个Entrance之间怎么协调呢？这里我们利用ETCD本身的特性做了分布式锁，保证只有一个Entrance有权限向OpenResty更新信息，这样就实现了高可用。\nOpenResty件插# OpenResty是一个可以用Lua脚本来处理请求和业条逻辑的WEB应用，并且内置了众多Lua相关的指定和函数供开发者使用，很合适开发Restful API服务器，我们将OpenResty作为Entrance的插件之一原因如下：\n它是基于Nginx开发，所以稳定性和性能方面不用太担心。 比较接近我们的目标，OpenResty已经帮我们把Lua模块编译进去，我们可以很方便地用Lua脚本丰富负载均衡器的功能，可以让我们省去一些工作量。 同时支持L7和L4的负载均衡。 我们在OpenResty端嵌入了一个Rest API服务器，这些API是用Lua写的，前面说过OpenResty集成了Lua脚本功能，我们可以直接用Lua来处理请求，下面是Nginx配置文件的其中一部分：\n# custom api of upstream and server server { listen 10002; location ~ /v1/upstreams/([-_0-9a-zA-Z.@]+) { set $src_name $1; content_by_lua_file lua/upstream.lua; } location ~ /v1/servers/([-_0-9a-zA-Z.@]+) { set $src_name $1; content_by_lua_file lua/server.lua; } }当我们调用下面的API时：\ncurl -s localhost:10002/v1/servers/app1 -X POST -d \u0026#34;$json_data\u0026#34;OpenResty会执行相应的Lua脚中，也就是lua/server.lua，前面说过，OpenResty内置了很多Lua相关的指命与函数，可以让Lua与Nginx更好地交互，所以我们在脚本中很容器处理接收到的JSON数据，并将其转换为配置Nginx文件，由于Lua代码较多就不贴出来了，可以在本文的引用部分找到该项目地址。\n这里有个需要注意的地方，当收到大量修改server和upstream的请求时，OpenResty需要频繁加载配置文件，这样会增加负载且影响性能。实际上OpenResty有很多第三方插件可以使用，有一个叫dyups的插件可以做到动态修改upstream，它的使用方式如下，Lua代码：\n-- 增加或更新指定upstream dyups.update(\u0026#34;upstream_name\u0026#34;, [[server 127.0.0.1:8088;]]) -- 删除指定upstream dyups.delete(\u0026#34;upstream_name\u0026#34;)执行成功后就已经生效了，不需要我们执行nginx -s reload命令，这会提高一些效率。\n对于server的修改暂时还没有相应用插件做到动态修改，所以实际上我们的负载均衡器分两种情况，如果更新了upstream配置会即时生效，而更新server配置则需要加上nginx -s reload命令。\n结语# 我们用Entrance加OpenResty实现了一个可插拔且高可用的负载均衡器，整体来说并不复杂，最后希望本文能带给你一些帮助。现在我们已经把我们的OpenResty插件分离出来做为一个子项目并且开源在Github上，你可以下载并单独使用：Github地址。\n引用与参考# Openresty项目 dyups插件 好雨云帮 关于# 作者：sycki\n阅读：97\n点赞：0\n创建：2018-04-30\n"},{"id":4,"href":"/atlas-case/11111.html","title":"11111","section":"Atlas Case","content":"11111# 患者信息# 记录日期：2024.11.26\n出生年份：1992\n性别：男\n省份：四川\n症状：无症状\n诊断日期：2024.06.04\n诊断：寰枢椎脱位、颅底凹陷、小脑下疝、脊髓空洞\n影像：\n手术# 日期：2024.06\n医院：宜宾二医院\n主刀：黄思庆徒弟\n费用：自费1.6w\n术后状况：\n术后影像：\n康复记录：\n10多天下床，术后10多天感觉症状还在。\n2024.10.30，在宜宾二医院复查，螺钉脱离。类似喝醉的头晕，走路不稳，左手力量和灵活度下降，影像如下：\n二次手术# 日期：2024.11.23\n医院：湘雅三\n主刀：李劲松\n费用：自费1.6w\n术后影像：\n康复记录：\n第二天下床，术后15天，头不晕了，走路稳了，左手力量和灵活度稍有好转。 关于# 作者：sycki\n阅读：23\n点赞：0\n创建：2024-12-14\n"},{"id":5,"href":"/atlas-case/11112.html","title":"11112","section":"Atlas Case","content":"11112# 患者信息# 记录日期：2024.12.07\n出生年份：1993\n性别：女\n省份：新疆\n症状：腿不舒服，影响走路了，下楼腿抖，跑步跑不起来，高抬腿抬不起来\n诊断日期：2024.07.31\n诊断：颅底凹陷\n影像：\n手术# 日期：2024.08.30\n医院：新疆维吾尔自治区人民医院\n主刀：范涛\n费用：\n术后状况：术后颈后有积液\n术后影像：\n康复记录：\n2024.10.06，积液范围略小。\n2024.12.12，术前症状都没了，术后多了一个症状，右胳膊发凉，现在偶尔还会。\n关于# 作者：sycki\n阅读：2\n点赞：0\n创建：2024-12-14\n"},{"id":6,"href":"/atlas-case/11113.html","title":"11113","section":"Atlas Case","content":"11113# 患者信息# 记录日期：2024.12.09\n出生年份：\u0026lt;1980\n性别：女\n省份：\n症状：30岁时摔了一跤，当时短暂瘫痪，后面又慢慢恢复。多年后出现手麻脚麻\n诊断日期：2023.11\n诊断：30岁时齿状突骨折\n影像：\n手术# 日期：2023.11.16\n医院：北三医院\n主刀：王超\n费用：\n术后状况：\n术后影像：\n康复记录：\n2024.12.09，术后感觉良好，手麻脚麻症状已经没有了。 关于# 作者：sycki\n阅读：7\n点赞：0\n创建：2024-12-14\n"},{"id":7,"href":"/atlas-case/11114.html","title":"11114","section":"Atlas Case","content":"11114# 患者信息# 记录日期：2024.12.08 出生年份： 性别：女 省份： 症状： 诊断日期： 诊断：寰枢椎脱位 影像： 手术# 日期：2024.01 医院：北三医院 主刀：王超 费用： 术后状况： 术后影像： 康复记录：\n2024.11，去人民医院复查。\n关于# 作者：sycki\n阅读：1\n点赞：0\n创建：2024-12-14\n"},{"id":8,"href":"/atlas-case/11115.html","title":"11115","section":"Atlas Case","content":"11115# 患者信息# 记录日期：2024.12.11 出生年份： 性别：女 省份： 症状： 诊断日期： 诊断：寰枢椎脱位 影像： 手术# 日期：2024.06\n医院：上海某医院\n主刀：不详\n费用：\n术后状况：\n术后影像：\n康复记录：\n2024.10，复查，恢复良好。\n关于# 作者：sycki\n阅读：0\n点赞：0\n创建：2024-12-14\n"},{"id":9,"href":"/atlas-case/11116.html","title":"11116","section":"Atlas Case","content":"11116# 患者信息# 记录日期：2024.12.11\n出生年份：\n性别：\n省份：\n症状：\n诊断日期：\n诊断：寰枢椎脱位\n影像：\n手术# 日期：2024.11.03\n医院：\n主刀：李维新\n费用：\n术后状况：\n术后影像：\n康复记录：\n关于# 作者：sycki\n阅读：1\n点赞：0\n创建：2024-12-14\n"},{"id":10,"href":"/atlas-case/11117.html","title":"11117","section":"Atlas Case","content":"11117# 患者信息# 记录日期：2024.12.11\n出生年份：\n性别：男\n省份：\n症状：\n诊断日期：\n诊断：寰枢椎脱位\n影像：\n手术# 日期：2023.10.26\n医院：\n主刀：王建华\n费用：\n术后状况：\n术后影像：\n康复记录：\n2024.04，半年复查。\n关于# 作者：sycki\n阅读：0\n点赞：0\n创建：2024-12-14\n"},{"id":11,"href":"/atlas-case/11118.html","title":"11118","section":"Atlas Case","content":"11118# 患者信息# 记录日期：2024.12.12\n出生年份：\n性别：女\n省份：\n症状：左手冷的时候，会容易麻木\n诊断日期：\n诊断：寰枢椎脱位，颅底凹陷\n影像：\n手术# 日期：2023.09.01\n医院：\n主刀：王超\n费用：\n术后状况：\n术后影像：\n关于# 作者：sycki\n阅读：5\n点赞：0\n创建：2024-12-14\n"},{"id":12,"href":"/atlas-case/11119.html","title":"11119","section":"Atlas Case","content":"11119# 患者信息# 记录日期：2024.12.13\n出生年份：1970\n性别：女\n省份：\n症状：颈重，肩重，肩甲骨经常火辣辣的，头前伸寰枢椎那会嘎嘎响\n诊断日期：\n诊断：寰枢椎脱位，颅底凹陷\n影像：\n手术# 日期：2023.09\n医院：北三医院\n主刀：王超\n费用：\n术后状况：偶尔会肩甲骨酸，坐下就不酸\n术后影像：\n康复记录：\n2024.02.27，复查，对位良好，偶尔会肩甲骨酸，坐下就不酸。\n2024.10.20，复查，对位良好，偶尔会肩甲骨酸，坐下就不酸。\n关于# 作者：sycki\n阅读：3\n点赞：0\n创建：2024-12-14\n"},{"id":13,"href":"/atlas-case/11120.html","title":"11120","section":"Atlas Case","content":"11120# 患者信息# 记录日期：2024.12.15 出生年份：1961 性别：男 省份： 症状： 诊断日期： 诊断：寰枢椎脱位，颅底凹陷 影像： 手术# 日期：2013 医院：上海长征医院 主刀：史建刚 费用： 术后状况： 术后影像： 二次手术# 日期：2021 医院：上海长征医院 主刀：史建刚 费用： 术后状况： 术后影像： 康复记录：\n2023.09.01，复查，压迫脊髓。\n三次手术# 日期：2023.11 医院：北三医院 主刀：王超 费用： 术后状况： 术后影像： 康复记录：\n2024.03.09，复查，对位良好，但术前症状难以恢复。\n关于# 作者：sycki\n阅读：5\n点赞：0\n创建：2024-12-14\n"},{"id":14,"href":"/atlas-case/11121.html","title":"11121","section":"Atlas Case","content":"11121# 患者信息# 记录日期：2024.12.15 出生年份：1979 性别：女 省份： 症状： 诊断日期： 诊断：寰枢椎脱位 影像： 手术# 日期：2023.10 医院：北京301医院 主刀：彭宝淦 费用： 术后状况：手术方式是用肽缆把寰枢椎缠上，手术手脚麻木，医生让观察一段时间，一个月后手脚无力，背部僵硬。 术后影像： 康复记录：\n2024.02.18，复查，症状还在。\n2024.05，复查，症状还在。\n二次手术# 日期：2024.06.14 医院：北三医院 主刀：王超 费用： 术后状况：手脚无力的症状消失，低头时间久了脖子手术的地方酸痛。 术后影像： 康复记录：\n2024.11.07，复查，对位良好。\n关于# 作者：sycki\n阅读：6\n点赞：0\n创建：2024-12-14\n"},{"id":15,"href":"/atlas-case/11122.html","title":"11122","section":"Atlas Case","content":"11122# 患者信息# 记录日期：2024.12.15\n出生年份：1993\n性别：男\n省份：山西\n症状：自2020年开始，每天到下午5点以后头晕，主要发生在走路转身的时候，会有一瞬间头晕，类似恍惚的感觉，下次转身又会晕一下。刚开始每天头晕一两次，随后头晕频率慢慢变高，到2023年，上午也会出现头晕症状。还有就算握拳时有无力感。\n诊断日期：2024.08\n诊断：寰枢椎脱位、颅底凹陷\n影像：\n手术# 日期：2024.09.19\n医院：宣武医院\n主刀：段宛如、陈赞\n费用：自费1.6w\n术后状况：头晕症状还在，但握拳有劲了，第二天看ct片子，寰齿间隙还是大于3mm，颅底凹陷没有任何改善。\n术后影像：\n康复记录：\n术后颈后愈合良好，髂骨刀口拆线时有血水流出，然后本地医生一直给引流，两个月后北京化验积液，发现没有感染，遂取消引流，伤口3天愈合，愈合前一直头晕，比术前频繁且明显，愈合后头晕减轻。\n2024.12.04，头晕症状加重，双手开始出现无力感。并出现新的症状，胸闷、心慌、气短，休息时身体总想抽搐，听到大点的声音也会抽动一下，听到大的声音脑袋里面会嗡一下，像惊厥一样。\n2024.12.13，去北京拍片，当天走路有点多，感觉脑袋里面疼。影像报告脱位和颅底凹陷都还在，影像如下：\n关于# 作者：sycki\n阅读：23\n点赞：0\n创建：2024-12-14\n"},{"id":16,"href":"/atlas-case/11123.html","title":"11123","section":"Atlas Case","content":"11123# 患者信息# 记录日期：2025.01.18\n出生年份：\n性别：\n省份：\n症状：腿无力，开车踩离合时腿打颤，不由自主\n诊断日期：\n诊断：寰枢椎脱位、颅底凹陷\n影像：\n手术# 日期：2024.11.02\n医院：兰州某医院\n主刀：王景\n费用：\n术后状况：属于难复位型的，采取后路松解，且用了融合器\n术后影像：\n康复记录：\n术后两个月，腿有力了 关于# 作者：sycki\n阅读：0\n点赞：0\n创建：2025-01-19\n"},{"id":17,"href":"/atlas-case/11124.html","title":"11124","section":"Atlas Case","content":"11124# 患者信息# 记录日期：2025.01.18\n出生年份：\n性别：男\n省份：河北\n症状：\n诊断日期：\n诊断：寰枢椎脱位\n影像：\n手术# 日期：2024.10.11\n医院：宣武医院\n主刀：关键\n费用：\n术后状况：术前患者同意不放融合器，术中切了后宫，术后复位还不错\n术后影像：\n康复记录：\n关于# 作者：sycki\n阅读：0\n点赞：0\n创建：2025-01-19\n"},{"id":18,"href":"/atlas-case/11125.html","title":"11125","section":"Atlas Case","content":"11125# 患者信息# 记录日期：2025.01.18 出生年份：1959 性别： 省份： 症状： 诊断日期： 诊断：寰枢椎脱位 影像： 手术# 日期：2024.08.11\n医院：宣武医院\n主刀：陈赞\n费用：\n术后状况：术后症状加重，不能下床\n术后影像：\n康复记录：\n术后4个月，在天坛拍0.23低磁场核磁\n关于# 作者：sycki\n阅读：0\n点赞：0\n创建：2025-01-19\n"},{"id":19,"href":"/atlas-case/11126.html","title":"11126","section":"Atlas Case","content":"11126# 患者信息# 记录日期：2025.01.18 出生年份：1973 性别： 省份：湖南 症状：四肢麻木一年，能正常走路，下楼梯下肢僵直，左脚底有踩着东西的感觉 诊断日期： 诊断：寰枢椎脱位 影像： 手术# 日期：2024.07\n医院：人民医院\n主刀：王超\n费用：\n术后状况：可以正常走路了\n术后影像：\n康复记录：\n2025.01.08复查\n关于# 作者：sycki\n阅读：0\n点赞：0\n创建：2025-01-19\n"},{"id":20,"href":"/atlas-case/11127.html","title":"11127","section":"Atlas Case","content":"11127# 患者信息# 记录日期：2025.01.25\n出生年份：1986\n性别：女\n省份：\n症状：一走路头晕脑胀，迷迷糊糊，眼睛犯困\n诊断日期：2024.04.25\n诊断：寰枢椎脱位，颅底凹陷\n影像：\n手术# 日期：2025.01.09\n医院：人民医院\n主刀：王超\n费用：\n术后状况：体感上没有明显变化\n术后影像：\n康复记录：\n2025.01.25，体感上和术前一样，一走路就头晕脑胀 关于# 作者：sycki\n阅读：0\n点赞：0\n创建：2025-01-25\n"},{"id":21,"href":"/atlas-case/11128.html","title":"11128","section":"Atlas Case","content":"11128# 患者信息# 记录日期：2025.01.25\n出生年份：1980\n性别：女\n省份：\n症状：\n诊断日期：\n诊断：寰枢椎脱位\n影像：\n手术# 日期：2024.06\n医院：上海长征医院\n主刀：\n费用：\n术后状况：没有复位\n术后影像：\n二次手术# 日期：2024.12.30\n医院：人民医院\n主刀：王超\n费用：\n术后状况：\n术后影像：\n关于# 作者：sycki\n阅读：7\n点赞：0\n创建：2025-01-25\n"},{"id":22,"href":"/atlas-case/11129.html","title":"11129","section":"Atlas Case","content":"11129# 患者信息# 记录日期：2025.02.12\n出生年份：\n性别：男\n省份：\n症状：有左手麻木，左腿膝盖有点没劲\n诊断日期：\n诊断：寰枢椎脱位\n影像：\n手术# 日期：2024.12.21\n医院：宣武医院\n主刀：段婉茹主刀，陈赞一助\n费用：\n术后状况：复位良好，左腿膝盖有没劲缓解了，左手麻木还有\n术后影像：\n关于# 作者：sycki\n阅读：0\n点赞：0\n创建：2025-02-12\n"},{"id":23,"href":"/atlas-case/11130.html","title":"11130","section":"Atlas Case","content":"11130# 患者信息# 记录日期：2025.04.18 出生年份：2002 性别：男 省份： 症状： 诊断日期： 诊断：寰枢椎脱位 影像： 手术# 日期：\n医院：上海长征医院\n主刀：\n费用：\n术后状况：术中放了融合器\n术后影像：\n康复记录：\n清楚多久之后复位丢失。陈赞说是松解不到位导致的复位丢失。 二次手术# 日期：2024.12.31\n医院：宣武医院\n主刀：陈赞，段婉茹一助\n费用：\n术后状况：切除齿状突，效果良好\n术后影像：\n康复记录：\n2025.03.03 感觉症状缓解的不多。 "},{"id":24,"href":"/bigdata/ambari-quick-build-bigdata-platform.html","title":"Ambari - 三条命令创建集群","section":"大数据","content":"Ambari - 三条命令创建集群# 什么是 Ambari# Ambari 是大数据生态圈中的一员，但它不是一个大数据计算引擎，而是一个管理工具，用来管理其它大数据工具，如：Hadoop、Spark、Hive 等，当你用 Ambari 作为你的管理工具时，它可以帮你监视你的集群状态、每个组件状态、节点状态等，如下图（这个不是 apache 官方版，所以请原谅我打点马赛克）： 它还可以为你的集群增加或删除一个组件，比如，你的集群没有 Storm，而现在你可以通过 Ambari 来添加它，只需动一动鼠标即可完成。\n原理简介# 它是一个经典的主从架构的分布式的软件，主要由 Ambari-Server 与 Ambari-Agent 两部分组成，架构图如下： 以上是用户为集群添加一个组件（Hbase）的过程，用户在前端进行的所有操作都是通过 REST API 调用后台 Ambari 的，我们可以用浏览器抓包看一下： Ambari Bleprint 作用# 我们先来看一下用 Ambari 创建一个集群的步骤，当在一台机器上安装好 Ambari 后，进入它的 WEB UI，这时出现一个集群创建向导，如下： 这个向导还是很方便的，其中有添加主机，选择要安装的组件等功能。 而 Ambari Bleprint 可以看作是 Ambari 本身提供的一个功能，它的作用是：可以让用户免去 Ambari 的集群安装向导操作，可以让用户把安装集群的所有步骤脚本化，这是个很有用的场景，而它的基本原理就是通两个 JSON 文件来描述一个集群，如下： 图中左边是两个 JSON 文件，它们被通过 REST API 的方式发送给 Ambari-Server ，再由 Ambari-Server 创建出一个集群。\nAmbari Bleprint 的使用# 用 Ambari Bleprint 创建一个集群大概分为6步：\n安装好 Ambari 在你的主机上 准备 JSON 文件，它们用来描述创建什么样的集群，以及组件配置 向 Ambari 注册 Blueprint 修改 yum 源（可选步骤） 用注册的 Blueprint 创建集群 获取进度信息 安装好 Ambari 在你的主机上# 安装方法请参见官网，写的已经很清楚了：https://cwiki.apache.org/confluence/display/AMBARI/Installation+Guide+for+Ambari+2.4.2\n假设你有三台机器：node1.ambari.com, node2.ambari.com, node3.ambari.com, 第一台安装 Ambari-Server ，其它两台安装 Ambari-Agent。\n准备 JSON 文件# 第一个 JSON 文件（my_cluster.json），用来描述安装哪些组件，如：Hadoop、Hbase、Zookeeper 等，以及每个组件的配置，下面是一个简单的例子： { \u0026#34;host_groups\u0026#34; : [ { \u0026#34;name\u0026#34; : \u0026#34;master\u0026#34;, \u0026#34;components\u0026#34; : [ { \u0026#34;name\u0026#34; : \u0026#34;NAMENODE\u0026#34; }, { \u0026#34;name\u0026#34; : \u0026#34;SECONDARY_NAMENODE\u0026#34; }, { \u0026#34;name\u0026#34; : \u0026#34;RESOURCEMANAGER\u0026#34; }, { \u0026#34;name\u0026#34; : \u0026#34;HISTORYSERVER\u0026#34; }, { \u0026#34;name\u0026#34; : \u0026#34;ZOOKEEPER_SERVER\u0026#34; } ], \u0026#34;cardinality\u0026#34; : \u0026#34;1\u0026#34; }, { \u0026#34;name\u0026#34; : \u0026#34;slaves\u0026#34;, \u0026#34;components\u0026#34; : [ { \u0026#34;name\u0026#34; : \u0026#34;DATANODE\u0026#34; }, { \u0026#34;name\u0026#34; : \u0026#34;HDFS_CLIENT\u0026#34; }, { \u0026#34;name\u0026#34; : \u0026#34;NODEMANAGER\u0026#34; }, { \u0026#34;name\u0026#34; : \u0026#34;YARN_CLIENT\u0026#34; }, { \u0026#34;name\u0026#34; : \u0026#34;MAPREDUCE2_CLIENT\u0026#34; }, { \u0026#34;name\u0026#34; : \u0026#34;ZOOKEEPER_CLIENT\u0026#34; } ], \u0026#34;cardinality\u0026#34; : \u0026#34;1+\u0026#34; } ], \u0026#34;Blueprints\u0026#34; : { \u0026#34;blueprint_name\u0026#34; : \u0026#34;multi-node-hdfs-yarn\u0026#34;, \u0026#34;stack_name\u0026#34; : \u0026#34;HDP\u0026#34;, \u0026#34;stack_version\u0026#34; : \u0026#34;2.4\u0026#34; } }这个 JSON 串是相当长的，那要安装一个包含很多组件的集群，怎么办？最简单的办法就是从一个已有的集群中导出 JAON 串，并保存在文件中，假设我现在已有一个集群，其 Ambari-Server 的 IP 地址为 10.100.100.101，其集群名叫 my_cluster 那么导出令如下：\n[user@host1 ~]$ curl -H \u0026#34;X-Requested-By: ambari\u0026#34; -X GET -u admin:admin \\ http://10.100.100.101:8080/api/v1/clusters/my_cluster?format=blueprint \u0026gt; my_cluster.json这时 my_cluster.json 内就已包含了这个集群的所有信息。 2. 第二个 JSON 文件（host.json），是用来告诉 Ambari 哪些组件安装在哪个主机上，下面是一个示例：\n{ \u0026#34;blueprint\u0026#34; : \u0026#34;blueprint_name\u0026#34;, \u0026#34;default_password\u0026#34; : \u0026#34;my-super-secret-password\u0026#34;, \u0026#34;host_groups\u0026#34; :[ { \u0026#34;name\u0026#34; : \u0026#34;master\u0026#34;, \u0026#34;hosts\u0026#34; : [ { \u0026#34;fqdn\u0026#34; : \u0026#34;node1.ambari.com\u0026#34; } ] }, { \u0026#34;name\u0026#34; : \u0026#34;slaves\u0026#34;, \u0026#34;hosts\u0026#34; : [ { \u0026#34;fqdn\u0026#34; : \u0026#34;node2.ambari.com\u0026#34;, \u0026#34;fqdn\u0026#34; : \u0026#34;node3.ambari.com\u0026#34; } ] } ] } 第三个 JSON 文件（repo.json），它用来告诉 Ambari 要安装的组件的安装包在哪里，这一步可略，Ambari 默会用网上的公共源来下载安装，这个 JSON 示例如下： [user@host1 ~]$ cat repo.json { \u0026#34;Repositories\u0026#34; : { \u0026#34;base_url\u0026#34; : \u0026#34;http://my.repo.com/all_rpm\u0026#34;, \u0026#34;verify_base_url\u0026#34; : false } }向 Ambari 注册 Blueprint# 这里要用到第一个 JSON 文件，执行如下命令：\n[user@host1 ~]$ curl -H \u0026#34;X-Requested-By: ambari\u0026#34; -i -X POST -u admin:admin \\ -d @my_cluster.json \\ http://node1.ambari.com:8080/api/v1/blueprints/bluesingle?validate_topology=false说明： -H Ambari 特定的请求头 -u 是 Ambari 的用户名与密码 -d 是指 http 请求附带的数据\n修改 yum 源（可选）# 如果你有自已的 repo 源，可以修改为自的的源，这里要用到第三个 JSON 文件，命令如下：\n[user@host1 ~]$ curl -u admin:admin -H \u0026#34;X-Requested-By: ambari\u0026#34; -i -X PUT \\ -d @repo.json \\ http://node1.ambari.com:8080/api/v1/stacks/HDP/versions/1.0/operating_systems/redhat6/repositories/$repo_name说明： $repo_name 是 Ambari 内置的源的名字，也就是说我们要把哪个源修改为我们自己的源\n开始创建集群# 这时候用到第二个 JSON 文件：\n[user@host1 ~]$ curl -u admin:admin -H \u0026#34;X-Requested-By: ambari\u0026#34; -i -X POST \\ -d @host.json \\ http://node1.ambari.com:8080/api/v1/clusters/cluster_name这时候它就开始创建集群了，要等待较长时间，在返回的数据里面有一个 href，记下来。\n监视安装进度# 这时用上一步返回的 hred 可以查看进度信息：\n[user@host1 ~]$ curl -u admin:admin -H \u0026#34;X-Requested-By: ambari\u0026#34; -X GET $href { \u0026#34;aborted_task_count\u0026#34;: 10, \u0026#34;cluster_name\u0026#34;: \u0026#34;leap\u0026#34;, \u0026#34;completed_task_count\u0026#34;: 155, \u0026#34;create_time\u0026#34;: 1487360660489, \u0026#34;end_time\u0026#34;: -1, \u0026#34;exclusive\u0026#34;: false, \u0026#34;failed_task_count\u0026#34;: 1, \u0026#34;id\u0026#34;: 1, \u0026#34;inputs\u0026#34;: null, \u0026#34;operation_level\u0026#34;: null, \u0026#34;progress_percent\u0026#34;: 94.31515151515151, \u0026#34;queued_task_count\u0026#34;: 3, \u0026#34;request_context\u0026#34;: \u0026#34;Logical Request: Provision Cluster \u0026#39;leap\u0026#39;\u0026#34;, \u0026#34;request_schedule\u0026#34;: null, \u0026#34;request_status\u0026#34;: \u0026#34;FAILED\u0026#34;, \u0026#34;resource_filters\u0026#34;: null, \u0026#34;start_time\u0026#34;: -1, \u0026#34;task_count\u0026#34;: 165, \u0026#34;timed_out_task_count\u0026#34;: 0, \u0026#34;type\u0026#34;: null } ...其中 progress_percent 后面就是当前的安装进度，还可以看到安装状态等等。 这时还可以进入 Ambari 的 WEB UI ，也可以看到进度信息。 安装步骤也就到此结束了，如果你需要一个自动化的集群安装过程，或是你正在开发调试大数据组件，这篇文章也许也可帮到你，祝你工作愉快。\n参考资料# Ambari Blueprint 使用 Ambari 介绍与使用 关于# 作者：张佳军\n阅读：290\n点赞：1\n创建：2017-02-19\n"},{"id":25,"href":"/bigdata/bigdate-on-docker.html","title":"Bigdata on Docker","section":"大数据","content":"Bigdata on Docker# 最近我们在把自己的大数据平台迁移到 Docker 上面来，最初的想法是用 Docker 技术代替我们现在的 Open Stack 平台，也就是把 Docker 容器当成虚拟来用，看到这里可能人要说了：这明显没有遵循 Docker 的佳实践啊！是的，但具我了解，很多大数据厂商都在想办法把自家的平台容器化，而且有不少厂商已经做到了，也许这就是趋势吧，尽管 Docker 的设计者并不建议这样做，到后面我们会把大数据平台中的每一个组件一个个的拆开，每一个组件都容器化。现在我们已经实践有一段时间了，并且下一步打算用 Kubernetes 来管理我们的 Docker 集群，先来整理一下在 bigdata on docker 实践中遇到的问题，这些问题都是最实际的，正所谓：真正的工作在细节之中。也希望能给读者一条捷径。\n现在我们来把一个已有的大数据集群放在 Docker 环境中，我指的是重新搭一个集群而不是完整的挪过去。。\n系统环境# 大数据平台在整体架构上不会有什么变化，就是把物理机换成是 Docker 容器而已，那么容器从哪来？一般情况下你需要自己构建一个出来，比如你的大数据平台是基于 CentOS6.5 的，那就要自己动手构建一个基础镜像作为系统，这个镜像中可以包含你需要的一切依赖，如 jdk 和 python 等，然后用这个镜像创建指定数据的容器，比如我打算搭建一个 5 节点的集群那就创建 5 个容器出来，关于创建容器的参数下面再讲，因为还要涉及到网络和挂载之类的问题。\n同步镜像# 有了基础镜像之后，我们需要把这个镜像同步到所有物理机上，如果你如有一台物理机的话，可能不需要这一步，直接用 docker pull docker.io/... 这样的方式就可以了，但如果你有几十台或者更多的物理机，那么你需要搭建一个私有仓库，把你的镜像 push 进入，然后其它机器执行 docker pull 即可。\n网络# 解决 Docker 的跨主机通信是比较重要的一环，好在 Docker 1.9 版本中加入了 Overlay 网络，官方给出来两种方案解决跨主机的通信问题，一个是创建 Swarm 集群，创建好后它会自带一个 Overlay 网络，然后在这个集群中用 Docker service create 这样的方式创建一些容器，它们相互之间就可以直接通信了，但这个网络不能用 Docker run --network 的方式使用。\n另一个是依赖第三方的 Key Value 存储系统，如：etcd, zookeeper 等，让所有 Docker 将自己的信息注册进入，然后手动创建一个 Overlay 网络，以此来实现多主机通信，笔者一直在用 zookeeper 因为对它比较熟悉啦，关于实现的细节可以看 Docker 多主机通信之ZK。\n在 Docker 1.13 版本中，官方去除了一些限制，即前面说到的 Swarm 集群自带的网络，新的版本允许以 Docker run --network 的方式使用了，而且不依赖第三方工具，实现起来相当简单，只需在创建 Overlay 网络时加上 --attachable 选项即可。\n存储# 容器的大小是有限制的，如果安装 Docker 后不作任何配置，那么每个容器最多使用 10G 的容间，如果你之前只是用容器跑一些 Tomcat 之类的应用可以不会注意到这个问题，但是如果把它当虚机用的话，你很快会发现容器空间不足之类的告警，这时你需要重新配置你的 Docker 启动参数，如果你用的是 CentOS7 那么可以参考这篇文章。\n到这里还没完，我们还需要解决大数据的平台中的核心问题：“大数据的存储”，比如 HDFS 中的数据要存在哪？容器里当然是不行的，这时需要把物理机上的硬盘先 mount 到物理机上不同的目录上，如 mount /dev/sdb /data/sdb ，然后通过 Docker run -v /data/sdb:/data/sdb 参数将目录挂载到容器内，假设有三台物理机，每台物理有6块硬盘，然后我打算在每个物理机上跑两个容器，那么每个容器可以分到三个硬盘，这些应该提前规划好。\n访问端口# 到这里集群已经可以跑起来了，但是从外面访问集群内的服务时，你可能还会遇到问题：docker run -p 选项可以把想要访问的端口映射到物理机的IP上，问题是大数据平台中的端口那么多，难道我要全部通过这种方式指定？再者说动态端口怎么办？关于这个问题，首先 docker run -p 选项可以指定端口范围，比如这样：docker run -p 6000-6010:6000-6010，这样就映射了十个端口出来，但是如果范围太大就不行了，启动慢不说，还浪费系统资源，而且它不能动态修改，容器启动后万一想修改一下是比较麻烦的。\n其实还可以动态映射，docker run -p 选项其实是通过 iptables 命令实现的，像上面那条命令实际是在物理机上创建了十条 iptables 规则，我们可以通过直接操作 iptables 达到 Docker 动态映射端口 的目的，比如物理机的IP是 10.100.0.101 ，容器的IP是 10.200.0.101 ，那么只需要一条命令就可以把所有端口映射出来，如下：\niptables -t nat -A DOCKER -d 10.100.0.101/32 ! -i docker_gwbridge \\ -p tcp -m tcp --dport 1025:62000 -j DNAT --to-destination 10.200.0.101但是如果同一物理机上有两个容器相需要映射相同的端口出来，岂不是会冲突？当然会啊，，这个问题我总结出来的最好用的方式是，在每个物理机上设定多个IP，不同的容器映射到不同的IP上就可以了，CentOS7 增加子IP的方式可以参考这篇文章。\n启动参数# 结合上面所有的内容，创建容器时可能会用到以下参数：\ndocker run \\ --name node1 \\ -h node1.overlay.com \\ --network overlay.com \\ -v /data/sdb-node1:/data \\ --privileged=true \\ -p 10.100.0.101:23:22 \\ -tid registry.io:5000/centos:6.5-End-\n关于# 作者：张佳军\n阅读：19\n点赞：2\n创建：2017-09-16\n"},{"id":26,"href":"/bigdata/bigdate-on-kubernetes.html","title":"Bigdata on Kubernetes","section":"大数据","content":"Bigdata on Kubernetes# 把大数据平台容器化，是我们的必经之路，一是因为容器化有诸多好处，比如：将每个组件标准化、易于管理、快速升级、快速部署等等，二是因为趋势，当其它人做了而你没做，就可能被客户抛弃，所以最好在所有人之前就开始做。\n基础镜像# 把大数据平台容器化，首先得先把平台中的组件一个个拆开，然后每个组件（Hadoop, Spark等）的运行环境都有差别，比如 Hadoop 依赖 DJK7 而 Spark 依赖 JDK8 等，这就需要将组件和相应的依赖打包成在一起，但所有组件应该共用一个基础镜像，这个基础镜像很小，可能只有几十到几百 MB ，构建基础镜像看这里，有了基础镜像之后就可以在写每个 Dockerfile 时指定的你 FROM 参数。\n构建组件镜像# 我们先用 hadoop-yarn 来举例编写一个 Dockerfile，如果我有自己的 yum 源，那么可以直接使用 yum install hadoop-yarn 这样的方式来安装，下面是一个例子：\nFROM centos6.5-base ADD /jdk-7u79-linux-x64.tar.gz /opt COPY /bigdata.repo /etc/yum.repos.d/bigdata.repo ENV JAVA_HOME=/opt/jdk1.7.0_79 RUN yum install hadoop* -y COPY /bootstrap.sh /bootstrap.sh CMD /bootstrap.sh其中 bigdata.repo 是一个 yum 源文件，里面定义了 hadoop, spark, hbase 等组件的 rpm 源；bootstrap.sh 是一个服务启动脚本，里面写了很重要的逻辑。\n镜像仓库# 有了 Yarn 镜像之后，我们需要将它放到私有仓库中，以便 Kubernetes 集群中的所有机器都能使用它，这时你需要搭建一个私有仓库，然后把所有组件的镜像 push 进去。\nStatefulSet# Kubernetes 中的 StatefulSet 是为了解决有状态的服务而设计的，简单来说它可以定义一组 Pod 而且可以指定规模，而这些 Pod 并不是一模一样的，它们都有自己的状态，这个特性刚好与 Hadoop Yarn 的需求相吻合，下面是 StatefulSet 的部分示例：\napiVersion: apps/v1beta1 kind: StatefulSet metadata: name: node spec: serviceName: \u0026#34;node\u0026#34; replicas: 3 template: spec: containers: - name: node image: hadoop-yarn:v1 env: - name: SCALE value: \u0026#34;3\u0026#34;当执行这个 yaml 后，Kubernetes 会启动三个 Pod ，它们都用了同一个镜像，这意味着三个容器的内容都是一样的，它们唯一的区别在于它们的主机名，在上例中，第一个容器的主机名是 node-0，最后一个是 node-2。\n然后我们就可以根据它们的主机名来启动容器中不同的服务，比如第一个容器启动 resourcemanager，其它容器启动 nodemanager，这个智能的事情谁来做呢，那就是 bootstrap.sh 这个脚本。\n启动脚本# 其实 bootstrap.sh 做的事情不只是启动服务，还有一些自动化配置的动作，下面是部分示例：\n#!/bin/bash ID=${HOSTNAME##*-} for i in $(seq 0 $(($SCALE-1))); do echo -e \u0026#34;node-${i}\u0026#34; \u0026gt;\u0026gt; \u0026#34;$HADOOP_YARN_CONF_DIR/slaves\u0026#34; done if [ \u0026#34;ID\u0026#34; = \u0026#34;0\u0026#34; ]; then sbin/yarn-daemon.sh start resourcemanager else sbin/yarn-daemon.sh start nodemanager fiHOSTNAME 是我们前面说过的 node-0 这样的形式，这样我们在容器里就可以知道当前容器应该启动什么服务，SCALE 变量是我们在 ymal 文件中定义的，这样我们在容器里就可以知道一共有多少个 Yarn 节点。\n存储# 即然每个容器不一样，那它们都有自己的数据，这些数据需要持久化，也就是说容器挂掉然后重启，原来的数据应该还在才行，这个其实很好解决，Kubernetes 提供了多种持久化数据方案。\n首先需要提前创建一些 PV 以备使用，然后我们在定义 StatefulSet 时加上 PVC 即可：\napiVersion: apps/v1beta1 kind: StatefulSet metadata: name: node spec: serviceName: \u0026#34;node\u0026#34; replicas: 3 template: spec: containers: - name: node image: hadoop-yarn:v1 env: - name: SCALE value: \u0026#34;3\u0026#34; volumeMounts: - name: bigdata mountPath: /data volumeClaimTemplates: - metadata: name: bigdata spec: accessModes: [ \u0026#34;ReadWriteOnce\u0026#34; ] resources: requests: storage: 1Gi这样 Kubernetes 会为每个 Pod 挂载不同的 PVC ，在本例中，这些 PVC 将以 bigdata-node-0 的形式被命名，每个 PVC 会绑定到不同的 PV 上，比较完美地解决了我们需求。\n持续部署# 当上面的流程走通以后，基本就可以做到持续部署，当 Yarn 开发了新的版本后，我们只需用之前的 Dockerfile 重新构建一次镜像，并 push 到私有仓库，然后修改 yaml 文件中的 image 版本，再重新执行就可以在线升级我们的 Yarn 集群，如果升级失败还可以降回去。\n结语# Kubernetes 有多种插件为我们解决了 Docker 多主机通信的问，所以本文中并没有提及到。大数据平台中的大部分组件通过本文中的方式基本可以实现容器化，而实际中还有很多问题等待解决，比如集群的监控与管理、前端展示、集群配置的动态修改与查看等等。\n关于# 作者：张佳军\n阅读：53\n点赞：1\n创建：2017-09-16\n"},{"id":27,"href":"/bigdata/spark-performance-optimiz.html","title":"Spark on Yarn - 性能调优","section":"大数据","content":"Spark on Yarn - 性能调优# 自从领导让我接手了 Docker 方面的工作，虽然每天还是会跟大数据接触，但主要精力已转到了 Docker 上，感觉自己离大数据越来越远了，今天写一篇关于 Spark 调优的文章，也算是复习一下。\n环境描述# Spark 有多种运行模式，本次我们以 yarn-cluster 模式来运行，环境如下： Spark 版本：2.0.0 Spark 运行模式：yarn-cluster 计算节点数：10 单节点内存大小：64g 单节点 CPU 核数：16\nYarn配置# 即然用 Yarn 来管理资源，那么 Spark 最终运行性能也会受 Yarn 的影向，我们先来说明 Yanr 中几个关键配置参数。\nyarn.nodemanager.resource.memory-mb = 61440： 这个参数指每个（假设我们所有节点配置都是一样的）物理机要供献出多少内存给 Yarn，每个节点有 64G，减去给系统预留的 2G，NodeManager 和 DataNode 各占用 1G，如果没有其它服务的话，还剩下 60G，最终得出 60 * 1024 = 61440 ， 这只是个参考，实际上在系统开机以后已经没有 64G 了，而且每个节点上还不止这两个服务，如 Hbase、impala、logstash 等等，所以还要考虑这些服务所占用的资源。\nyarn.nodemanager.resource.cpu-vcores = 15： 这个值指单个节点要供献出多少CPU给Ynar来调度，同样要减去系统与其它服务所占用的核数，我们就以15为例了。 另外关于这个值还有个说法，比如物理机是12核，减去其它服务占用的2个，那么在配置里面写的时候就是15 * 1.5 = 22 或者 10 * 2 = 30，而我试过以后，觉得性能并没有明显变化。 这时，Yarn的总资源： 总内存：61440M * 10个节点 = 614400M 总核数：15核 * 10个节点 = 150核\nyarn.scheduler.maximum-allocation-mb = 61440 = 60G： 这个值用来限制Yanr中的每个container可使用的最大内存，应该设为小于或等于单个节点的内存大小，也就是 60G yarn.scheduler.minimum-allocation-mb = 512 这个值用来限制 Yanr 中的每个 container 可使用的最小内存，一般为 512 或 1G，太大会造成资源浪费 yarn.scheduler.maximum-allocation-vcores = 15 这个值用来限制 Yanr 中的每个 container 可使用的最大 CPU 核数，应该设为小于或等于单个节点的核数，也就是15。\nyarn.scheduler.minimum-allocation-vcores = 1： 这个值用来限制 Yanr 中的每个 container 可使用的最小 CPU 核数，就给1个吧。\nyarn.nodemanager.resource.percentage-physical-cpu-limit = 100： 这个值是用来限制 Yarn 容器可使用的所有 CPU 的百分比，上面写的是 physical，但官没说这个百分比是针对物理机还是我们给 Yarn 分配的 10 核，总之，如果你的这个值不是 100 的话，就把它设为 100 吧。\nSpark配置# 提交任务时，Spark默认会读取 conf/spark-defaults.conf 内的参数，下面一个示例配置：\n[user@node1 spark]$ vi conf/spark-defaults.conf spark.master yarn-cluster spark.shuffle.service.enabled true spark.eventLog.enabled true spark.eventLog.dir hdfs:///spark-history/ spark.executor.instances 26 spark.executor.memory 11G spark.executor.cores 2 spark.default.parallelism 60 spark.driver.memory 25g spark.driver.cores 5 spark.dynamicAllocation.enabled false我们先来说明一下它的运行原理，下面是一个示例任务：\nbin/spark-submit \\ --class com.clean.MyClass \\ lib/clean-data.jar \\ /input/data.txt /output/ ...当我们提交一个 Spark 任务后，它会在各个节点上分配多个 Executor，Executor 的数量可以由用户指定，然后用户提交的任务被分解成多个 Job，每全 Job 划分成多个 Stage，每个 Stage 中包括很多 Task，如果你启动了 Spark-History-Server，可以通这 WEB UI 看到它们的执行过程。 上面配置文件的 spark.dynamicAllocation.enabled 这项参数很关键，如果指定为 true，用户可以省去 spark.executor.instances 这项配置，Spark 将自动分配 Executor 数量，大多数情况下它可以较好地执行，但它会用最保守的方案把任务执行完，性能并不是最优的，而一般我们在写一个 Spark 程序时是知道数据量有多大的，因此有时候我们需要自己指定 Executor 数据与它们可使用的资源。\n参数说明# 如果我们要处理一个100g的文件，可以怎样规划计算资源？上面配置中有6个重要的参数，我们一个一个说明。\nspark.executor.instances： 首先共有10台机器，所以它的值不能小于10，这样才能让所有机器并行跑起来，其次每台机器有15个核，那么每台机器上的 Executor 数量不能超过 15。\nspark.executor.cores：每个 Executor 会执行很多 Task，如果只给每个 Executor 分配一个核，那这个 Executor 内所有的 Task 就只能一个一个执行了，所以这个值决定了每个 Executor 的计算能力，每个节点有 15 个核，如果这节点上有 2 个 Executor，那每个 Executor 最多只能给 7 个核了。\nspark.executor.memory：这个值跟上面同理，根据 Executor 数量决定，与 spark.executor.cores 也有关系，假设我们给每个 Executor 分配了两个核，而内存却给了 100G，也就是说两个 Task 每人可以分到 50G 的内存！那结果只有少部分内存会被利用起来。\n以个人经验，每个 Task 平均分到的内存应控制在 3G 到 10G 之间为好，太小不能发挥集群性能，也有可能 OOM，而太大无疑会降低任务整体的并行度，我们就先设为 5G。\n参数取值# 如上，我们设每个 Task 平均到的内存为 5G，所以单节点上可以同时跑的 Task 数量为 60G / 5G = 12 个，这又意味着单节点有 12 个核被利用起来，而单节点核数 15 个，看起来已经接近最优了，因为每个节点的内存跟 CPU 都用到了大部分，上面说过每个节点上的 Executor 不能超过 15，且每个 Executor 的核至少为 2，如果每个节点 3 个 Executor，每个 Executor 分配 4 个核，3 * 4 = 12，跟上面的结论接近。\nspark.executor.instances：的值就等于 3 * 10节点 - 1 = 29，为什么减掉一个呢，因为还有一个 SparkDriver，它也是一个 Executor，所以 29 加上一个 Driver 刚好为 30 个 Executor，可以均匀分布在 10 个节点上。\nspark.executor.cores：的值就等于上面说的 4。\nspark.executor.memory：的值等于 60G / 3个Executor = 20G。\nspark.driver.memory：上面算出来每个 Executor 的内存为 20G，因为已经没有多余的内存，所以它最多也只有 20G 了。\nspark.driver.cores：同理，它也是 4，现在我们还有 150 - 4 * 30 = 30 个核，也可以多给它一些。\nspark.default.parallelism：这个指所有任务的并行度，它的值应该给到所有 Executor 的两到三倍，30 * 2 = 60。\n当然上面这 6 项参数也可以在自己的 Spark 程序中指定，还可以在提交任务时指定，这样当执行不同的任务就可以使用不同的配置，现在用上面计算出来的值来提交任务：\nbin/spark-submit \\ --num-executors 29 \\ --executor-cores 4 \\ --executor-memory 20G \\ --driver-memory 20 \\ --driver-cores 4 \\ --class com.clean.MyClass \\ lib/clean-data.jar \\ /input/data.txt /output-End- 关于# 作者：张佳军\n阅读：206\n点赞：4\n创建：2017-03-25\n"},{"id":28,"href":"/bigdata/spark-performance-test.html","title":"Spark - 性能测试","section":"大数据","content":"Spark - 性能测试# 本文介绍一种测试 Spark 性能的方法，也可以用来测试 Spark SQL 模块对 SQL 语句的覆盖率，主要用 TPC-DS 这个工具，希望可以帮到一些同学。\n什么是 TPC-DS# TPC-DS 是事务性能管理委员会 (TPC) 发布的大数据测试基准 它可以生成 99 个测试案例，遵循 SQL'99 和 SQL 2003 的语法标准，SQL 案例比较复杂 分析的数据量大，并且测试案例是在回答真实的商业问题 测试案例中包含各种业务模型（如分析报告型，迭代式的联机分析型，数据挖掘型等） 几乎所有的测试案例都有很高的 IO 负载和 CPU 计算需求 它有常用的两个功能: 按指定规模生成测试数据 按指定的 SQL 标准生成测试用的 99 条 SQL 语句 基本原理# 首先用 TPC-DS 生成测试数据与 99 条 SQL 语句，然后将数据导入 Hive，配置 Spark 使其能够访问到 Hive 元数据，这时启用 Spark 自身的 spark thrift server 服务，如果没有了解过，可以参考这里，向这个 Server 发送 SQL 语句，最后获取其执行时间并记录下来。\n生成数据与sql语句# 下载最新版 tpc-ds-tool.zip, 放在 /root/modules 目录下，官网地址：www.tpc.org\n解压并得到一个目录 v2.1.0:\ncd /root/modules unzip tpc-ds-tool.zip编译\nsudo yum install –y gcc cd /root/modules/v2.1.0/tools/ make生成测试数据：\n./dsdgen -scale 100 -dir /root/modules/data100/命令意思是在 /root/modules/data100/ 目录下生成规模为 100G 的测试数据 完成后会在 /root/modules/data100/ 目录下生成 25 个文件。\n将生成的 25 个数据文件加载到对应的 Hive 表中, 以 call_center 表为例, 命令如下:\nhive\u0026gt; load data local inpath \u0026#34;/root/modules/data100/call_center.dat\u0026#34; overwrite into table call_center.dbgen_version;修改 SQL 模板文件:\ncd /root/modules/v2.1.0/query_templates for i in {1..99}; do sed -i \u0026#39;1s/^/define _END = \u0026#34;\u0026#34;;\\n/\u0026#39; query$i.tpl; done成生 SQL 语句:\nmkdri -p /data/sql_100g for i in {1..99}; do echo query$i.tpl \u0026gt; ../query_templates/tpl.lst ./dsqgen -directory ../query_templates –template ../query_templates/query1.tpl -dialect netezza -scale 100 -output_dir /data/tpc-ds/sql_100g/ -input ../query_templates/tpl.lst mv /data/tpc-ds/sql_100g/query_0.sql /data/tpc-ds/sql_100g/query_$i.sql done在每条测试 SQL 语句前添加数据库名\ncd /data/sql_100g for i in {1..99}; do sed -i \u0026#39;1s/^/use leap_text_100g;\\n/\u0026#39; query_$i.sql; done运行测试 SQL 并保存测试结果\ncd $SPARK_HOME for i in `seq 1 99` do bin/beeline -u \u0026#34;jdbc:hive2://host1.kxdmmr.com:10016/default;httpPath=cliservice;transportMode=http\u0026#34; -f /data/tpc-ds/sql_100gquery_$i.sql echo -e \u0026#34;$i\\t$?\u0026#34; \u0026gt;\u0026gt; leap_text_100g.tsv done参考# Spark Thrift Server官方介绍 Spark Thrift Server使用与配置 关于# 作者：张佳军\n阅读：510\n点赞：2\n创建：2017-02-26\n"},{"id":29,"href":"/bigdata/spark-source-code-1.html","title":"Spark - 源码分析（一）","section":"大数据","content":"Spark - 源码分析（一）# 最近抽时间读了下 Spark 源码，最初只是想看看它是怎样跟 HDFS 交互的，每个 task 怎样在它所在的机器上找到及读取数据块的，这一看就是好几天，罢了，那就从头到尾研究一下吧，准备每周更新一篇。在这里记录的只是我所理解的东西，这样一个庞大的软件，如果没有当初的设计图纸是很难梳理清楚其全部原理的，我就看到哪写到哪吧。\nSpark简介# Spark 是一个基于内存的大数据处理框架，官方称 Spark 比M apReduce 的处理速度快 10 到 100 倍。它最初在 2009 年由加州大学伯克利分校的 AMPLab 开发，并于 2010 年成为 Apache 的开源项目之一。\nSpark版本：2.1.0\n部署模式：Spark on Yarn\n我们就从提交一个任开始，看看Spark都做了些什么，先写一个简单的Spark任务，如下：\npackage com.example import org.apache.spark.{SparkContext,SparkConf} object SparkCode { def main(args: Array[String]): Unit = { val spark = SparkSession .builder .appName(\u0026#34;Spark Pi\u0026#34;) .getOrCreate() spark.sparkContext.textFile(\u0026#34;hdfs:///spark/input/file.txt\u0026#34;, 3) .map { x =\u0026gt; (x, 1)} .reduceByKey(_ + _) .saveAsTextFile(\u0026#34;hdfs:///spark/output\u0026#34;) spark.stop() } }将上面代码打成Jar包，然后通过 spark-submit 命令提交这个任务，提交以后它在干什么？\n解析命令行# 首先，从这个 spark-submit 命令说起，它是一个 shell 脚本，它里面最终调用了 spark-class 这个脚本，并且把所有参数也带过去，这个 spark-class 做了一些环境变量设置，如：JAVA_HOME CLASS-PATH SPARK_HOME 等等，设置好环境变量以后调起了一个Java类：org.apache.spark.launcher.Main，这个类的作用是分析所有参数合法性，并根据用户提供的指定的集群部署方式决定这个任务的提交方式，拼接出最终执行命令并打印到标准输出流中，这时又回到 spark-class 这个脚本中，它会从标准输出流中捕获到刚才拼接好的最终提交任务命令，这时又调起一个Java类：org.apache.spark.deploy.SparkSubmit，它又会分析一次用户提供的所有参数，用反射封装用户指定的类，也就是我们上面写的 com.example.SparkCode 类，并找到类中的 main 方法，然后执行它。\n初始化SparkSession# 进入我们的 main 方法后，先创建了一个 SparkSession 对象，在 Spark2.0 之前是直接创建 SparkContext 对象，在 Spark2.0 之后有了 SparkSession，这个对象中包含了 SparkContext 与 SparkSQL，并提供一些创建 DataFrame 的方法，DataFrame 在 Spark2.0 中也是个比较突出的概念，它又是一个更高级的抽象，可以同时用 RDD 算子和 SQL 语法对其进行处理，好了，接下来介绍重量级的人物：SparkContext\n初始化SparkContext# SparkContext 是 Spark 功能的主要入口，即然这么重要，先来看看它包含了什么，下面列出它的几个重要成员：\npackage org.apache.spark class SparkContext(config: SparkConf) extends Logging { private var _conf: SparkConf = _ private var _env: SparkEnv = _ private var _jobProgressListener: JobProgressListener = _ private var _executorMemory: Int = _ private var _taskScheduler: TaskScheduler = _ private var _schedulerBackend: SchedulerBackend = _ private var _applicationId: String = _ @volatile private var _dagScheduler: DAGScheduler = _ ... def runJob[T, U: ClassTag]( rdd: RDD[T], func: (TaskContext, Iterator[T]) =\u0026gt; U, partitions: Seq[Int]): Array[U] = { val results = new Array[U](partitions.size) runJob[T, U](rdd, func, partitions, (index, res) =\u0026gt; results(index) = res) results } ...除此之外，它还包含各种创建 RDD 的方法，比如我们常用的 testFile() 等，看到这些就知道它的重要性了，当 SparkContext 被创建时，这些成员也会被创建出来，来简单说说这些成员的作用：\n_dagScheduler：它的作用是根据各 RDD 间的依赖情况整个 Job 切分成不同的 Stage，它包含了两个集合：\nclass DAGScheduler( ... val jobIdToStageIds = new HashMap[Int, HashSet[Int]] //这个用来存放每个job对应哪些Stage val stageIdToStage = new HashMap[Int, Stage] //这个存放具体的Stage对象 ..._applicationId：看到这个成员，应该知道一个 SparkContext 对象就对应用户提交的一个 Application\n_taskScheduler：它来决定哪些 tash 跑在哪个 Executor 上\n_schedulerBackend：它于 _taskScheduler 配合，一起完成 task 分发工作。\n其中 _taskScheduler 和 _schedulerBackend 的初始化与用户指定的 masterUrl 有关，见如下代码：\nprivate def createTaskScheduler( sc: SparkContext, master: String, deployMode: String): (SchedulerBackend, TaskScheduler) = { import SparkMasterRegex._ // When running locally, don\u0026#39;t try to re-execute tasks on failure. val MAX_LOCAL_TASK_FAILURES = 1 master match { case \u0026#34;local\u0026#34; =\u0026gt; val scheduler = new TaskSchedulerImpl(sc, MAX_LOCAL_TASK_FAILURES, isLocal = true) val backend = new LocalSchedulerBackend(sc.getConf, scheduler, 1) scheduler.initialize(backend) (backend, scheduler) ... case masterUrl =\u0026gt; val cm = getClusterManager(masterUrl) match { case Some(clusterMgr) =\u0026gt; clusterMgr case None =\u0026gt; throw new SparkException(\u0026#34;Could not parse Master URL: \u0026#39;\u0026#34; + master + \u0026#34;\u0026#39;\u0026#34;)提交Job# SparkContext 初始化完成后就到了我们加载的 hdfs 文件这部分，textFile()` 方法是在 SparkContext 内定义的，它最终创建了一个 HadoopFile 对象：\ndef hadoopFile[K, V]( path: String, inputFormatClass: Class[_ \u0026lt;: InputFormat[K, V]], keyClass: Class[K], valueClass: Class[V], minPartitions: Int = defaultMinPartitions): RDD[(K, V)] = withScope { ... new HadoopRDD( this, confBroadcast, Some(setInputPathsFunc), inputFormatClass, keyClass, valueClass, minPartitions).setName(path) }这个HadoopFile类继承了 RDD 类，也就是说我们调用的 textFile() 这个方法最终返回了一个 RDD， 下一步我们又调用了这个 RDD 的几个算子方法，我们看看 RDD 里有哪些方法：\nabstract class RDD[T: ClassTag]( .. def map[U: ClassTag](f: T =\u0026gt; U): RDD[U] = withScope { val cleanF = sc.clean(f) new MapPartitionsRDD[U, T](this, (context, pid, iter) =\u0026gt; iter.map(cleanF)) } ... def foreach(f: T =\u0026gt; Unit): Unit = withScope { val cleanF = sc.clean(f) sc.runJob(this, (iter: Iterator[T]) =\u0026gt; iter.foreach(cleanF)) } ... private[spark] def collectPartitions(): Array[Array[T]] = withScope { sc.runJob(this, (iter: Iterator[T]) =\u0026gt; iter.toArray) } ...看到 sc.runJob() 了吗，是的，它就是之前在初始化 SparkContext 小节所提到的 runJob 方法，至于为什么有的算子调用了 sc.runJob() 而有的算子却没调用，这就和 Spark 中的宽依赖和窄依赖有关了，窄依赖的算子不会触发实际的计算任务而只会被记录下来。\n今天就先写到这里，下一篇接着 runJob() 这个方法继续分析。\n关于# 作者：张佳军\n阅读：63\n点赞：4\n创建：2017-04-16\n"},{"id":30,"href":"/bigdata/spark-source-code-2.html","title":"Spark - 源码分析（二）","section":"大数据","content":"Spark - 源码分析（二）# 小节：Scala中的闭包# 本篇开始之前，先说一下闭包的概念，一会要用到，在 Scala 中的写一个闭包是很容易的，甚至比 js 还简单，如下：\nobject Test { var base = 3 val myFunc = (i:Int) =\u0026gt; i * base def main(args: Array[String]) { println(myFunc(5)) } }如上的 myFunc 函数依赖了一个外部变量 base，也就是说 base 成为了 myFunc 函数的一部分，因为 myFunc 函数被其它地方引用，所以使得 base 变量也不会被GC回收，这样就形成了一个闭包。\n提交Job# 上篇中讲到了，假如我们在 RDD 对象上调用了产生宽依赖的算子时，那么该算子将最终会调用 SparkContext 类中的 runJob 方法，本篇中我们接着这个 runJob 方法继续往下看，先回顾下调用 runJob 的地方，以下是RDD类中的一个算子：\n... def foreach(f: T =\u0026gt; Unit): Unit = withScope { val cleanF = sc.clean(f) sc.runJob(this, (iter: Iterator[T]) =\u0026gt; iter.foreach(cleanF)) } ...参数列表中的 f，就是我们在调用 foreach 的时候，自己写的匿名函数，在这里它被另一个匿名函数引用，这样就形成了一个闭包，最后将这个闭包函数传递给了 runJob 方法。以下是 runJob 方法的原型：\n... /** * Run a function on a given set of partitions in an RDD and pass the results to the given * handler function. This is the main entry point for all actions in Spark. * * @param rdd target RDD to run tasks on * @param func a function to run on each partition of the RDD * @param partitions set of partitions to run on; some jobs may not want to compute on all * partitions of the target RDD, e.g. for operations like `first()` * @param resultHandler callback to pass each result to */ def runJob[T, U: ClassTag]( rdd: RDD[T], func: (TaskContext, Iterator[T]) =\u0026gt; U, partitions: Seq[Int], resultHandler: (Int, U) =\u0026gt; Unit): Unit = { if (stopped.get()) { throw new IllegalStateException(\u0026#34;SparkContext has been shutdown\u0026#34;) } val callSite = getCallSite val cleanedFunc = clean(func) logInfo(\u0026#34;Starting job: \u0026#34; + callSite.shortForm) if (conf.getBoolean(\u0026#34;spark.logLineage\u0026#34;, false)) { logInfo(\u0026#34;RDD\u0026#39;s recursive dependencies:\\n\u0026#34; + rdd.toDebugString) } dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get) progressBar.foreach(_.finishAll()) rdd.doCheckpoint() } ...小节：clean()函数的作用# 刚开始看到这里的时候，不太明白 val cleanedFunc = clean(func) 这一步是在干嘛，而且很多地方都用到了这个方法，后来上网查找一翻才大概明白，前面说过，这里的 func 是一个闭包，而闭包在执行时会自动捕获它依赖的外部变量，在将闭包函数分发到其它机器上时，会将这个依赖的变量也一并发过去，这一步是在清除一些多余的外部变量，以免浪费集群带宽。\n上面的 runJob() 在最后又调用的 DAGScheduler 类中的 runJob 方法，然后又调用 DAGScheduler 的 submitJob 方法，submitJob 会把我们的 RDD 与闭包函数等封装成JobSubmitted 对象，这个对象是 DAGSchedulerEvent 的子类，并将其加入到 eventProcessLoop 对象的 eventQueue 集合中，这个 DAGScheduler 在实例化的时候调用了这样一句调代码：\nprivate[spark] class DAGScheduler( ... private[scheduler] val eventProcessLoop = new DAGSchedulerEventProcessLoop(this) ... eventProcessLoop.start() }Stage的生成及划分# 这个 eventProcessLoop 对象是 EventLoop 的子类，如上 start() 方法会调用父类的 start() 方法，这时会创建一个新线程，循环取出 eventQueue 集合中的 DAGSchedulerEvent 对象，然后交由子类 eventProcessLoop 的 doOnReceive() 方法，根据 DAGSchedulerEvent 不同子类型调用 DAGScheduler 的不同方法，来做出不同的动作，其中包括 Stage 的划分与依赖计算，如下是 doOnReceive() 方法的定义：\nprivate def doOnReceive(event: DAGSchedulerEvent): Unit = event match { case JobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties) =\u0026gt; dagScheduler.handleJobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties) case MapStageSubmitted(jobId, dependency, callSite, listener, properties) =\u0026gt; dagScheduler.handleMapStageSubmitted(jobId, dependency, callSite, listener, properties) case StageCancelled(stageId, reason) =\u0026gt; dagScheduler.handleStageCancellation(stageId, reason) case JobCancelled(jobId, reason) =\u0026gt; dagScheduler.handleJobCancellation(jobId, reason) case JobGroupCancelled(groupId) =\u0026gt; dagScheduler.handleJobGroupCancelled(groupId) ...handleJobSubmitted() 和 handleMapStageSubmitted() 两个方法会将取出来的Job根据所属的 DAGSchedulerEvent 不同子类，来创建不同的 Stage，handleJobSubmitted() 创建的是 ResultStage，而 handleMapStageSubmitted() 创建 的是 ShuffleMapStage，它们其实都是 Stage 的子类，handleMapStageSubmitted() 创建的是 ShuffleMapStage 类型的的 Stage，这种 Stage 对象中会包含父 Stage 的信息，创建好 Stage 后又会创建一个相应的 ActiveJob，再调用 DAGScheduler 的 submitStage() 方法，此方法定义如下：\nprivate def submitStage(stage: Stage) { val jobId = activeJobForStage(stage) if (jobId.isDefined) { logDebug(\u0026#34;submitStage(\u0026#34; + stage + \u0026#34;)\u0026#34;) if (!waitingStages(stage) \u0026amp;\u0026amp; !runningStages(stage) \u0026amp;\u0026amp; !failedStages(stage)) { val missing = getMissingParentStages(stage).sortBy(_.id) logDebug(\u0026#34;missing: \u0026#34; + missing) if (missing.isEmpty) { logInfo(\u0026#34;Submitting \u0026#34; + stage + \u0026#34; (\u0026#34; + stage.rdd + \u0026#34;), which has no missing parents\u0026#34;) submitMissingTasks(stage, jobId.get) } else { for (parent \u0026lt;- missing) { submitStage(parent) } waitingStages += stage } } } else { abortStage(stage, \u0026#34;No active job for stage \u0026#34; + stage.id, None) } }看出来了吗，这是一个递归方法，不管是哪种 RDD 传进来以后，都会先检查其有没有依赖的父 Stage，如果有父 Stage 且还没有被计算，那么先提交父 Stage，如此递归下去，那么所有被依赖的 Stage 中的最顶层的 Stage 将最先被执行，这时会调起 submitMissingTasks(stage, jobId.get) 这个方法。\n这一篇就先分析到这里，下篇继续。\n关于# 作者：张佳军\n阅读：37\n点赞：1\n创建：2017-04-22\n"},{"id":31,"href":"/bigdata/spark-source-code-3.html","title":"Spark - 源码分析（三）","section":"大数据","content":"Spark - 源码分析（三）# 生成Task# 上篇中我们讲到Spark在提交一个Stage后，DAGScheduler中的submitStage()方法会以递归的方式找到该Stage依赖的最上层的父Stage，找到后会将这个最上层的Stage传给submitMissingTasks()方法，该方法定义如下：\n... /** Called when stage\u0026#39;s parents are available and we can now do its task. */ private def submitMissingTasks(stage: Stage, jobId: Int) { logDebug(\u0026#34;submitMissingTasks(\u0026#34; + stage + \u0026#34;)\u0026#34;) ... runningStages += stage stage match { case s: ShuffleMapStage =\u0026gt; outputCommitCoordinator.stageStart(stage = s.id, maxPartitionId = s.numPartitions - 1) case s: ResultStage =\u0026gt; outputCommitCoordinator.stageStart( stage = s.id, maxPartitionId = s.rdd.partitions.length - 1) } val taskIdToLocations: Map[Int, Seq[TaskLocation]] = try { stage match { case s: ShuffleMapStage =\u0026gt; partitionsToCompute.map { id =\u0026gt; (id, getPreferredLocs(stage.rdd, id))}.toMap case s: ResultStage =\u0026gt; partitionsToCompute.map { id =\u0026gt; val p = s.partitions(id) (id, getPreferredLocs(stage.rdd, p)) }.toMap } ... taskBinary = sc.broadcast(taskBinaryBytes) ...上面只贴出了该方法的一部分，我们刚才说的父Stage被传进来后，做了几件事情，我们分步骤说一下：\n首先先将该Stage加入一个正在运行的Stage任务队列，然后根据这个Stage的所属类型（上篇中说到Stage有两个子类，ResultStage和ShuffleMapStage）设置这个Stage最大分区ID，也就是outputCommitCoordinator.stageStart()方法，它并不用来启动Stage线程的 第二步，根据RDD的分区数生成Task的ID，并决定了每个Task将被放在哪个Executor中执行，这些对应信息被放在taskIdToLocations这个MAP集合中。 第三步，根据Stage类型创建一个Task序列化器。 第四步，根据Stage类型来生成不同类型的Task，Task类型有两种：ResultTask和ShuffleMapTask，再根据RDD的分区数这个关键因素，来生成多个Task，并将每个Task序列化生二进制。 第五步，通过taskScheduler.submitTasks()方法来提交这些生成好的Task。 提交Task# TaskSchedulerImpl是TaskScheduler的子类，它重写了父类的submitTasks方法，上面的taskScheduler.submitTasks()调用的其实就是TaskSchedulerImpl类重写后的方法，原型如下：\noverride def submitTasks(taskSet: TaskSet) { val tasks = taskSet.tasks logInfo(\u0026#34;Adding task set \u0026#34; + taskSet.id + \u0026#34; with \u0026#34; + tasks.length + \u0026#34; tasks\u0026#34;) this.synchronized { val manager = createTaskSetManager(taskSet, maxTaskFailures) val stage = taskSet.stageId val stageTaskSets = taskSetsByStageIdAndAttempt.getOrElseUpdate(stage, new HashMap[Int, TaskSetManager]) stageTaskSets(taskSet.stageAttemptId) = manager val conflictingTaskSet = stageTaskSets.exists { case (_, ts) =\u0026gt; ts.taskSet != taskSet \u0026amp;\u0026amp; !ts.isZombie } if (conflictingTaskSet) { throw new IllegalStateException(s\u0026#34;more than one active taskSet for stage $stage:\u0026#34; + s\u0026#34; ${stageTaskSets.toSeq.map{_._2.taskSet.id}.mkString(\u0026#34;,\u0026#34;)}\u0026#34;) } schedulableBuilder.addTaskSetManager(manager, manager.taskSet.properties) if (!isLocal \u0026amp;\u0026amp; !hasReceivedTask) { starvationTimer.scheduleAtFixedRate(new TimerTask() { override def run() { if (!hasLaunchedTask) { logWarning(\u0026#34;Initial job has not accepted any resources; \u0026#34; + \u0026#34;check your cluster UI to ensure that workers are registered \u0026#34; + \u0026#34;and have sufficient resources\u0026#34;) } else { this.cancel() } } }, STARVATION_TIMEOUT_MS, STARVATION_TIMEOUT_MS) } hasReceivedTask = true } backend.reviveOffers() }这个方法最后用到了backend对象，这个对象是SchedulerBackend类型，这一个特质（相当于Java中的接口），其下面有多个实现，它是在创建SparkContext的时候，根据用户指定的\u0026ndash;master参数来创建的，如果指定的Yarn模式的话，这个具体的实现类是YarnClusterSchedulerBackend，它又是CoarseGrainedSchedulerBackend的子类，这个reviveOffers()就是定义在reviveOffers()中的，YarnClusterSchedulerBackend中并没有重写这个方法，也就是说这里其实是调用了CoarseGrainedSchedulerBackend中的reviveOffers()方法\noverride def reviveOffers() { driverEndpoint.send(ReviveOffers) }方法中的ReviveOffers是一个样例类，在这里被当作一种消息经序列化后被RPC框架传输，这个ReviveOffers会被发送给SparkDriver，SparkDriver端也有一个相应的方法来接收消息\noverride def receive: PartialFunction[Any, Unit] = { case StatusUpdate(executorId, taskId, state, data) =\u0026gt; scheduler.statusUpdate(taskId, state, data.value) if (TaskState.isFinished(state)) { executorDataMap.get(executorId) match { case Some(executorInfo) =\u0026gt; executorInfo.freeCores += scheduler.CPUS_PER_TASK makeOffers(executorId) case None =\u0026gt; // Ignoring the update since we don\u0026#39;t know about the executor. logWarning(s\u0026#34;Ignored task status update ($taskId state $state) \u0026#34; + s\u0026#34;from unknown executor with ID $executorId\u0026#34;) } } case ReviveOffers =\u0026gt; makeOffers()Driver在收到ReviveOffers后调用了makeOffers()方法\n// Make fake resource offers on all executors private def makeOffers() { // Make sure no executor is killed while some task is launching on it val taskDescs = CoarseGrainedSchedulerBackend.this.synchronized { // Filter out executors under killing val activeExecutors = executorDataMap.filterKeys(executorIsAlive) val workOffers = activeExecutors.map { case (id, executorData) =\u0026gt; new WorkerOffer(id, executorData.executorHost, executorData.freeCores) }.toIndexedSeq scheduler.resourceOffers(workOffers) } if (!taskDescs.isEmpty) { launchTasks(taskDescs) } }取出所有活着的Executor，activeExecutors里面存放着executorData对象，每个executorData里记录着自已所在的主机，RPC地址，剩余的Core数等，然后从中提取出每个Executor的资源信息，封装为WorkerOffer返回，还是上代码吧，没有什么比这更直观了\nprivate[org.apache.spark.scheduler.cluster] class ExecutorData(val executorEndpoint: RpcEndpointRef, val executorAddress: RpcAddress, override val executorHost: String, var freeCores: Int, override val totalCores: Int, override val logUrlMap: Map[String, String]) extends ExecutorInfo为Task分配资源# 取出资源信后就开始为所有Task分配资源了，也就是上面的resourceOffers(workOffers)方法\ndef resourceOffers(offers: IndexedSeq[WorkerOffer]): Seq[Seq[TaskDescription]] = synchronized { // Mark each slave as alive and remember its hostname // Also track if new executor is added var newExecAvail = false for (o \u0026lt;- offers) { if (!hostToExecutors.contains(o.host)) { hostToExecutors(o.host) = new HashSet[String]() } if (!executorIdToRunningTaskIds.contains(o.executorId)) { hostToExecutors(o.host) += o.executorId executorAdded(o.executorId, o.host) executorIdToHost(o.executorId) = o.host executorIdToRunningTaskIds(o.executorId) = HashSet[Long]() newExecAvail = true } for (rack \u0026lt;- getRackForHost(o.host)) { hostsByRack.getOrElseUpdate(rack, new HashSet[String]()) += o.host } } // Before making any offers, remove any nodes from the blacklist whose blacklist has expired. Do // this here to avoid a separate thread and added synchronization overhead, and also because // updating the blacklist is only relevant when task offers are being made. blacklistTrackerOpt.foreach(_.applyBlacklistTimeout()) val filteredOffers = blacklistTrackerOpt.map { blacklistTracker =\u0026gt; offers.filter { offer =\u0026gt; !blacklistTracker.isNodeBlacklisted(offer.host) \u0026amp;\u0026amp; !blacklistTracker.isExecutorBlacklisted(offer.executorId) } }.getOrElse(offers) val shuffledOffers = shuffleOffers(filteredOffers) // Build a list of tasks to assign to each worker. val tasks = shuffledOffers.map(o =\u0026gt; new ArrayBuffer[TaskDescription](o.cores)) val availableCpus = shuffledOffers.map(o =\u0026gt; o.cores).toArray val sortedTaskSets = rootPool.getSortedTaskSetQueue for (taskSet \u0026lt;- sortedTaskSets) { logDebug(\u0026#34;parentName: %s, name: %s, runningTasks: %s\u0026#34;.format( taskSet.parent.name, taskSet.name, taskSet.runningTasks)) if (newExecAvail) { taskSet.executorAdded() } } // Take each TaskSet in our scheduling order, and then offer it each node in increasing order // of locality levels so that it gets a chance to launch local tasks on all of them. // NOTE: the preferredLocality order: PROCESS_LOCAL, NODE_LOCAL, NO_PREF, RACK_LOCAL, ANY for (taskSet \u0026lt;- sortedTaskSets) { var launchedAnyTask = false var launchedTaskAtCurrentMaxLocality = false for (currentMaxLocality \u0026lt;- taskSet.myLocalityLevels) { do { launchedTaskAtCurrentMaxLocality = resourceOfferSingleTaskSet( taskSet, currentMaxLocality, shuffledOffers, availableCpus, tasks) launchedAnyTask |= launchedTaskAtCurrentMaxLocality } while (launchedTaskAtCurrentMaxLocality) } if (!launchedAnyTask) { taskSet.abortIfCompletelyBlacklisted(hostToExecutors) } } if (tasks.size \u0026gt; 0) { hasLaunchedTask = true } return tasks }在这个方法中大概做的事情如下： 在所有Executor中过滤掉被加入黑名单的Executor 随机打乱Executor的顺序 为每个Task分配资源\n分发Task到对应的Executor# 分配完资源后就回到了makeOffers()方法，下一步就是发射所有Task了，也就是launchTasks(taskDescs)方法\n// Launch tasks returned by a set of resource offers private def launchTasks(tasks: Seq[Seq[TaskDescription]]) { for (task \u0026lt;- tasks.flatten) { val serializedTask = TaskDescription.encode(task) if (serializedTask.limit \u0026gt;= maxRpcMessageSize) { scheduler.taskIdToTaskSetManager.get(task.taskId).foreach { taskSetMgr =\u0026gt; try { var msg = \u0026#34;Serialized task %s:%d was %d bytes, which exceeds max allowed: \u0026#34; + \u0026#34;spark.rpc.message.maxSize (%d bytes). Consider increasing \u0026#34; + \u0026#34;spark.rpc.message.maxSize or using broadcast variables for large values.\u0026#34; msg = msg.format(task.taskId, task.index, serializedTask.limit, maxRpcMessageSize) taskSetMgr.abort(msg) } catch { case e: Exception =\u0026gt; logError(\u0026#34;Exception in error callback\u0026#34;, e) } } } else { val executorData = executorDataMap(task.executorId) executorData.freeCores -= scheduler.CPUS_PER_TASK logDebug(s\u0026#34;Launching task ${task.taskId} on executor id: ${task.executorId} hostname: \u0026#34; + s\u0026#34;${executorData.executorHost}.\u0026#34;) executorData.executorEndpoint.send(LaunchTask(new SerializableBuffer(serializedTask))) } } }循环取出每个Task，将其序列化，然后封成LaunchTask发送到某个Executor，至于是哪个Executor，这是在前面生成Task那一步已经分配好的，如果忘了可以翻回去看一下，这个LaunchTask被RPC框架发送到Executor进行处理，具体实现在CoarseGrainedExecutorBackend中\noverride def receive: PartialFunction[Any, Unit] = { case RegisteredExecutor =\u0026gt; logInfo(\u0026#34;Successfully registered with driver\u0026#34;) try { executor = new Executor(executorId, hostname, env, userClassPath, isLocal = false) } catch { case NonFatal(e) =\u0026gt; exitExecutor(1, \u0026#34;Unable to create executor due to \u0026#34; + e.getMessage, e) } case RegisterExecutorFailed(message) =\u0026gt; exitExecutor(1, \u0026#34;Slave registration failed: \u0026#34; + message) case LaunchTask(data) =\u0026gt; if (executor == null) { exitExecutor(1, \u0026#34;Received LaunchTask command but executor was null\u0026#34;) } else { val taskDesc = TaskDescription.decode(data.value) logInfo(\u0026#34;Got assigned task \u0026#34; + taskDesc.taskId) executor.launchTask(this, taskDesc) } case KillTask(taskId, _, interruptThread) =\u0026gt; if (executor == null) { exitExecutor(1, \u0026#34;Received KillTask command but executor was null\u0026#34;) } else { executor.killTask(taskId, interruptThread) } ...将接收到的Task反序列化，然后又调用了Executor类中的launchTask方法\ndef launchTask(context: ExecutorBackend, taskDescription: TaskDescription): Unit = { val tr = new TaskRunner(context, taskDescription) runningTasks.put(taskDescription.taskId, tr) threadPool.execute(tr) }可以看到它创建了一个TaskRunner对象并将它加入线程池开始执行，这个TaskRunner其实就是实现了Java的Runnable接口，并且可以看出，每一个Task都持有一个Executor的上下文，也就是那个context对象，即然TaskRunner实现了Runner接口，那也就重写了run()方法，接下来我们进run()方法看看TaskRunner中的执行逻辑。\n今天就分析先到这里，下一篇继续分析。\n关于# 作者：张佳军\n阅读：22\n点赞：0\n创建：2017-04-29\n"},{"id":32,"href":"/bigdata/spark-source-code-4.html","title":"Spark – 源码分析（四）","section":"大数据","content":"Spark – 源码分析（四）# Task 的执行# 上篇讲到TaskRunner类，这个类是定义在Executor类中，实现了Java的Runnable接口，Spark运行过程中的一个Task就是一个TaskRunner实例，下面是它的 run() 方法\noverride def run(): Unit = { threadId = Thread.currentThread.getId Thread.currentThread.setName(threadName) val threadMXBean = ManagementFactory.getThreadMXBean val taskMemoryManager = new TaskMemoryManager(env.memoryManager, taskId) val deserializeStartTime = System.currentTimeMillis() val deserializeStartCpuTime = if (threadMXBean.isCurrentThreadCpuTimeSupported) { threadMXBean.getCurrentThreadCpuTime } else 0L Thread.currentThread.setContextClassLoader(replClassLoader) val ser = env.closureSerializer.newInstance() logInfo(s\u0026#34;Running $taskName (TID $taskId)\u0026#34;) execBackend.statusUpdate(taskId, TaskState.RUNNING, EMPTY_BYTE_BUFFER) var taskStart: Long = 0 var taskStartCpu: Long = 0 startGCTime = computeTotalGcTime() try { // Must be set before updateDependencies() is called, in case fetching dependencies // requires access to properties contained within (e.g. for access control). Executor.taskDeserializationProps.set(taskDescription.properties) updateDependencies(taskDescription.addedFiles, taskDescription.addedJars) task = ser.deserialize[Task[Any]]( taskDescription.serializedTask, Thread.currentThread.getContextClassLoader) task.localProperties = taskDescription.properties task.setTaskMemoryManager(taskMemoryManager) ... // Run the actual task and measure its runtime. taskStart = System.currentTimeMillis() taskStartCpu = if (threadMXBean.isCurrentThreadCpuTimeSupported) { threadMXBean.getCurrentThreadCpuTime } else 0L var threwException = true val value = try { val res = task.run( taskAttemptId = taskId, attemptNumber = taskDescription.attemptNumber, metricsSystem = env.metricsSystem) threwException = false res } finally { val releasedLocks = env.blockManager.releaseAllLocksForTask(taskId) val freedMemory = taskMemoryManager.cleanUpAllAllocatedMemory() if (freedMemory \u0026gt; 0 \u0026amp;\u0026amp; !threwException) { val errMsg = s\u0026#34;Managed memory leak detected; size = $freedMemory bytes, TID = $taskId\u0026#34; if (conf.getBoolean(\u0026#34;spark.unsafe.exceptionOnMemoryLeak\u0026#34;, false)) { throw new SparkException(errMsg) } else { logWarning(errMsg) } } ... val resultSer = env.serializer.newInstance() val beforeSerialization = System.currentTimeMillis() val valueBytes = resultSer.serialize(value) val afterSerialization = System.currentTimeMillis() ... execBackend.statusUpdate(taskId, TaskState.FINISHED, serializedResult) ...看起来有点长，只贴出部分，在这里分步总结一下：\n将传进来的taskDescription对象反序列化为Task实例，这个实例是Task的子类，就是之前生成Task时的两种类型，ResultTask和SuffleMapTask，我们就用ResultTask为例 调用Task的 run() 方法，run() 又调用 runTask()，如下是ResultTask类中的 runTask() 方法 override def runTask(context: TaskContext): U = { // Deserialize the RDD and the func using the broadcast variables. val threadMXBean = ManagementFactory.getThreadMXBean val deserializeStartTime = System.currentTimeMillis() val deserializeStartCpuTime = if (threadMXBean.isCurrentThreadCpuTimeSupported) { threadMXBean.getCurrentThreadCpuTime } else 0L val ser = SparkEnv.get.closureSerializer.newInstance() val (rdd, func) = ser.deserialize[(RDD[T], (TaskContext, Iterator[T]) =\u0026gt; U)]( ByteBuffer.wrap(taskBinary.value), Thread.currentThread.getContextClassLoader) _executorDeserializeTime = System.currentTimeMillis() - deserializeStartTime _executorDeserializeCpuTime = if (threadMXBean.isCurrentThreadCpuTimeSupported) { threadMXBean.getCurrentThreadCpuTime - deserializeStartCpuTime } else 0L func(context, rdd.iterator(partition, context)) }方法内将该Task内包含的RDD与闭包都反序列化出来，然后执行这个闭包，闭包来处理RDD数据 3. Task的 run() 方法运行结束后就回到了TaskRunner中的 run() 方法，且返回了结果数据，这代表Task的主要任务已经完成了，所心要收集一些信息，包括：该Task的运行时长，CPU时长，GC时长等，这些数据应该会发给UI端，但我没有细找这一块 4. 然后将结果数据序列化，最后通过ExecutorBackend的 statusUpdate() 方法将结果传回到SparkDirver，ExecutorBackend是个物特质，在上一篇中提到过，CoarseGrainedExecutorBackend是其中的一个实现，它对 statusUpdate() 方法的实现如下\noverride def statusUpdate(taskId: Long, state: TaskState, data: ByteBuffer) { val msg = StatusUpdate(executorId, taskId, state, data) driver match { case Some(driverRef) =\u0026gt; driverRef.send(msg) case None =\u0026gt; logWarning(s\u0026#34;Drop $msg because has not yet connected to driver\u0026#34;) } }将data封装成StatusUpdate，将其作为消息通过RPC框架发送给Driver端 SparkDriver端的接收逻辑\noverride def receive: PartialFunction[Any, Unit] = { case StatusUpdate(executorId, taskId, state, data) =\u0026gt; scheduler.statusUpdate(taskId, state, data.value) if (TaskState.isFinished(state)) { executorDataMap.get(executorId) match { case Some(executorInfo) =\u0026gt; executorInfo.freeCores += scheduler.CPUS_PER_TASK makeOffers(executorId) case None =\u0026gt; // Ignoring the update since we don\u0026#39;t know about the executor. logWarning(s\u0026#34;Ignored task status update ($taskId state $state) \u0026#34; + s\u0026#34;from unknown executor with ID $executorId\u0026#34;) } } ...Driver收到StatusUpdate后，处理流程如下：\n先调用了调度器的 statusUpdate() 方法，调度器将数据返序列出来，将Task标记为完成状态 然后调度器又调用了DAGScheduler的post方法，将Task和数据封装为CompletionEvent放入DAGScheduler类的事件池eventProcessLoop中等待处理 这时回到 receive() 中，回收该Task的CPU资源 调用 makeOffers(executorId) 方法启动下一个Task任务，直到所有Task都执行完 因为DAGScheduler的事件池是被一直被循环取出来处理的，所以刚才完放进去的CompletionEvent很快会被DGAScheduler的 doOnReceive() 方法取出来做出相应的动作 private def doOnReceive(event: DAGSchedulerEvent): Unit = event match { ... case completion: CompletionEvent =\u0026gt; dagScheduler.handleTaskCompletion(completion) ...看handleTaskCompletion()的处理逻辑\nprivate[scheduler] def handleTaskCompletion(event: CompletionEvent) { val task = event.task val taskId = event.taskInfo.id val stageId = task.stageId val taskType = Utils.getFormattedClassName(task) ... // The stage may have already finished when we get this event -- eg. maybe it was a // speculative task. It is important that we send the TaskEnd event in any case, so listeners // are properly notified and can chose to handle it. For instance, some listeners are // doing their own accounting and if they don\u0026#39;t get the task end event they think // tasks are still running when they really aren\u0026#39;t. listenerBus.post(SparkListenerTaskEnd( stageId, task.stageAttemptId, taskType, event.reason, event.taskInfo, taskMetrics)) ... val stage = stageIdToStage(task.stageId) event.reason match { case Success =\u0026gt; task match { case rt: ResultTask[_, _] =\u0026gt; // Cast to ResultStage here because it\u0026#39;s part of the ResultTask // TODO Refactor this out to a function that accepts a ResultStage val resultStage = stage.asInstanceOf[ResultStage] resultStage.activeJob match { case Some(job) =\u0026gt; if (!job.finished(rt.outputId)) { updateAccumulators(event) job.finished(rt.outputId) = true job.numFinished += 1 // If the whole job has finished, remove it if (job.numFinished == job.numPartitions) { markStageAsFinished(resultStage) cleanupStateForJobAndIndependentStages(job) listenerBus.post( SparkListenerJobEnd(job.jobId, clock.getTimeMillis(), JobSucceeded)) } // taskSucceeded runs some user code that might throw an exception. Make sure // we are resilient against that. try { job.listener.taskSucceeded(rt.outputId, event.result) } catch { case e: Exception =\u0026gt; // TODO: Perhaps we want to mark the resultStage as failed? job.listener.jobFailed(new SparkDriverExecutionException(e)) } } ...这的逻辑实在有点长，就只贴出一部分来\nlistenerBus.post 是通知所有监听者此Task执行结束，listenerBus显然是一个集合，里面有各路监听者，当他们得知此消息后可以做出相应的动作 SparkListenerJobEnd(job.jobId, clock.getTimeMillis(), JobSucceeded)) 如果这个Task是ResultTask类型，则通知所有监听者此Job执行结束 job.listener.taskSucceeded(rt.outputId, event.result) 和 job.listener.jobFailed(new SparkDriverExecutionException(e)) 行是通知JobWaiter对象Task执行结果，然后JobWaiter就可以结束等待了 这时就回到了当初提交Job的地方 def runJob[T, U]( rdd: RDD[T], func: (TaskContext, Iterator[T]) =\u0026gt; U, partitions: Seq[Int], callSite: CallSite, resultHandler: (Int, U) =\u0026gt; Unit, properties: Properties): Unit = { val start = System.nanoTime val waiter = submitJob(rdd, func, partitions, callSite, resultHandler, properties) // Note: Do not call Await.ready(future) because that calls `scala.concurrent.blocking`, // which causes concurrent SQL executions to fail if a fork-join pool is used. Note that // due to idiosyncrasies in Scala, `awaitPermission` is not actually used anywhere so it\u0026#39;s // safe to pass in null here. For more detail, see SPARK-13747. val awaitPermission = null.asInstanceOf[scala.concurrent.CanAwait] waiter.completionFuture.ready(Duration.Inf)(awaitPermission) waiter.completionFuture.value.get match { case scala.util.Success(_) =\u0026gt; logInfo(\u0026#34;Job %d finished: %s, took %f s\u0026#34;.format (waiter.jobId, callSite.shortForm, (System.nanoTime - start) / 1e9)) case scala.util.Failure(exception) =\u0026gt; logInfo(\u0026#34;Job %d failed: %s, took %f s\u0026#34;.format (waiter.jobId, callSite.shortForm, (System.nanoTime - start) / 1e9)) // SPARK-8644: Include user stack trace in exceptions coming from DAGScheduler. val callerStackTrace = Thread.currentThread().getStackTrace.tail exception.setStackTrace(exception.getStackTrace ++ callerStackTrace) throw exception } }JobWaiter等待结束后，case scala.util.Success(_) 或 case scala.util.Failure(exception) 行会打印相关日志，此时提交Job的整个流程也就结束了。\n能力有限，文章可能看起来有些乱，博主还没有理清全部流程，后面会抽时间出来画张流程图，帮助理解。\n关于# 作者：张佳军\n阅读：12\n点赞：1\n创建：2017-05-06\n"},{"id":33,"href":"/bigdata/spark-source-code-5.html","title":"Spark - 源码分析（图）","section":"大数据","content":"Spark - 源码分析（图）# 最近抽时间画了张Spark的流程图，帮助理解Spark架构，供参考，点击图片可以放大。\n关于# 作者：张佳军\n阅读：27\n点赞：2\n创建：2017-05-17\n"},{"id":34,"href":"/bigdata/spark-thrift-mode.html","title":"Spark - Thrift Server模式","section":"大数据","content":"Spark - Thrift Server模式# Spark Thrift Server 其实是 Spark SQL 的一种运行方式，笔者前段时间用它来做测试了 Spark 的性能，以及 Spark SQL 对 SQL 语句的支持情况，用起来还是十分方便的，后来在网上一搜，除了官网很少有写 Spark Thrift Server 的文章，那么本文将让你了解并且会使用它。\nSpark Thrift Server 运行原理# 即然它被称为 Server，那它就是提供某种服务的，它提供的服务就是解析 SQL 语句，然后将解析完的 SQL 转换为 Spark 任务，并提交给 Spark 集群去执行（这里要取决于 Spark 的运行方式，如：Spark on Yarn、Spark Standalone 等），最后捕获运行结果。\n配置一个可用的 Spark Thrift Server# 前提准备# 我们为以三台机器的作为示例，分别为：host1、host2、host3 在三个节点上部署好 Hadoop2.7 集群，并启动 Hdfs、Yarn 在 host1 上部署好 Hive1.2.1，并且可以运行 在 host1 上安装 Spark2.0，先不用配置 修改 spark-env.sh# 要使用 Spark Thrift Server，需要用到三个配置文件，我们一个一个来配置 首先是 Spark 安装目录下的 conf/spark-env.sh\n[user@host1 spark]$ vi conf/spark-env.sh #!/usr/bin/env bash # 这里是Spark的配置文件所在的目录，在本例中为：$spark_home/conf export SPARK_CONF_DIR=/usr/lib/spark/conf # 为Spark Thrift Server分配的内存大小 export SPARK_DAEMON_MEMORY=1024m # Hadoop的安装目录 export HADOOP_HOME=/usr/lib/hadoop # Hadoop的配置文件目录 export HADOOP_CONF_DIR=/usr/lib/hadoop/etc/hadoop # 这里的java安装路经要换成自己的 export JAVA_HOME=/usr/lib/java修改spark-defaults.conf# 然后是Spark安装目录下的 conf/spark-defaults.conf\n[user@host1 spark]$ vi conf/spark-defaults.conf # 指定Spark的运行模式 spark.master yarn-client # 这里我们让Spark自己去动态调整资源 spark.dynamicAllocation.enabled true spark.dynamicAllocation.initialExecutors 2 spark.dynamicAllocation.maxExecutors 10 spark.dynamicAllocation.minExecutors 2 # 这几个可有可无了，， spark.history.fs.logDirectory hdfs:///spark-history/ spark.shuffle.service.enabled true spark.yarn.queue spark-thrift复制hive-site.xml# 最后将 Hive 的配置文件复制到 spark_home/conf 内\n[user@host1 spark]$ ll conf/ -rw-r--r-- 1 spark spark 899 2月 28 22:23 hive-site.xml # Hive的配置文件 -rw-r--r-- 1 root root 865 1月 11 05:14 slaves.template # 这个没有用到 -rw-r--r-- 1 spark spark 719 3月 19 05:54 spark-defaults.conf -rw-r--r-- 1 spark spark 1710 2月 28 22:23 spark-env.sh启动服务# sbin/start-thriftserver.sh --hiveconf hive.server2.thrift.port=10011查看启动是否成功# [user@host1 spark]$ ss -apln | grep 10011 LISTEN 0 50 :::10011 :::* users:((\u0026#34;java\u0026#34;,8600,389))使用 Spark Thrift Server# 在 Hive 里创建一个数据库# [user@host1 spark]$ hive WARN conf.HiveConf: HiveConf of name hive.optimize.mapjoin.mapreduce does not exist 17/03/19 07:20:11 WARN conf.HiveConf: HiveConf of name hive.heapsize does not exist 17/03/19 07:20:11 WARN conf.HiveConf: HiveConf of name hive.sentry.conf.url does not exist 17/03/19 07:20:11 WARN conf.HiveConf: HiveConf of name hive.auto.convert.sortmerge.join.noconditionaltask does not exist Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j.properties WARNING: Hive CLI is deprecated and migration to Beeline is recommended. hive\u0026gt; create thrift; # 创建数据库 OK hive\u0026gt; quit; #然后退出 [user@host1 spark]$ 进入 Spark Thrift 客户端# [root@spark ~]# bin/beeline Beeline version 1.2.1.spark2 by Apache Hive beeline\u0026gt; !connect jdbc:hive2://localhost:10011 Connecting to jdbc:hive2://localhost:10011 Enter username for jdbc:hive2://localhost:10011: # 直接回车 Enter password for jdbc:hive2://localhost:10011: # 直接回车 Connected to: Spark SQL (version 2.0.0) Driver: Hive JDBC (version 1.2.1.spark2) Transaction isolation: TRANSACTION_REPEATABLE_READ 0: jdbc:hive2://localhost:10011\u0026gt; show databases; # 查看所有数据库 +---------------+--+ | databaseName | +---------------+--+ | default | | thrift | +---------------+--+ 3 rows selected (0.082 seconds) 0: jdbc:hive2://localhost:10011\u0026gt; 到这里Spark Thrift配置完毕，进入客户端后就可以执行SQL语句了，不过更多的时候我们是用脚本的方式来执行SQL任务，这当然也可以了， 先把我们需要执行的SQL语句写到一个文件里，如：my_task.sql，然后执行以下命令：\nbin/beeline -u \u0026#34;jdbc:hive2://host1:10011/default;httpPath=cliservice\u0026#34; -f my_task.sql关于# 作者：张佳军\n阅读：1264\n点赞：7\n创建：2017-03-19\n"},{"id":35,"href":"/develop-tool/git.html","title":"GIT常用命令","section":"开发工具","content":"GIT常用命令# 克隆指定分支\ngit clone -b \u0026lt;branch_name\u0026gt; --depth 1 https://github.com/containerd/containerd.git克隆指定TAG\ngit clone -b \u0026lt;tag_name\u0026gt; --depth 1 https://github.com/containerd/containerd.git检出分支\ngit checkout tags/\u0026lt;tag_name\u0026gt;检出TAG并创建分支\ngit checkout tags/\u0026lt;tag_name\u0026gt; -b \u0026lt;branch_name\u0026gt;关于# 作者：sycki\n阅读：4\n点赞：0\n创建：2020-05-08\n"},{"id":36,"href":"/develop-tool/jq.html","title":"JQ命令","section":"开发工具","content":"JQ命令# 当key中包含点时或当key以数字开头时 docker inspect $container_id | jq -r \u0026lsquo;.[-1].NetworkSettings.Networks|.[\u0026ldquo;lenovo.com\u0026rdquo;].IPAddress\n根据元素的值选择元素 echo \u0026lsquo;{ \u0026ldquo;data\u0026rdquo;: [ { \u0026ldquo;joinTime\u0026rdquo;: 1507521910625, \u0026ldquo;memo\u0026rdquo;: \u0026ldquo;\u0026rdquo;, \u0026ldquo;projectId\u0026rdquo;: 48, \u0026ldquo;projectName\u0026rdquo;: \u0026ldquo;jiajun2\u0026rdquo; }, { \u0026ldquo;joinTime\u0026rdquo;: 1507794370272, \u0026ldquo;memo\u0026rdquo;: \u0026ldquo;\u0026rdquo;, \u0026ldquo;projectId\u0026rdquo;: 49, \u0026ldquo;projectName\u0026rdquo;: \u0026ldquo;jiajun22\u0026rdquo; } ], \u0026ldquo;msg\u0026rdquo;: \u0026ldquo;\u0026rdquo;, \u0026ldquo;result\u0026rdquo;: 0 }\u0026rsquo; | jq \u0026lsquo;.data[]|select(.projectName|startswith(\u0026ldquo;jiajun22\u0026rdquo;))|.projectId\u0026rsquo; 或 kubeapi /api/v1/namespaces/p48-u25-jiajun2/pods/|jq -r \u0026ndash;arg ip 172.210.130.86 \u0026lsquo;.items[]|select(.status.podIP==$ip)|.status\u0026rsquo;\n关于# 作者：sycki\n阅读：1\n点赞：0\n创建：2020-05-08\n"},{"id":37,"href":"/develop-tool/vscode.html","title":"vscode for go","section":"开发工具","content":"vscode for go# 常用快捷键# ctrl+space ：手动代码提示（Trigger Suggest） ctrl+k ctrl+0：折叠所有代码块（Fold ALL） ctrl+k ctrl+j：折叠所有代码块（Unfold ALL） alt+left =\u0026gt; ctrl+-：回退到上一步（go back） alt+right =\u0026gt; ctrl++：下一步（go forward） ctrl+f：在当前文件中查找（Find in file） ctrl+h：在当前文件中替换（Replace in file） ctrl+shift+f：在当前文件夹查找（Find in files） ctrl+shift+h：在当前文件夹替换（Replace in files） none：代码格式化（Format Document） gopls# 之前vscode for golang需要安装十几个命令行工具，在vscode 1.30.2版本（2018年）中，vscode加入了后台进程gopls来更好的支持代码提示、转到定义等功能。gopls for vscode更多用法参见 github\ngopls安装# 以 ubuntu on windows 为例，安装 vs code with gopls。\n下载最新版本golang，并配置好环境变量\ncd; mkdir -p program golang curl -Lo go.tar.gz https://dl.google.com/go/go1.14.2.linux-amd64.tar.gz tar -zxf go.tar.gz -C program/ echo export GOROOT=$PWD/program/go \u0026gt;\u0026gt; .bashrc echo export GOPATH=$PWD/golang \u0026gt;\u0026gt; .bashrc echo \u0026#39;PATH=$PATH:$GOROOT/bin:$GOPATH/bin\u0026#39; \u0026gt;\u0026gt; .bashrc . .bashrc 安装gopls\ngo get golang.org/x/tools/gopls@latest 启动vscode\ncode -n golang/myproject 常用配置# 打开配置文件 File \u0026gt; Preferences \u0026gt; Settings \u0026gt; User \u0026gt; Open JSON：\n// 将设置放入此文件中以覆盖默认设置 { \u0026#34;editor.minimap.enabled\u0026#34;: true, \u0026#34;editor.minimap.renderCharacters\u0026#34;: false, \u0026#34;window.zoomLevel\u0026#34;: 0, // go plugin Configuragion // \u0026#34;go.buildOnSave\u0026#34;: true, \u0026#34;go.toolsEnvVars\u0026#34;: {\u0026#34;GOOS\u0026#34; : \u0026#34;linux\u0026#34;}, \u0026#34;go.useCodeSnippetsOnFunctionSuggest\u0026#34;: false, // gopls Configuration \u0026#34;go.useLanguageServer\u0026#34;: true, \u0026#34;[go]\u0026#34;: { \u0026#34;editor.formatOnSave\u0026#34;: false, \u0026#34;editor.codeActionsOnSave\u0026#34;: { \u0026#34;source.organizeImports\u0026#34;: true, }, // Optional: Disable snippets, as they conflict with completion ranking. \u0026#34;editor.snippetSuggestions\u0026#34;: \u0026#34;none\u0026#34;, }, \u0026#34;[go.mod]\u0026#34;: { \u0026#34;editor.formatOnSave\u0026#34;: true, \u0026#34;editor.codeActionsOnSave\u0026#34;: { \u0026#34;source.organizeImports\u0026#34;: true, }, }, \u0026#34;gopls\u0026#34;: { // Add parameter placeholders when completing a function. \u0026#34;usePlaceholders\u0026#34;: true, // If true, enable additional analyses with staticcheck. // Warning: This will significantly increase memory usage. \u0026#34;staticcheck\u0026#34;: false, }, // code-runner Configuration \u0026#34;code-runner.runInTerminal\u0026#34;: true, \u0026#34;terminal.integrated.shell.linux\u0026#34;: \u0026#34;/bin/bash\u0026#34; }依赖私有仓库# 如果项目的go.mod中依赖了私有仓库，则vscode会因为go get私有仓库失败而导致加载项目失败，这时需要如下配置。 跳过对私有仓库的代理和校验：\ngo env -w GOPRIVATE=\u0026#34;*.name.com,gitlab.com/xyz\u0026#34;go get私有仓库时使用ssh方式而非https方式，在~/.gitconfig加入：\n[url \u0026#34;git@gitlab.com:\u0026#34;] insteadOf = https://gitlab.com/加载项目# 加载golang项目有两种情况：\ngo mod项目：直接打开项目即可。 go vendor项目：打开项目后，修改Workspace区配置并重启vscode： \u0026#34;go.toolsEnvVars\u0026#34;: {\u0026#34;GO111MODULE\u0026#34;: \u0026#34;off\u0026#34;} 问题记录# 当代码提示功能失效时，可能是mod中的包无法解析，包版本不能写v0.0.0，应该写latest，执行go mod tidy即可。\n当vscode自动编译项目时提示找不到某动态库，则可以在启动vscode时注入环境变量\nGO111MODULE=on CGO_LDFLAGS=\u0026#34;-L /Users/sycki/coding/src/qiaodata.com/sycki/slimming-service/lib/darwin-amd64\u0026#34; http_proxy=socks5://localhost:1080 https_proxy=socks5://localhost:1080 no_proxy=github.com,gopkg.in code src/qiaodata.com/sycki/slimming-service关于# 作者：sycki\n阅读：71\n点赞：0\n创建：2020-03-25\n"},{"id":38,"href":"/golang/find-index-in-cycle-array.html","title":"在循环数组中找出索引","section":"Golang","content":"在循环数组中找出索引# 需求# 给定一个循环有序的int数组和一个int值，返回该值在数组中的位置，如果数组中没有该值则返回-1。\n解法概述# 先将数组分成两个小数组，这两个小数组中必定有一个是有序的，而另一个是循环有序的。\n对于有序的小数组，我们可以通过首尾值判断要找的值是否在该数组中，如果是，则用二分法处理该有序数组；如果否，我们则对另一个环循有序小数组进行第一步的处理。\n实例# func binaryFind(arr []int, start int, end int, v int) int { center := (start+end)/2 if v \u0026lt; arr[center] { return binaryFind(arr, start, center-1, v) }else if arr[center] \u0026lt; v { return binaryFind(arr, center+1, end, v) }else{ if arr[center] == v { return center }else{ return -1 } } } func getIndex(arr []int, start int, end int, v int) int { center := (start+end)/2 if arr[start] \u0026lt; arr[center] { if arr[start] \u0026lt;= v \u0026amp;\u0026amp; v \u0026lt;= arr[center] { return binaryFind(arr, start, center, v) }else{ return getIndex(arr, center+1, end, v) } }else{ if arr[center] \u0026lt;= v \u0026amp;\u0026amp; v \u0026lt;= arr[end] { return binaryFind(arr, center, end, v) }else{ return getIndex(arr, start, center-1, v) } } } func main() { arr := []int{5,6,7,8,9,1,2,3,4} index := getIndex(arr, 0, len(arr)-1, 3) println(index) }关于# 作者：sycki\n阅读：7\n点赞：0\n创建：2018-07-20\n"},{"id":39,"href":"/golang/golang-get-package-to-dir.html","title":"下载包到指定目录","section":"Golang","content":"下载包到指定目录# 在一个go项目中，通常会引很多第三方包，如果想要你的源码在任何地方都能被编译，那么需要将这些包放在项目的vendor目录下，可惜的是go get命令不能将包下载到指定的目录中去，这就需要先下载到$GOPATH/src/目录下，再手动铐到vendor下，这能忍吗？\n这时有两种选，一种是使用govendor命令，但我不喜欢，它的运行本身会依赖一个配置文件，还需要我去记一些乱七八糟的命令，另一种思路是先创建一个空的临时的GOPATH目录，将需要的包用go get下载进入，然后将所有的包铐出来，可以写个简单的脚本来做。\n假设我现在要下载一个golang包github.com/coreos/etcd/client到当前项目的vendor目录下，执行以下命令：\nmkdir /tmp/go/{bin,pkg,src} GOPATH=/tmp/go go get -v github.com/coreos/etcd/client /bin/mv -rf /tmp/go/src/* vendor/ /bin/rm -rf /tmp/go关于# 作者：sycki\n阅读：161\n点赞：1\n创建：2018-07-20\n"},{"id":40,"href":"/golang/golang-grpc-demo.html","title":"gRPC使用指南","section":"Golang","content":"gRPC使用指南# gRPC是一个由google开源的RPC框架，它支持多种语言之间的相互调用，依赖开源项目google/protobuf作为序列化工具。\n相比于几年前的gRPC，新版gRPC的使用方式发生了一些改变，编译.proto文件时所用的命令已经与之前不同，而且产生的.go文件按照数据结构序列化和使用接口分为两个文件。\n准备环境# 安装Golang# 安装最新版Golang。\n安装Protocol Buffers编译器# protoc是一个二进制文件，用来编译.proto文件，输出指定语言的源码。进入protoc发布页面，按照自己的平台下载相应压缩包，比如64位linux就下载protoc-3.13.0-linux-x86_64.zip，解压后把二进制文件放到$GOPATH/bin下。\n$ protoc --version libprotoc 3.13.0安装Protocol Buffers GO插件# protoc-gen-go和protoc-gen-go-grpc两个二进制文件会被protoc调用，前者用于从.proto文件中编译出Go语言数据结构，这些数据结构带有序列化函数，后者用于产生Go语言编程接口。\ngo get google.golang.org/protobuf/cmd/protoc-gen-go \\ google.golang.org/grpc/cmd/protoc-gen-go-grpc添加Golang依赖# 执行前确保项目中包含了go.mod文件。\ngo get google.golang.org/grpc编写一个Demo# 新建一个名为grpc的Golang项目，结构如下：\ngrpc ├── client │ └── main.go ├── proto │ ├── grpc_grpc.pb.go │ ├── grpc.pb.go │ └── grpc.proto └── server └── main.go编写.proto文件# grpc/proto/grpc.proto\nssyntax = \u0026#34;proto3\u0026#34;; package proto; option go_package = \u0026#34;github.com/sycki/examples/grpc\u0026#34;; // 定义一个RPC Server // 包含两个函数可供Client调用 service Greeter { rpc AddUser (UserRequest) returns (UserResponse); rpc GetUser (UserRequest) returns (UserResponse); } // 调用时的数据格式 message UserRequest { string Name = 1; int32 age = 2; string address = 3; } // 返回值的数据格式 message UserResponse { string Name = 1; int32 age = 2; string address = 3; }生成Golang代码# protoc --go_out=. --go_opt=paths=source_relative \\ --go-grpc_out=. --go-grpc_opt=paths=source_relative \\ proto/grpc.proto执行成功会在proto目录下生成两个.go源文件，一个包含了数据结构，一个包含调用接口。\n编写RPC服务端# grpc/server/main.go\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net\u0026#34; pb \u0026#34;github.com/sycki/examples/grpc/proto\u0026#34; \u0026#34;google.golang.org/grpc\u0026#34; \u0026#34;google.golang.org/grpc/reflection\u0026#34; ) // 实现grpc/proto包中的GreeterServer接口 type server struct { pb.UnimplementedGreeterServer } func (s *server) AddUser(ctx context.Context, user *pb.UserRequest) (*pb.UserResponse, error) { log.Println(\u0026#34;add user:\u0026#34;, user.Name) return \u0026amp;pb.UserResponse{ Name: user.Name + \u0026#34; a year older\u0026#34;, Age: user.Age + 1, }, nil } func (s *server) GetUser(ctx context.Context, user *pb.UserRequest) (*pb.UserResponse, error) { log.Println(\u0026#34;get user:\u0026#34;, user.Name) return \u0026amp;pb.UserResponse{ Name: user.Name + \u0026#34; be get\u0026#34;, }, nil } func main() { // 监听一个端口 lis, err := net.Listen(\u0026#34;tcp\u0026#34;, \u0026#34;localhost:8081\u0026#34;) if err != nil { log.Fatal(err) } // 创建一个RPC服务端并启动 s := grpc.NewServer() pb.RegisterGreeterServer(s, \u0026amp;server{}) reflection.Register(s) if err := s.Serve(lis); err != nil { log.Fatal(\u0026#34;failed to serve: \u0026#34;, err.Error()) } }编写RPC客户端# grpc/client/main.go\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;log\u0026#34; pb \u0026#34;github.com/sycki/examples/grpc/proto\u0026#34; \u0026#34;google.golang.org/grpc\u0026#34; ) var address = \u0026#34;localhost:8081\u0026#34; func main() { // 连接到端口 conn, err := grpc.Dial(address, grpc.WithInsecure()) if err != nil { log.Fatal(\u0026#34;can not connect to server:\u0026#34;, address) } defer conn.Close() // 创建RPC客户端 client := pb.NewGreeterClient(conn) // 调用服务器中的AddUser方法，并得到返回值 result, err := client.AddUser(context.Background(), \u0026amp;pb.UserRequest{ Name: \u0026#34;jack\u0026#34;, Age: 18, }) if err != nil { log.Println(\u0026#34;failed AddUser\u0026#34;) return } log.Printf(\u0026#34;success AddUser =\u0026gt; Name:%v, Age:%v\\n\u0026#34;, result.Name, result.Age) }启动服务端# go run server/main.go启动客户端# go run clinet/main.go 2017/12/31 17:46:02 success AddUser =\u0026gt; Name:jack a year older, Age:19关于# 作者：张佳军\n阅读：105\n点赞：3\n创建：2017-12-31\n"},{"id":41,"href":"/golang/golang-profiler.html","title":"应用性能分析","section":"Golang","content":"应用性能分析# 本文介绍使用pprof工具分析go程序的方法，让你快速找到程序中的性能瓶颈。\n基本原理# 在go的安装包中提供了pprof性能分析工具，使用方法为go tool pprof \u0026lt;运行时数据\u0026gt;，然后它可以根据我们提供的数据分析出程序中各函数据CPU使用情况、内存使用情况、各协程运行情况，其中\u0026lt;运行时数据\u0026gt;的获取有两种方式，一种是让程序在执行时直接保存运行时数据，一种是通过程序的HTTP端口动态获取，这两种方式都需要入侵代码，也就是在目标程序中插入runtime/pprof或net/http/pprof两个包的代码来生成pprof工具需要的数据。下面以HTTP方式为例介绍pprof用法。\n准备数据# 只需在目标程序中以匿名方式引入net/http/pprof包，然后启动一个http服务即可，如下：\nimport _ \u0026#34;net/http/pprof\u0026#34; func main(){ go func() { logger.Error(http.ListenAndServe(\u0026#34;:8081\u0026#34;, nil)) }() ... }然后pprof就可以从8081端口上分析程序了。\n开始分析# 这时先在本地安装一个命令行工具，pprof在生成图片时会用到：brew install graphviz（mac命令）\n分析CPU性能：\ngo tool pprof http://localhost:8081/debug/pprof/profile命令执行后会阻塞30秒来收集数据，然后进入一个shell界面，等待输入命令：\nFetching profile over HTTP from http://localhost:8081/debug/pprof/profile Saved profile in /Users/sycki/pprof/pprof.samples.cpu.001.pb.gz Type: cpu Time: Oct 16, 2018 at 6:31pm (CST) Duration: 30.01s, Total samples = 240ms ( 0.8%) Entering interactive mode (type \u0026#34;help\u0026#34; for commands, \u0026#34;o\u0026#34; for options) (pprof) png这时我们输入png它就会生成一张png的图片，很清楚的画出占用CPU时间较多函数以及它们关系： 分析内存的方法一样：\ngo tool pprof http://localhost:3999/debug/pprof/heap协程执行情况：\ngo tool pprof http://localhost:3999/debug/pprof/goroutine除了上面三项还有其它数据可以分析，具体可以打开提供数据的端口查看：http://localhost:8081/debug/pprof/。\n小结# 可以看出pprof是个很实用的工具，但有一点，就是需要侵入代码，所以可以在编写程序的时候预先把pprof的逻辑加进去，把它做成可以动态打开和关闭的模块，这样我们可以在线上应用出问题时进行远程分析,以最快最直接的方式找出问题点并解决它。\n关于# 作者：sycki\n阅读：68\n点赞：2\n创建：2018-10-17\n"},{"id":42,"href":"/golang/golang-regexp-performance.html","title":"正则性能优化","section":"Golang","content":"正则性能优化# 最近在编码时用到了golang（版本为1.11.0）正则库中的ReplaceAllLiteral(data, nil)函数，发现golang的正则替换比java和php的正则替换慢很多，后来找到了方法，可以大幅度提升golang正则的性能，文章后面部分有具体的对比数据。\ngolang正则为什么比php慢# 我并不是第一个发现golang的正则库比php慢的，因为在golang的开源社区中有人已经提出了类似的问题，见issues，有人回复说：像php、python这样的语言的正则引擎，是用高度优化的C代码实现的，所以比较快。好吧，但java也比golang快至少可以说明golang的正则引擎还是有优化的空间。\n提升golang正则性能# 后来我现了一个叫rure-go的项目，详见 github.com，这个项目是一个用Rust语言实现的golang正则引擎，原理是先将Rust语言的正则库编译为一个.so动态库，然后用golang语言封装出来一套正则处理函数去调用.so动态库中的函数。\n具体步骤：\n先安装Rust：\ncurl https://sh.rustup.rs -sSf | sh 将Rust的正则库编译为动态库（Rust支持交叉编译）：\ngit clone git://github.com/rust-lang-nursery/regex cargo build --release --manifest-path ./regex/regex-capi/Cargo.toml这时在./regex/target/release/目录下会出现一个.so文件，把它放到你喜欢地方，比如在你的golang项目下新建一个./lib目录，然后就可以删掉Rust相关的东西了。\n在你的golang项目中引入rure-go项目，然后编写你的业务逻辑，注意这个包中没有ReplaceAll()这样的函数，所以要达到ReplaceAll(data, nil)的效果，需要先用FindAll()在data中找到所有匹配的位置，然后把所有不匹配的内容提取出来拼到一起。\n编译和运行时需要指定动态库的位置，因为github.com/BurntSushi/rure-go会依赖它，注意，当指定了动态库后就不能使用go的交叉编译了：\n如果你的电脑是MAC 编译 CGO_LDFLAGS=\u0026#34;-L$(pwd)/lib\u0026#34; go build ./your/project/main 启动 DYLD_LIBRARY_PATH=./lib ./main 如果你的电脑是Linux 编译 CGO_LDFLAGS=\u0026#34;-L$(pwd)/lib\u0026#34; go build ./your/project/main 启动 LD_LIBRARY_PATH=./lib ./main 性能对比# 对比php# 调用次数 php用时(秒) go优化前(秒) go优化后(秒) 100 3.119 未记录 1.864 1000 27.115 45.285 16.475 5000 135.88 未记录 82.42 对比java# 调用次数 java用时(秒) go优化前(秒) go优化后(秒) 1000 5.210 9.399 1.155 2000 10.652 18.844 2.286 10000 50.156 93.974 11.163 总结# golang标准库中的正则处理引擎性能比较差，如果遇到对性能要求较高的需求，可以用github.com/BurntSushi/rure-go包代替，它可以大幅提升正则处理性能。\n附一个rure-go库的使用示例：# import ( ... rure \u0026#34;github.com/BurntSushi/rure-go\u0026#34; ) func main() { var data []byte var buf = bytes.NewBuffer(make([]byte, 0, len(data))) var reg = rure.MustCompile(`(?is)\u0026lt;table[^\u0026gt;]*class=\u0026#34;box2\u0026#34;[^\u0026gt;]*\u0026gt;`) index := reg.FindAllBytes(data) data = deleteMatch(data, index, buf) // 删掉匹配到的内容 } func deleteMatch(data []byte, idx []int, buf *bytes.Buffer) []byte { if len(idx) \u0026lt; 1 { return data } buf.Reset() buf.Write(data[0:idx[0]]) for i := 1; i \u0026lt; len(idx); i += 2 { if i+1 \u0026gt;= len(idx) { start := idx[i] buf.Write(data[start:]) } else { start := idx[i] end := idx[i+1] buf.Write(data[start:end]) } } data = buf.Bytes() return data }关于# 作者：sycki\n阅读：647\n点赞：4\n创建：2019-01-06\n"},{"id":43,"href":"/golang/golang-tls-web.html","title":"web应用开发","section":"Golang","content":"web应用开发# 编写一个基于安全协议的WEB服务器，并将80端口重定向到443端口。\n创建一个服务端# package main func main() { // 指定证书文件并启动服务 s := \u0026amp;http.Server{Addr: \u0026#34;:443\u0026#34;, Handler: buildMux()} go s.ListenAndServeTLS(\u0026#34;/opt/ssl/cert.pem\u0026#34;, \u0026#34;/opt/ssl/privkey.pem\u0026#34;) // 再创建一个监听在80端口的Server，它负责把80上的所有请求重定向到443端口 s80 := \u0026amp;http.Server{Addr: \u0026#34;:80\u0026#34;, Handler: http.HandlerFunc(redirect80), ErrorLog: ctx.GetLogger()} go s80.ListenAndServe() // 创建一个channel，用来监听kill信号 stop := make(chan string, 1) defer close(stop) signal.Notify(sig, os.Interrupt, syscall.SIGTERM) \u0026lt;-stop s.Shutdown(context.Background()) s80.Shutdown(context.Background()) } func redirect80(w http.ResponseWriter, r *http.Request) { target := \u0026#34;https://\u0026#34; + r.Host + r.URL.Path if len(r.URL.RawQuery) \u0026gt; 0 { target += \u0026#34;?\u0026#34; + r.URL.RawQuery } http.Redirect(w, r, target, http.StatusTemporaryRedirect) }URI匹配规则# 下面代码是向ServerMux注册一个Handler：\nmux.HandleFunc(\u0026#34;/users\u0026#34;, user)理想情况是当访问/users/jack时执行user函数，而实事上你会得到一个404，它只能通过/users被访问，为了达到理想结果，正确的写法应该是下面这样：\nmux.HandleFunc(\u0026#34;/users/\u0026#34;, user)这样一来，当我们访问/users/jack时，服务器会先匹配/users/jack，如果没有该路径则会查找/users/，如果还没有匹配到则查找/，如果还没有就404。\n参数解析规则# func (m *Manager) myHandler(w http.ResponseWriter, r *http.Request) { println(r.FormValue(\u0026#34;name\u0026#34;)) // jack println(r.PostFormValue(\u0026#34;phone\u0026#34;)) // 空 resume, _ := ioutil.ReadAll(r.Body) println(string(resume)) // user=jack\u0026amp;age=18 } go中有2个函数用于获取参数，FormValue()用于解析url和body中的参数，PostFormValue()只解析body中的参数而不解析url中的参数 当客户端设置Content-Type的值为application/x-www-form-urlencoded时，body中的数据会被解码一次，然后当作键值对解析，最后保存在map结构中，当值为application/json或application/octet-stream等时，body中的原始数据不会被进行任何处理。 请求体只能被读取一次# 有时候我们会写这样的代码：\nfunc user(w http.ResponseWriter, r *http.Request) { // 先测试一下r.Body中有没有值 body := ioutil.ReadAll(r.Body) println(body) method := r.Method if method == POST { user := r.FormValue(\u0026#34;user\u0026#34;) println(user) } }然后发起请求：\ncurl -H \u0026#34;Content-Type: application/x-www-form-urlencoded\u0026#34; -d \u0026#39;user=jack\u0026#39; localhost/users/结果是在第一次获取请求体时没问题，但r.FormValue(\u0026quot;user\u0026quot;)返回的值为空，为什么？这是因为r.Body本身是个输入流，即然是流当然是有指针的，当第一次从流中读完数据后，流中的指针已经指向了流的最后，当执行r.FormValue(\u0026quot;user\u0026quot;)的时候，它会触发r.ParseForm()函数，进而再次从r.Body中读取数据并解析到map中供我们使用，但这时流中已经没有数据可以读了，自然也就得不到正确的结果。\n关于# 作者：张佳军\n阅读：34\n点赞：0\n创建：2017-12-27\n"},{"id":44,"href":"/golang/golang-web-and-curl.html","title":"解析客户端参数","section":"Golang","content":"解析客户端参数# 1.发送简单键值对# curl -H \u0026#34;Content-Type: application/x-www-form-urlencoded\u0026#34; \\ -d id=100 \\ -d name=golang \\ localhost/books这时curl会把数据拼接为id=100\u0026amp;name=golang形式放入请求体中，不对数据进行编码，意味着如果数据中含有\u0026amp; ? = @等符号时，服务端将无法解析出键值对。\n服务端解析# 服务端收到请求后发现内容类型为application/x-www-form-urlencoded，所以当r.ParseForm()函数被执行时，它会将r.Body中的数据解析为键值对形式，并保存在map结构中，以便使用r.FormValue()函数获取。\nfunc books(w http.ResponseWriter, r *http.Request) { id := r.FormValue(\u0026#34;id\u0026#34;) name := r.FormValue(\u0026#34;name\u0026#34;) }2.发送编码的键值对# curl -H \u0026#34;Content-Type: application/x-www-form-urlencoded\u0026#34; \\ localhost/books --data-urlencode \u0026#39;id=100\u0026#39; \\ --data-urlencode \u0026#34;bookContent=`\u0026lt;/tmp/book.txt`\u0026#34;这时curl同样会将所有键值对用\u0026amp;符号拼接起来，不同的是它会将所有值部分编码为%39%AC+%0B形式，也就是用三个字节代表一个字节，这样服务端就不会出现解析不了的问题，如果有多个键值对，必须用多个--data-urlencode标识。\n服务端解析# 服务端收到请求后发现内容类型为application/x-www-form-urlencoded，所以当r.ParseForm()函数被执行时，它会将r.Body中的数据解析为键值对形式，同时所有数据会被解码为原始数据（其实在上一种情况中，数据同样会被解码一次，但解码后的数据与解码前是一样的），然后保存在map结构中，以便使用r.FormValue()函数获取。\nfunc books(w http.ResponseWriter, r *http.Request) { id := r.FormValue(\u0026#34;id\u0026#34;) bookContent := r.FormValue(\u0026#34;bookContent\u0026#34;) }3.发送文件中的数据# curl -H \u0026#34;Content-Type: text/plain\u0026#34; \\ --data-binary @file.txt \\ \u0026#39;localhost/books?id=100\u0026amp;name=book1\u0026#39;这时我们并不希望我们的数据被服务端当做键对来解析，所以应该用其它字段值来替换application/x-www-form-urlencoded，比如我们设置为text/plain或application/json都是可以的，这里没有用-d是因为-d会默认去掉文件内容中所有的\\n和\\r，然后放入请求体中，显然我们并不希望数据被偷偷改掉，所以应该用--data-binary选项来代替-d。\n服务端解析# 服务端收到请求后发现内容类型为text/plain，这时你仍可以执行r.ParseForm()函数，但它只会解析query中的参数而不会解析r.Body中的数据。\nfunc books(w http.ResponseWriter, r *http.Request) { id := r.FormValue(\u0026#34;id\u0026#34;) name := r.FormValue(\u0026#34;name\u0026#34;) data, _ := ioutil.ReadAll(r.Body) }4.发送字符串# curl -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;k1\u0026#34;: \u0026#34;v1\u0026#34;}\u0026#39; \\ localhost/books这时数据被直接放入请求体，它与text/plain的行为是一样的。\n服务端解析# 处理方式与前一种相同\nfunc books(w http.ResponseWriter, r *http.Request) { data, _ := ioutil.ReadAll(r.Body) }5.发送文件# curl localhost/books \\ -F book1=@/tmp/book1.txt \\ -F book2=@/tmp/book2.txt这时每一个-F数据块的前后都会被加上一个含有随机串的标识符：--------------------------b6c4d1ab01136e11--，然后放入请求体中，且Content-Type会被设置为multipart/form-data; boundary=------------------------b6c4d1ab01136e11。\n服务端解析# 服务端发现内容类型为multipart/form-data，会读出紧跟在后面的标识符，然后将r.Body中的数据用该标识符切分成开，切分开的每个数据块中都含有一个name字段，这时我们就可以用该name获取相应的数据了。\nfunc books(w http.ResponseWriter, r *http.Request) { book1, book1Meta, err := r.FormFile(\u0026#34;book1\u0026#34;) book2, book2Meta, err := r.FormFile(\u0026#34;book2\u0026#34;) ... }总结# 当服务器端收到一个HTTP请求时，需要从原始的二进制数据中解析出客户端参数，这些参数大概可以分三个部分。\n请求方法与URL# 这两个部分都可以自定义，虽然HTTP协议标准定义了几种请求类型，如：POST、GET、DELETE、PATCH等，但大多数编程语言中并没有硬性规定，所以如果你喜欢的话，可以使用任何字符串来代替POST。URL后面的query参数会被服解析出来放在map结构中，golang的http库还会对URL进行安全检查，比如不能含有..。\n请求头# 这里的参数都是键值对形式，用户可以添加自定义的请求头，并且也没有限制。但有一个常用的请求头：Content-Type，它标识了请求体的数据格式，服务器端在收到请求后，会根据该字段来解析请求体中的数据，HTTP协议标准为这个字段预定义了几十种值，但我们常用的只有几种：application/x-www-form-urlencoded、multipart/form-data、application/json、text/plain等。\n请求体# 这里是最主要的存放数据的地方，服务端收到请求后，先判断请求头Content-Type的值：\n如果是application/x-www-form-urlencoded，则认为请求本中的数据为key=value形式，对数据进行解码然后保存在map中； 如果是multipart/form-data，则用符识符对数据进行切分，然后保存在map中； 如果是application/json或text/plain等类型，则不对请求体进行处理。 关于# 作者：sycki\n阅读：123\n点赞：0\n创建：2019-03-02\n"},{"id":45,"href":"/golang/unicode-to-zh.html","title":"unicode转中文","section":"Golang","content":"unicode转中文# // unicode2zh 将unicode转为中文，并去掉空格 func unicode2zh(uText string) (context string) { for i, char := range strings.Split(uText, `\\\\u`) { if i \u0026lt; 1 { context = char continue } length := len(char) if length \u0026gt; 3 { pre := char[:4] zh, err := strconv.ParseInt(pre, 16, 32) if err != nil { context += char continue } context += fmt.Sprintf(\u0026#34;%c\u0026#34;, zh) if length \u0026gt; 4 { context += char[4:] } } } context = strings.TrimSpace(context) return context }关于# 作者：sycki\n阅读：7\n点赞：0\n创建：2018-07-20\n"},{"id":46,"href":"/java/java-from-class-to-machine-code.html","title":"Java - 从CLASS到机器码","section":"Java","content":"Java - 从CLASS到机器码# 你真的了解Java代码的执行过程吗？我们都知道，Java代码是被编译成字节码后被JVM所执行，那JVM又是怎么执行的？不是说CPU只能认识机器码吗？那么从CLASS到机器码之间发生了什么？\n记得刚开始学Java的时候，老师告诉我们说：“Java是一种解释型语言，我们写的Java代码会被编译成字节码，然后由JVM解释执行”。这样说当然没错啊，只是还不够全面而已。当时在学习Java，同时我也在自学C语言（这么经典的编程语言怎么可以不学），想看看它们之们到底有什么不同之处，后来了解到C语言在被编译时，中间发生了很多事情，最后变成了二进制码，才能被CPU直接执行。可Java呢？老师明明说Java的源码是被编译成了字节码，CPU怎么可能直接认识字节码呢？上网搜了很多资料，都在说Class被JVM加载的过程，千篇一律，这个问题真的困惑我很久，后来有幸看到一篇文章（链接在本文尾部），才渐渐明白了困惑我很久问题。\n了解编译过程# 我们先简单回顾一下C语言的执行过程： 假设在Linux系统上用GCC编译一个C文件，下面结论只为帮助理解。\n编写源文件[a.c] -\u0026gt; 预编译 -\u0026gt; [a.i] -\u0026gt; 编译 -\u0026gt; [a.s] -\u0026gt; 汇编（可选） -\u0026gt; [a.o] -\u0026gt; 链接 -\u0026gt; 机器码 -\u0026gt; CPU\n再回顾一下Java的执行过程： 编写源文件[a.java] -\u0026gt; 编译 -\u0026gt; [a.class/a.jar] -\u0026gt; JVM -\u0026gt; ? -\u0026gt; CPU\n其中，在编译过程中，也有类似C语言中的链接的过程，比如自定a.java中引用了第三方jar包中的类，那么需要在编译时将这个第三方jar包与a.java打包在一起（类似C中的静态链接），否则需要在运行a.class时，通过指定运行时参数引用到这个第三方jar包（类似C中的动态链接） 关键在class被JVM加载到内存以后，JVM会把字节码翻译成机器码，然后被CPU执行，只不过这一步发生在内存当中，翻译过的机器码也不会被保存在磁盘上（取决于JVM的实现），所以很少有人注意，为会什么JVM的默认实现不把翻译过的机器码保存在磁盘上呢？很简单，虽然是同一个class文件，但放在windowns下翻译出来的机器码跟Linux下翻译出来的机器码并不相同，这样就失去了Java程序的一大特性：可移植性。\nJIT优化# 说到这里，就得提一提JIT这个东西了，JIT是just in time的缩写, 也就是即时编译编译器，是JVM的一部分，它是的作用是什么呢？上面说过，class字节码是在被CPU执行前翻译成机器码的，那如果我们写了一段循环执行的代码，就这样一句一句翻译再执行必然会很慢，JIT的作用就是把那些需要频繁执行的代码一次性编译成机器码保存在内存当中，这样就避免了大量重复的翻译工作，也就加快了运行速度，而JIT编译代码本身也需要时间，所以那些只需要运行一次的代码则不经过JIT，由解释器解释执行。\n那什么样的代码才算是“频繁执行的代码”？这里JIT有自己的判断标准，主要由两个计数器决定，一个是方法被调用的次数，另一个是方法中循环被回弹执行的次数，当次数到达一定界限时，该段代码才会被编译，这个阈值可以通过 -XX:CompileThreshold=N 这个选项指定，默认值为10000 最后再总结一下Java代码被执行的过程： 编写Java源码（示例）：\nint a = 1 int b = 2 int c = a + b然后经过javac命令编译后变成了class文件，如果此时将其反汇编，将看到如下：\n0 iload_1 1 iload_2 2 iadd 3 istore_3当这个class被执行时，它在内存中会被一句一句翻成机器码，可能像下面这样：\n0000,0000,000000010000 0000,0001,000000000001 0001,0001,000000010000 0001,0001,000000000001这时才能被CPU直接执行！\n参考资料# C语言的编译链接过程详解 深入浅出 JIT 编译器 关于 作者：张佳军\n阅读：149\n点赞：0\n创建：2017-04-03\n"},{"id":47,"href":"/java/java-nio-frame.html","title":"Java - NIO框架","section":"Java","content":"Java - NIO框架# 什么是 NIO# 为了弥补 Java IO 的不足，Java 从 1.4 版本开始引入了 NIO 框架，也就是说 NIO 是一套新的 IO 框架，一般解释为 Non-blocking IO，有时也解释为 New IO。它可以在实现高并发服务器的同时占用很少的资源。\n传统 IO 框架# 对于传统的 IO 框架，NIO 的优势主要体现在对请求的处理上，下面我们先来看看用传统 IO 怎样建立一个 TCP 连接。 首先创建服务端，并等待新的连接，下面是伪代码：\nServerSocket ser = new ServerSocket(8081); while (true) { Socket sk = ser.accept(); startNewThread(sk); } 第一行是创建一个服务端对象，并准备监听本机的 8081 端口。 Socket sk = ser.accept(); 表示开始监听新的连接请求，这行代码是阻塞式的，直到有新的客户端连接过来，会返回一个新 Socket 连接对象。 startNewThread() 方法是处理 Socket 连接的主要逻辑，它会创建一个新的线程并立即返回，该线程的工作是一直监听客户端发来的数据，然后作出回应。 这时又会回到循环的项部，继续等待新的连接，这就是一个服务端了。 缺点：乍一看没什么问题，一切都很美好不是吗，但是当连接的请求多了呢，比如几万或者几百万？这样的话就会有几百万个线程同时运行，因为每一个Socket都需要一个线程来监听数据，即使大部分 Socket 都是空闲状态，只有一小部分正在繁忙的交换数据，这一百万个线程如果每个只占用 1M 的内存，那也需要 1T 的内存！这显然不太明智。\nNIO 框架# 在上例中共有两种监听事件，一种是主线程不断地监听新的连接，另一种是其它子线程监听它所负责的Socket上的数据，那能不能把这些需要监听对象放在一起呢？然后用一个线程或少量几个线程专门负责监听所有对象上的请求？？这样就能节省大量资源啊！实际上这也是完全合理的，并且在新的 IO 框架中就是这么做的！那么我们来看看NIO的实现方式吧。\n创建 Selector 对象# 根据以上思路，我们首先需要一个池用来存放这些需要监的对象，它就是 NIO 框架中的 Selector 类，那怎么创建呢？如下：\nSelector sel = Selector.open();创建 ServerSocketChannel 对象# 然后我们需要一个类似旧 IO 框架中的 ServerSocket，用来监听某端口上的连接请求，在 NIO 中它叫做 ServerSocketChannel：\nServerSocketChannel ssc = ServerSocketChannel.open(); ssc.socket().bind(new InetSocketAddress(\u0026#34;localhost\u0026#34;, 8081)); ssc.configureBlocking(false);创建好之后需要设置一些相应的参数，无非就是 IP 端口之类，最后一行是将它设置为非阻塞，还记得传统 IO 的实现方式吗？它有一步是监听新的连接，也就是 ser.accept() 这一句，它是阻塞式的，当设置了 ssc.configureBlocking(false) 之后，accept() 方法就不再阻塞，而是立即返回，这样主线程就可以腾出时间做更多的事情，而不是一味地干等着。\n注册新连接事件# 然后把这个 ServerSocketChannel 对象放入 Selector 中，在 NIO 中，这个动作被称为注册，像下面这样：\nssc.register(sel, SelectionKey.OP_ACCEPT);这样 ssc 就被当作事件注册到 Selector 中，register() 的第二个参数是为了标记这个事件的类型，也就是说 Selector 中可以多种类型的事件。\n处理事件# 现在，事件池中已经有一个事件了，这时我们需要写一个循环，不断地从池中取出已发生的事件并处理掉，取出来的事件是一个 Set 集合，然后遍历里面的事件：\nsel.select(); Set\u0026lt;SelectionKey\u0026gt; selectionKeys = sel.selectedKeys(); Iterator\u0026lt;SelectionKey\u0026gt; it = selectionKeys.iterator(); while(it.hasNext()){ SelectionKey key = it.next(); ...注册接收数据事件# 这个 key 要怎么处理呢？它不是就是我们放进去的 ServerSocketChannel 吗，我们调用它的 accept() 方法就可以了，它返回一个 Socket 对象，然后将这个新的对象也放入池中，让 Selector 统一管理与监听，放进去之前也要将它设置为非阻塞：\nServerSocketChannel c = (ServerSocketChannel)key.channel(); SocketChannel sc = c.accept(); sc.configureBlocking(false); //将新的连接加入监听器中 sc.register(sel, SelectionKey.OP_READ);注意这时 register() 方法的第二个参数不一样了，它被标记为 OP_READ 类型的事件。\n完整示例# 这时，池中已经有两种事件了，所以我们在取出事件后，要对其作区分，不同事件做不同的处理，下面是服务端的完整逻辑：\npublic static void main(String[] args) throws Exception { //创建一个事件池 Selector sel = Selector.open(); //创建一个服务端并加入事件池 ServerSocketChannel ssc = ServerSocketChannel.open(); ssc.socket().bind(new InetSocketAddress(\u0026#34;localhost\u0026#34;, 8888)); ssc.configureBlocking(false); ssc.register(sel, SelectionKey.OP_ACCEPT); //不断监听并处理池中的事件 while(true){ //不断监听并取出池中的事件 int num = sel.select(); System.out.printf(\u0026#34;read [%s] event from selector!\\n\u0026#34;, num); Set\u0026lt;SelectionKey\u0026gt; selectionKeys = sel.selectedKeys(); Iterator\u0026lt;SelectionKey\u0026gt; it = selectionKeys.iterator(); //处理取出的事件 while(it.hasNext()){ SelectionKey key = it.next(); if(key.readyOps() == SelectionKey.OP_ACCEPT){ System.out.println(\u0026#34;检测到新的连接\u0026#34;); ServerSocketChannel c = (ServerSocketChannel)key.channel(); SocketChannel sc = c.accept(); sc.configureBlocking(false); //将新的连接加入监听器中 sc.register(sel, SelectionKey.OP_READ); } else if (key.readyOps() == SelectionKey.OP_READ){ System.out.println(\u0026#34;检测到新的请求\u0026#34;); SocketChannel sc = (SocketChannel)key.channel(); try{ handelRequest(sc); }catch (IOException e) { System.out.println(\u0026#34;有连接被断开\u0026#34;); sc.close(); key.cancel(); } } } selectionKeys.clear(); Thread.sleep(1000); }数据处理# handelRequest() 方法是需要自己实现的，它主要是从 SocketChannel 中接收数据然后作出相应的动作，如果接收时抛出异常，我们就认为这个连接已经断开，并取消对其的监听，也就是 key.cancel();，为了让程序跑的慢一些以便看清楚它的流程，所以加入了 Thread.sleep(1000);。\nhandelRequest() 方法实现如下：\nstatic ByteBuffer buff = ByteBuffer.allocate(128); static void handelRequest(SocketChannel sc) throws IOException{ buff.clear(); //读取客户端发来的数据 if(sc.read(buff) \u0026lt; 1) return; //处理接收到的数据 buff.flip(); System.out.println(new String(buff.array())); //向客户端作出回应 sc.write(ByteBuffer.wrap(\u0026#34;I am Server!\u0026#34;.getBytes())); }第一行创建是 NIO 框架专用的 ByteBuffer。\nif(sc.read(buff) \u0026lt; 1) 是将客户端发来的数据读到 ByteBuffer 中，如果客户端没有发数据过来，此方法会返回 -1。\nNIO中各对象的关系# 到这里整个流程差不多就结束了，但有些概念还是需要捋一捋，在 NIO 框架中，所有对象可以分为三种，Channel、ByteBuffer 和 Selector，其中 Channel 的概念上面没有提到，其实它就是一个抽象，因为我们要把所有需监听的对象放到 Selector 这个池中，所以当然需要将放进去的东西统一抽象出来，也就是说在 NIO 中，所有连接都被看作是 Channel，而从一个 Channel 中读出数据或者是写入数据都要先经过 ByteBuffer，ByteBuffer 起到缓冲的作用，在 NIO 中所有读写操都是非阻塞的，而如果客户端发一条很长的数据过来，由于网卡和操作系统的限制，必然后被切分成多条子消息发送到服务端，为了避免数据的不完整性，缓冲机制就成了必须品。\n参考# IBM有一篇NIO的文章，写的相当详细适合刚接触NIO的同学： NIO 入门 在实际项目中，一般不会自主实现一套产品级的高可用 NIO 框架，现在比较成熟的 NIO 开源框架是 Netty，在很多项目中被使用，我之前写过一篇关于 Netty 的文章： Netty 使用教程 关于# 作者：张佳军\n阅读：42\n点赞：2\n创建：2017-07-08\n"},{"id":48,"href":"/java/netty-guide.html","title":"Java - Netty入门","section":"Java","content":"Java - Netty入门# 什么是Netty# Netty 是一个高性能高可用的 NIO 框架，并且使用简单，用它可以轻松地开发诸如协议服务器和客户端之类的网络应用程序。它大大简化了网络编程，如 TCP 和 UDP 套接字服务器开发。已经被其它很多项目使用，比如 Spark。\n为什么不用 Java 标准库中的 NIO 框架# 它实际上是 Java 标准库中的 NIO 框架的一个实现，并将它做成了一个通用框架，最重要的是它经过了很多项目的验证与打磨，现在已经非常稳定，如果项目组要从头开始实现一个这样好用的 NIO 框架成本是很高的，而且还需要长时间的打磨。由于 Netty 是开源项目，社区又很活跃，因此很多项目从一开始就选择 Netty 作为自己的 NIO 框架，以降低开发成本。\n开始动手# 我们 Netty 框架来编写一个简易应用程序，并说明它的执行流程，这个程序分为服务端和客户端两个部分，每个部分有两个类。\n编写服务端# 先来写服务端的主程序 TestServer 类，其实不是一个类，因为它只包含一个 main 方法而已：\npackage com.kxdmmr.demo.netty; import io.netty.bootstrap.ServerBootstrap; ... public class TestServer { static final int PORT = Integer.parseInt(System.getProperty(\u0026#34;port\u0026#34;, \u0026#34;8081\u0026#34;)); static final int SIZE = Integer.parseInt(System.getProperty(\u0026#34;size\u0026#34;, \u0026#34;128\u0026#34;)); public static void main(String[] args) throws Exception { NioEventLoopGroup bossGroup = new NioEventLoopGroup(); NioEventLoopGroup workerGroup = new NioEventLoopGroup(); try{ // 1.创建服务端 ServerBootstrap b = new ServerBootstrap(); // 2.配置服务端 b.group(bossGroup, workerGroup) .channel(NioServerSocketChannel.class) .childHandler(new ChannelInitializer\u0026lt;SocketChannel\u0026gt;() { @Override public void initChannel(SocketChannel ch) throws Exception { // 注入自定义的处理程序 ch.pipeline().addLast(new TestServerHandler()); } }) .option(ChannelOption.SO_BACKLOG, SIZE) .childOption(ChannelOption.SO_KEEPALIVE, true); // 3.启动服务端 ChannelFuture f = b.bind(PORT).sync(); f.channel().closeFuture().sync(); } finally { // 停止所有线程池中的线程 workerGroup.shutdownGracefully(); bossGroup.shutdownGracefully(); } } } 创建服务端的整个过程可以分为三个步骤：1.创建服务端，2.配置服务端，3.启动服务端。\nbossGroup 与 workerGroup 是两个线程池，bossGroup 用来存放服务器端监听连接的事件，也就是监听着某一端口的线程，一旦有新的客户端连进来，它将被注册为等待接收数据的事件，并放入 workerGroup 中。\nServerBootstrap 类用来创建一个服务端程序，它简化了创建非阻塞式服务端程序的过程。\nNioServerSocketChannel 是 Netty 中定义的 Channel，底层封装了 Java 标准库中的 ServerSocketChannel。\nChannelInitializer 类的 initChannel 方法用来为程序配置 Channel，在这里可以将我们自定义的处理 Channel 的方法，也就是 TestServerHandler 这个类，它是来用处理其它客户端发来的数据。\noption() 和 childOption() 可以做一些其它配置，最后 b.bind(PORT).sync() 就是启动服务端了。\n上面用到了 TestServerHandler 类，其定义如下：\npackage com.kxdmmr.demo.netty; import io.netty.buffer.ByteBuf; import io.netty.channel.ChannelHandlerContext; import io.netty.channel.ChannelInboundHandlerAdapter; public class TestServerHandler extends ChannelInboundHandlerAdapter { @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws InterruptedException { ByteBuf in = (ByteBuf) msg; while (in.isReadable()) System.out.print((char) in.readByte()); System.out.println(); System.out.flush(); in.writeBytes(\u0026#34;I am server!\u0026#34;.getBytes()); Thread.sleep(1000); ctx.write(msg); } @Override public void channelReadComplete(ChannelHandlerContext ctx) { ctx.flush(); } @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) { cause.printStackTrace(); ctx.close(); } }当一个己连接到服务器的 Client 向服务端发数消息时，上面 channelRead() 会被调起，它的作用是处理从 Client 接收到的数据，这个就需要自己根据业务来定义了，在本例中我只是将接收到的消息打印出来，然后调用 in.writeBytes() 向 Client 发送一条消息作为回应，为了看清楚它的运行过程，所以加了 Thread.sleep(1000) 这一句。\n当 channelRead() 方法结束后意味着数据处理完毕，这时 channelReadComplete() 方法会被调起，用户可以根据需要在这里加一些其它逻辑。\n如果服务器端运行出现异常，比如连接意外中断，那么 exceptionCaught() 会被调用。\n编写客户端# 客户端的创建步骤与服务端是一样的，不同的是这里用的是 Bootstrap 类，而且只需要一个线程池：\npackage com.kxdmmr.demo.netty; import io.netty.bootstrap.Bootstrap; ... public class TestClient { static final String HOST = System.getProperty(\u0026#34;host\u0026#34;, \u0026#34;127.0.0.1\u0026#34;); static final int PORT = Integer.parseInt(System.getProperty(\u0026#34;port\u0026#34;, \u0026#34;8081\u0026#34;)); static final int SIZE = Integer.parseInt(System.getProperty(\u0026#34;size\u0026#34;, \u0026#34;128\u0026#34;)); public static void main(String[] args) throws Exception { EventLoopGroup group = new NioEventLoopGroup(); try { // 创建客户端 Bootstrap b = new Bootstrap(); // 配置客户端 b.group(group).channel(NioSocketChannel.class).option(ChannelOption.TCP_NODELAY, true) .handler(new ChannelInitializer\u0026lt;SocketChannel\u0026gt;() { @Override public void initChannel(SocketChannel ch) throws Exception { ChannelPipeline p = ch.pipeline(); // p.addLast(new LoggingHandler(LogLevel.INFO)); p.addLast(new TestClientHandler()); } }); // 启动客户端 ChannelFuture f = b.connect(HOST, PORT).sync(); // 在连接关闭之前一直等待 f.channel().closeFuture().sync(); } finally { // 停掉所有线程 group.shutdownGracefully(); } } }客户端同样用到了一个自定义的类：TestClientHandler，用来处理服务端发来的消息，其定义如下：\npackage com.kxdmmr.demo.netty; import io.netty.buffer.ByteBuf; import io.netty.buffer.Unpooled; import io.netty.channel.ChannelHandlerContext; import io.netty.channel.ChannelInboundHandlerAdapter; public class TestClientHandler extends ChannelInboundHandlerAdapter { private final ByteBuf firstMessage; /** * 创建一个客户端的处理器并初始化数据 */ public TestClientHandler() { firstMessage = Unpooled.copiedBuffer(\u0026#34;I am client!\u0026#34;.getBytes()); } @Override public void channelActive(ChannelHandlerContext ctx) { ctx.writeAndFlush(firstMessage); } @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws InterruptedException { ByteBuf in = (ByteBuf) msg; while (in.isReadable()) System.out.print((char) in.readByte()); System.out.println(); System.out.flush(); in.writeBytes(\u0026#34;I am client!\u0026#34;.getBytes()); Thread.sleep(1000); ctx.write(msg); } @Override public void channelReadComplete(ChannelHandlerContext ctx) { ctx.flush(); } @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) { cause.printStackTrace(); ctx.close(); } }它与服务端唯一不同的是，比服务端多了一个方法 channelActive()，其它三个方法与服务端作用是一样的，channelActive() 是用来在启动客户端时首次向服务端发送消息，也就是：\u0026ldquo;I am client!\u0026quot;。\n-End-\n关于# 作者：张佳军\n阅读：58\n点赞：2\n创建：2017-07-02\n"},{"id":49,"href":"/kubernetes/docker-auto-test-product.html","title":"产品自动化测试","section":"容器开发","content":"产品自动化测试# 目前公司正在开发大数据平台，在开发过程中，每当发布了新版本时，开发组通知测试组，然后测试组从 Hudson （一个持续集成工具）下载所有的 rpm 包，在测试机上进行安装部署，然后跑测试用例，出测试结果，恢复系统环境以供再次安装测试。这个过程中显然有很多重复性工作，因此，为了缩短产品迭代周期，节省劳动力，我们需要一套自动化测试方案。 本文就笔者目前条件和环境探讨一个可行的方案，不作为具体实现，触类旁通，具体实现还请参考实际情况。\n基本思路# 首先，我们需要一个 Linux 集群，这里我们就用 Docker ，这样我们可以快速的创建一个 Linux 集群出来，并且很好的隔离系统环境，不用每次安装完再恢复系统环境。 其次是大数据集群的部署，其中每个组件都有那么多的配置项，怎么办？？笔者目前所开发的大数据平台是用 Ambari 来作为集群管理工具，Ambari 是一个主从架构的分布式管理工具，主要有 Seriver 和 Agent 两个服务组成，它的功能包括有集群的创建 、组件的安装与卸载、监控集群状态等等，它还提供了一个功能叫 Ambari Blueprint ，这个功能可以帮我们免去一切人工操作与各组件的配置，并让我们可以更容易地实现将大数据集群的部署与配置脚本化。在这里我们就用它来创建集群。 关于更多 Ambari Blueprint 介绍和使用方法请看本文尾部提供的链接。 实现步骤# 即然要部署大数据集群，那么安装包从何而来？？我们的大数据平台是用 Hudson 来实现持续集成的，而实现自动化测试的环境是另一台服务器，所以需要远程检测 Hudson 上发布的新版本并下载，这里可以用爬虫方式来检测指定页面上的内容，如果发新有的版本就下载即可。也可以在 Hudson 服务器上写一段小程序，做为一个触发点，每当有版本编译出来就远程通知给测试机，让测试机开始下载。 测试机一旦下载成功，便开始创建 Docker 容器，这里我们把容器当作虚拟机来用，所以打算搭建什么规模的集群就创建几个容器，本次我们打算搭建5个节点的集群，所以创建5个容器出来，这里需要注意的是，用来创建容器的镜像最好提前配置好，比如说大数据平台依赖的一些系统配置等等。 部署 Ambari，我们刚才创建了一共5个容器，在第一个容器内安装 Ambari-Server 与 Ambari-Agent，其它四个容器只安装 Ambari-Agent 就可以了 用 Ambari Blueprint 安装各个组件（如：Haoop、Spark、Hive 等），这里会用到两个 JSON 文件，这两个文件是重中之重，这两个文件作用如下： 上图中左边是两个 JSON 文件，它们通 http 请求发送给 Ambari-Server 端，然后 Ambari-Server 会根据 JSON 信息创建出一个集群来。 那么 JSON 从哪来呢，最简单的方法是从一个已安装好的 Ambari 集群导出，像下面这条命令： curl -H \u0026#34;X-Requested-By: ambari\u0026#34; -u admin:admin -X GET \\ http://$ambari_server_ip:8080/api/v1/clusters/$cluster_name?format=blueprint \u0026gt; blueprint_document.json这样相当于把当前的集群状态记录下来，而第二个 JSON 文件需要自己定义。下面是两个 JSON 文件中部分示例： 下面开始创建集群，将上面第一个 JSON 文件发送给 Ambari—Server 并注册一个 Blueprint：\ncurl -H \u0026#34;X-Requested-By: ambari\u0026#34; -i -u admin:admin \\ -X POST -d @blueprint_document.json \\ http://$ambari_server_ip:8080/api/v1/blueprints/blue_cluster?validate_topology=false为 Ambari 添加一个自己的私有 yum 源，让 Ambari 找到我们的安装包：\ncurl -u admin:admin \\ -H \u0026#34;X-Requested-By: ambari\u0026#34; -i \\ -X PUT -d @my_repo.json \\ http://$ambari_server_ip:8080/api/v1/stacks/LEAP/versions/1.0/operating_systems/redhat6/repositories/$repo_name用注册的 Blueprint 创建集群：\ncurl -u admin:admin \\ -H \u0026#34;X-Requested-By: ambari\u0026#34; -i \\ -X POST -d @create_template.json \\ http://$ambari_server_ip:8080/api/v1/clusters/$cluster_name这样 Ambari 就开始创建集群了，进入 Ambari WEB UI 可以看到进度 5. 安装好了以后就可以运行测试用例并记录结果，结果可以是自定义格式或记录 log 的方式 6. 发布测试结果给管理人员，这里最好是以发邮件的方式，比效方便而且还可以把结果也一并发出，在 Linux 下发邮件也是有很多学问的，大概可以分为两种方式，一种是把 Linux 本身当做邮件服务器，由它来直接发送给其它发件人，但这种方式有个弊端，假设我要给 abc@163.com 发一份邮件，那么邮件到达 163 服务时，它可能把我们的邮件当成垃圾邮件，导致邮件被拒收。还有一种方式是通过第三方服务器发送，假设我要给 abc@163.com 发一份邮件，我可以先把邮件交给 qq 邮件服务器，qq 服务器再转发给 163 服务器，这样就不会被拒收了，但这种方式需要先登录 qq 邮件服务器认证才行 7. 到这里就接近尾声了，脚本的实现千差万别，这里就不赘述。最后希望大家能通过本文获得一星半点的灵感，那就再好不过了。\n参考资料# Ambari Blueprint 介绍与使用 在 Linux 下发送邮件 关于# 作者：张佳军\n阅读：16\n点赞：0\n创建：2017-02-18\n"},{"id":50,"href":"/kubernetes/docker-bigdata-develope.html","title":"与大数据平台开发","section":"容器开发","content":"与大数据平台开发# 发现问题# 场景一 在大数据平台的开发过程中，开发人员通常需要自己有一套集群，以便反复测试自己负责的模块，难道要给每人都配几台机器？ 场景二 测试组需要反复的安装整个平台以便发现问题，而一旦安装就很难再让 Linux 系统恢复到一个干净的状态，或者说需要花费很多时间，那如何快速地恢复系统环境？ 场景三 测试人员在测试中发现了一个 Bug，需要保存现场，可测试还要继续，怎么办？ 场景四 如何把一个部署好的大数据平台快速地迁移到其它地方？\n传统解决方案# 解决这个问题第一个想到的当然是用虚拟机了，而之前也确实用的是虚拟机，但这种方式不能完美的解决以上问题，比如：\n虽然它也可以迁移，但这并不是它所擅长的，不够灵活，很笨重 虚拟机的快照可以保存当前的状态，但要恢复回去就得把当前正在运行的虚拟机关闭，并不适合频繁保存当前状态 虽然可以给每个人都分配几个虚拟机用，但它是一个完整的系统，本身需要较多的资源，底层物理机的资源很快就被用完了，我们需要寻找其它方式来弥补这些不足 Docker 技术的引入# Docker 项目的目标是实现轻量级的操作系统虚拟化解决方案，换句话说，它可以让我们把一台物理机虚拟成多台来使用，而且它还可以保修改、完整的迁移到其它地方、性能损耗小等等好处，可以说很好解决了我们的问题。 那为什么不用虚拟机？ 因为它比虚拟机更轻便，Docker容器中不包含操作系统，启动一个Docker容器只要几秒种的时间，在一台物理机上可以创建几百上千个容器，而虚拟做不到。 下面是 Docker 与虚拟机的实现原理图\nDocker 设计图# VM 设计图# 可见容器是在操作系统层面上实现虚拟化，直接复用本地主机的操作系统，而传统方式则是在硬件层面实现。\n环境的搭建# 在实践过程中，部署一套可用的 Docker 环境，必需做好以下前提工作：\n搭建私有镜像仓库，用来统一存放构建好的镜像 搭建一个安装包仓库，用来存放我们发布的各种版本的大数据安装包等 使多个物理机上的 Dcoker 容器可以相互通信，官方已存给出了方案 为平台定制基础镜像# 即然要在Docke容器内安装我们的平台，那就需要一个统一的 Linux 系统做为我们的 Dcoker 容器，比如 Ubuntu、CentOS 等发行商都会发布自己的Docker基础镜像到 Docker Hub 上，如果 Docker Hub 上恰好没有你需要的镜像，也可以自己制作。 比如用 CentOS 做为我们的基础镜像，那么先把它 pull 下来 [user@host1 ~]$ docker pull centos:6.8 Using default tag: latest latest: Pulling from ubuntu 8aec416115fd: Extracting [================\u0026gt; ] 16.78 MB/50.31 MB 695f074e24e3: Download complete 946d6c48c2a7: Download complete bc7277e579f0: Verifying Checksum ... 然后我们用这个镜像创建一个容器，并在里面配一些我们的大数据平台依赖的参数，比如 ntpd、httpd 服务等等，最终生成我们平台专属的基础镜像。 [user@host1 ~]$ docker run -tid --name build -h build centos:6.8 bash # 创建一个容器 f5e71cd81252a3563a03ad8daee81047b62ab5d892ebbfbf71cf53415f29c130 [user@host1 ~]$ docker exec -ti build bash [root@build ~]$ yum install ntpd # 定制自已的配置 ... [root@build ~]$ exit [user@host1 ~]$ docker commit build my-centos # 保存为自已的镜像 ha256:71cd81252a3563a03ad8daee81047b62ab5d892ebbfbf71cf53415f29c130950 这是很关键的一步，有了它以后，所有人员可以随时创建一个自己需要的Linux环境出来，以便在其内进行产品的研究和实验，且每个人的环境互不相干，当容器内的环境被破坏后，可以删掉再创建，这样一来，场景一和二所遇到的问题也就迎刃面解。 将已经部署好的集群做成镜像# 我们可以把已经部署了集群的容器保存成多种镜像，如：只包含了 Hadoop 的集群、同时包含 Hadoop、Zookeeper、Hbase 的集群，或安装了所有组件的集群等等，然后上传到私有仓库，其它人需要的时候，直接启动自已需要的集群就可以了，因为免去了部暑与配置等步骤，因而大幅度提高了工作效率，也提高了产品迭代速度。\n[user@host1 ~]$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE registry.io:5000/hadoop-base latest 1bc6664767de 4 weeks ago 3.764 GB registry.io:5000/hadoop-standalone3100094120-node1 latest 54468c2b0de1 3 weeks ago 11.64 GB registry.io:5000/hadoop-all3100089120-node3 latest f334355d92e2 4 weeks ago 6.407 GB registry.io:5000/hadoop-all3100089120-node2 latest 3ff0ab7bc121 4 weeks ago 6.21 GB registry.io:5000/hadoop-all3100089120-node1 latest ac9bdb8165b5 4 weeks ago 11.32 GB ...上图是已经做好的镜像，上图中共三种类型的镜像：\n第一个是基础镜像 第二个是单节点版的大数据集群，所以只有一个镜像 最后三个是已经安装了大数据平台的镜像，因为是分布式，所以共有三个 镜像的修改与保存# Docker 提供了 commit 功能，可以将一个正在运行的容器保存起来，假如在测试过程中遇到一个 Bug 并且需要先保存下来，执行一条简单的命令即可，如：\ndocker commit container_name image:v2在以后需要复现的时候用这个镜像创建容器即可，像下面这样\ndocker run -tid --name c1 image:v2 bash但注意，并不是所有状态都能被保存下来，它只保存文件层面的状态，不能保存内存中的状态，所以再次启动容器的时候，容器内的所有服务都已经变成了停止状态，需要再手动启动一次，这样就导致有些类型的 Bug 不能复现。 不过欣慰的是，Docker 官方打算在后面的版本中加入 checkpiont 功能，它可以保存容中的所有状态，这样就可以完整地复现 Bug，这个新功能的用法就像下面这样：\n[user@host1 ~]$ docker checkpoint container1 c1 # 创建检查点 c1 [user@host1 ~]$ docker checkpoint ls container1 # 查看 CHECKEPOINT NAME container1 c1 [user@host1 ~]$ docker start --checkpoint c1 container2 # 恢复检查点这个功能对很多人来说，绝对是个好消息！！\n脚本化部署、监视、删除# 当然了，每个人都不应该把过多的精力放在怎么使用 Docker 的问题上，这样会为团队带来额外的工作量，最简单的办法当然是把所有重复性的工作脚本化，向每个人提供最简便的使用接口，只需要一条简单的命令就可以创建自己想要的集群环境，当不需要的时候一条命令即可删除，这样即降低了学习成本又解决了容器管理问题。 脚本化的实现应该考虑到几个方面：\n多种类型集群的创建 记录每个集群的所属者，容器所属的物理机，创建时间等等 可实时查看所有容器的运行状态，物理机资源使用情况 删除指定的集群 结语# 其实现在已经有很多开源的 Docker 容器管理框架，但需求都是复杂多变的，并不能适用所有人的情况，比如我们的大数据平台就需要为每个容器做端口映射、内含大数据组件的镜像在启动后还要做 Hostname 与 IP 映射等等，这些都需要手动去做。\n在实践过程中，遇到了很多问题，目前虽还算不上成熟，但已经使原本部署与配置很复杂的大数据平台变的简单快速，让一部分人感到很方便，当然了，我们还在不断的完善中。 关于# 作者：张佳军\n阅读：25\n点赞：1\n创建：2017-02-10\n"},{"id":51,"href":"/kubernetes/docker-build-centos65.html","title":"手工构建基础镜像","section":"容器开发","content":"手工构建基础镜像# 前段时间，老大说要把目前正在开发的产品放在Docker容器中跑，而且容器的系统版本必须跟开发一致（CentOS6.5），然后我跑去Docker Hub上找一圈，发现Centos官方并没有提供这个版本！纳尼？？开什么玩笑？好吧，然后又搜了一圈，找到了下面这个链接，解释了为什么没有CentOS6.5： https://github.com/CentOS/sig-cloud-instance-images/issues/13 ，大意是说，官方觉得没必要用CentOS6.5，因为有更高的版本能可以用，并且修复了低版本中的Bug，如果他们提供了6.5的版本，那可能还需要提供6.4，6.3等等，所以直接就不提供低版本了。\n好吧，但最终要用哪个版本不是我说了算的，然后上Docker官网看看怎么手动构建自已的基础镜像，不幸的是，它虽提供了构建CentOS的基础镜像，但并不能构建出6.5版本的，于是乎又是一顿google，最终还是找到一个比较靠谱的文章，英文，且简略，地址我写在本文最后的部分，下面是我结合自已的需求构建镜像的全部过程。\n需要的环境# 一个正常运行的CentOS6.5系统，这里我用的是我们产品的开发环境系统 可以联外网，因为要下载东西3. 一个CentOS6.5的yum源，这里我用的是一个ISO文件直接mount到本地就可以用了 开始构建# 安装febootstrap与xz\nyum install -y febootstrap xz挂载自己的repo镜像，以下是一个例子：\nmount -t iso9660 -o loop /data/CentOS-6.5-x86_64-DVD.iso /data/iso另注：如果你没有这样的iso文件，可以去CentOS官方下载一个，下载这个就行：CentOS-6.5-x86_64-minimal.iso，然后按以上方式挂载。 当然，如果你有自已的yum源服务器也可以，那它应该长这个样子：http://your.host.com/repodir，或者这样子：file:///var/www/html/repodir。 开始生成镜像文件夹\nfebootstrap -i bash -i coreutils -i tar -ai bzip2 -i gzip \\ -i vim-minimal -i wget -i patch -i diffutils -i iproute \\ -i yum centos6.5 centos6.5-base-image file:///data/iso说明： file:///data/iso：自己的repo源 centos6.5：镜像版本 centos6.5-base-image：文件夹名字\n在文件夹内创建三个文件\ntouch centos6.5-base-ks/etc/resolv.conf touch centos6.5-base-ks/sbin/init echo -e \u0026#39;NETWORKING=yes\\nHOSTNAME=build\u0026#39; /etc/sysconfig/network打包文件夹\ntar --numeric-owner -Jcpf centos6.5-base.tar.xz -C centos6.5-base-image . 把打包好的文件发送到Docker环境下，并导入到Docker\nscp centos6.5-base.tar.xz docker.host.com:/root然后去对应的机器上（docker.host.com），把包导入到Docker，使之成为一个Docker镜像\ncat centos6.5-base.tar.xz | sudo docker import - kxdmmr/centos6.5-base:v1 启动镜像\ndocker run -dti --name build -h build \\ kxdmmr/centos6.5-base:v1 bash 验证版本\ndocker exec -ti build cat /etc/redhat-release大功告成，现在可以往里面加自己需要的服务了！\n参考资料# (http://qiita.com/hnakamur/items/8e3136488fcfe763802c) (https://github.com/CentOS/sig-cloud-instance-images/issues/13) (https://github.com/docker/docker/blob/master/contrib/mkimage-yum.sh) 关于# 作者：张佳军\n阅读：31\n点赞：0\n创建：2017-01-07\n"},{"id":52,"href":"/kubernetes/docker-code-create-container.html","title":"DOCKER源码-创建容器","section":"容器开发","content":"DOCKER源码-创建容器# Daemon结构体# daemon/daemon.go\ntype Daemon struct { // ... 省略多行 ... containerdCli *containerd.Client containerd libcontainerdtypes.Client volumes *volumesservice.VolumesService // ... 省略多行 ... }client结构体# Daemon中的containerd字段负责容器相关操作，Daemon对象所有对容器的操作都是通过调用containerd对象相应函数来完成的，它是一个接口，相应的实现定义在libcontainerd/remote/client.go文件中，它具有以下函数（只列出部分）：\nfunc (c *client) Version(ctx context.Context) (containerd.Version, error) func (c *client) Restore(ctx context.Context, id string, attachStdio libcontainerdtypes.StdioCallback) (alive bool, pid int, p libcontainerdtypes.Process, err error) func (c *client) Create(ctx context.Context, id string, ociSpec *specs.Spec, runtimeOptions interface{}, opts ...containerd.NewContainerOpts) error func (c *client) Start(ctx context.Context, id, checkpointDir string, withStdin bool, attachStdio libcontainerdtypes.StdioCallback) (int, error) func (c *client) Exec(ctx context.Context, containerID, processID string, spec *specs.Process, withStdin bool, attachStdio libcontainerdtypes.StdioCallback) (int, error)Client结构体# client结构体是一个较高层的封装，它在实例化的时候依赖Client结构体，而Client结构体实际是一个RPC客户端，所有容器操作都是通过RPC调用containerd进程，关键代码： vendor/github.com/containerd/containerd/api/services/containers/v1/containers.pb.go\nfunc (c *containersClient) Create(ctx context.Context, in *CreateContainerRequest, opts ...grpc.CallOption) (*CreateContainerResponse, error) { out := new(CreateContainerResponse) err := c.cc.Invoke(ctx, \u0026#34;/containerd.services.containers.v1.Containers/Create\u0026#34;, in, out, opts...) if err != nil { return nil, err } return out, nil }containerd程序的启动# 下面进入containerd项目，看一下containerd是怎样处理RPC请求的，先简单说一下containerd的启动流程： main()函数定义在cmd/containerd/main.go：\nfunc main() { app := command.App() if err := app.Run(os.Args); err != nil { fmt.Fprintf(os.Stderr, \u0026#34;containerd: %s\\n\u0026#34;, err) os.Exit(1) } } 它先创建了一个app对象，该对象中包含一个启动函数app.Action()，该函数中启动了RPC Server。 紧接着执行APP对象的Run()函数，此时app.Action()函数被执行，RPC Server启动。 RPC Server接口定义# api/services/containers/v1/containers.pb.go\ntype ContainersServer interface { Get(context.Context, *GetContainerRequest) (*GetContainerResponse, error) List(context.Context, *ListContainersRequest) (*ListContainersResponse, error) ListStream(*ListContainersRequest, Containers_ListStreamServer) error Create(context.Context, *CreateContainerRequest) (*CreateContainerResponse, error) Update(context.Context, *UpdateContainerRequest) (*UpdateContainerResponse, error) Delete(context.Context, *DeleteContainerRequest) (*types.Empty, error) }RPC路由定义# ContainersServer接口没有直接实现，而是通过路由方式将请求映射到相应函数，以下路由定义：\nvar _Containers_serviceDesc = grpc.ServiceDesc{ // ... 省略多行 ... { MethodName: \u0026#34;Create\u0026#34;, Handler: _Containers_Create_Handler, }, // ... 省略多行 ... }RPC Server接口实现# 可以看到，负责创建容器的函数是_Containers_Create_Handler()： api/services/containers/v1/containers.pb.go\nfunc _Containers_Create_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) { in := new(CreateContainerRequest) if err := dec(in); err != nil { return nil, err } if interceptor == nil { return srv.(ContainersServer).Create(ctx, in) } info := \u0026amp;grpc.UnaryServerInfo{ Server: srv, FullMethod: \u0026#34;/containerd.services.containers.v1.Containers/Create\u0026#34;, } handler := func(ctx context.Context, req interface{}) (interface{}, error) { return srv.(ContainersServer).Create(ctx, req.(*CreateContainerRequest)) } return interceptor(ctx, in, info, handler) }路由注册# 由上面函数可以知道，创建容器的动作其实是调用srv对象的Create()函数，那srv对象是谁传进来的呢？我们看一下上面路由时怎样注册的，上面的_Containers_serviceDesc变量在注册RPC Server的时候被使用：\nfunc RegisterContainersServer(s *grpc.Server, srv ContainersServer) { s.RegisterService(\u0026amp;_Containers_serviceDesc, srv) }RegisterContainersServer()函数又是由server调用的： services/containers/service.go\nfunc (s *service) Register(server *grpc.Server) error { api.RegisterContainersServer(server, s) return nil }RPC接口实现# 由此可见，RPC Server中的Create()函数其实是调用service对象的Create()函数： services/containers/service.go\nfunc (s *service) Create(ctx context.Context, req *api.CreateContainerRequest) (*api.CreateContainerResponse, error) { return s.local.Create(ctx, req) }local结构体# 而service对象的Create()函数是调用的local字段的Create()函数，local是一个插件，它是在local包被加载的时候注册的，local的Create()函数如下： services/containers/local.go\nfunc (l *local) Create(ctx context.Context, req *api.CreateContainerRequest, _ ...grpc.CallOption) (*api.CreateContainerResponse, error) { var resp api.CreateContainerResponse if err := l.withStoreUpdate(ctx, func(ctx context.Context) error { container := containerFromProto(\u0026amp;req.Container) created, err := l.Store.Create(ctx, container) if err != nil { return err } resp.Container = containerToProto(\u0026amp;created) return nil }); err != nil { return \u0026amp;resp, errdefs.ToGRPC(err) } if err := l.publisher.Publish(ctx, \u0026#34;/containers/create\u0026#34;, \u0026amp;eventstypes.ContainerCreate{ ID: resp.Container.ID, Image: resp.Container.Image, Runtime: \u0026amp;eventstypes.ContainerCreate_Runtime{ Name: resp.Container.Runtime.Name, Options: resp.Container.Runtime.Options, }, }); err != nil { return \u0026amp;resp, err } return \u0026amp;resp, nil }运行时插件# local调用了db字段的Create()函数，db是一个运行时插件，在1.2.13版本的containerd中包含两个版本的运行时： runtime/v1/linux/runtime.go\nfunc init() { plugin.Register(\u0026amp;plugin.Registration{ Type: plugin.RuntimePlugin, ID: \u0026#34;linux\u0026#34;, InitFn: New, Requires: []plugin.Type{ plugin.MetadataPlugin, }, Config: \u0026amp;Config{ Shim: defaultShim, Runtime: defaultRuntime, }, }) }runtime/v2/manager.go\nfunc init() { plugin.Register(\u0026amp;plugin.Registration{ Type: plugin.RuntimePluginV2, ID: \u0026#34;task\u0026#34;, Requires: []plugin.Type{ plugin.MetadataPlugin, }, Config: \u0026amp;Config{ Platforms: defaultPlatforms(), }, InitFn: func(ic *plugin.InitContext) (interface{}, error) { supportedPlatforms, err := parsePlatforms(ic.Config.(*Config).Platforms) if err != nil { return nil, err } ic.Meta.Platforms = supportedPlatforms if err := os.MkdirAll(ic.Root, 0711); err != nil { return nil, err } if err := os.MkdirAll(ic.State, 0711); err != nil { return nil, err } m, err := ic.Get(plugin.MetadataPlugin) if err != nil { return nil, err } cs := metadata.NewContainerStore(m.(*metadata.DB)) return New(ic.Context, ic.Root, ic.State, ic.Address, ic.TTRPCAddress, ic.Events, cs) }, }) }调用运行时插件# 运行时插件包含两部分，shim和runc，shim来控制创建容器的整个流程，而设置容器的cgroup、namespace、启动容器中的进程等工作则调用runc来完成。 以v2版本为例，创建容器的函数如下： runtime/v2/runc/v2/service.go\nfunc (s *service) Create(ctx context.Context, r *taskAPI.CreateTaskRequest) (_ *taskAPI.CreateTaskResponse, err error) { s.mu.Lock() defer s.mu.Unlock() container, err := runc.NewContainer(ctx, s.platform, r) if err != nil { return nil, err } s.containers[r.ID] = container s.send(\u0026amp;eventstypes.TaskCreate{ ContainerID: r.ID, Bundle: r.Bundle, Rootfs: r.Rootfs, IO: \u0026amp;eventstypes.TaskIO{ Stdin: r.Stdin, Stdout: r.Stdout, Stderr: r.Stderr, Terminal: r.Terminal, }, Checkpoint: r.Checkpoint, Pid: uint32(container.Pid()), }) return \u0026amp;taskAPI.CreateTaskResponse{ Pid: uint32(container.Pid()), }, nil }创建containerd-shim进程# 此时的调用链如下： 调用运行时的Create()函数：\nruntime/v2/manager.go:Create()构建containerd-shim命令对象，准备执行：\nruntime/v2/binary.go:Start()构建一个命令对象（形式为：container-shim \u0026ndash;address /run/containerd/containerd.sock）并执行：\nruntime/v2/shim/util.go:Command()接下来进入containerd-shim程序的main()函数：\ncmd/containerd-shim/main_unix.go:main()开始执行shim任务：\ncmd/containerd-shim/main_unix.go:executeShim()创建TTRPC服务，用于与containerd进程通信。TTRPC基于GRPC开发，以翻译自TTRPC的官方描述：\nTTRPC是用于低内存环境的GRPC，现有的grpc-go项目需要大量的内存开销来导入包和运行时。这对于大多数服务来说是没问题的，但在低内存环境中这可能是一个问题。这个项目减少了二进制文件的大小和协议开销。我们通过省略\u0026quot;net/http\u0026quot;和\u0026quot;net/http2\u0026quot;来做到这一点和“grpc”包使用的grpc取代它与一个轻量级的框架协议。结果是使用较少驻留内存的较小二进制文件与GRPC一样易于使用。请注意，虽然这个项目支持生成的两端协议，生成的服务定义将与常规的不兼容GRPC服务，因为它们不使用相同的协议。\n创建Service对象，该对象中包含容器的增删改查等函数： cmd/containerd-shim/main_unix.go:executeShim()\nsv, err := shim.NewService( shim.Config{ Path: path, Namespace: namespaceFlag, WorkDir: workdirFlag, Criu: criuFlag, SystemdCgroup: systemdCgroupFlag, RuntimeRoot: runtimeRootFlag, }, \u0026amp;remoteEventsPublisher{address: addressFlag}, )注册Service对象到TTRPC路由中： cmd/containerd-shim/main_unix.go:executeShim()\nshimapi.RegisterShimService(server, sv)下面就到了Service对象的Create()函数，该函数包含创建容器的所有逻辑： runtime/v1/shim/service.go\nfunc (s *Service) Create(ctx context.Context, r *shimapi.CreateTaskRequest) (_ *shimapi.CreateTaskResponse, err error) { var mounts []process.Mount for _, m := range r.Rootfs { mounts = append(mounts, process.Mount{ Type: m.Type, Source: m.Source, Target: m.Target, Options: m.Options, }) } rootfs := \u0026#34;\u0026#34; if len(mounts) \u0026gt; 0 { rootfs = filepath.Join(r.Bundle, \u0026#34;rootfs\u0026#34;) if err := os.Mkdir(rootfs, 0711); err != nil \u0026amp;\u0026amp; !os.IsExist(err) { return nil, err } } config := \u0026amp;process.CreateConfig{ ID: r.ID, Bundle: r.Bundle, Runtime: r.Runtime, Rootfs: mounts, Terminal: r.Terminal, Stdin: r.Stdin, Stdout: r.Stdout, Stderr: r.Stderr, Checkpoint: r.Checkpoint, ParentCheckpoint: r.ParentCheckpoint, Options: r.Options, } defer func() { if err != nil { if err2 := mount.UnmountAll(rootfs, 0); err2 != nil { log.G(ctx).WithError(err2).Warn(\u0026#34;Failed to cleanup rootfs mount\u0026#34;) } } }() for _, rm := range mounts { m := \u0026amp;mount.Mount{ Type: rm.Type, Source: rm.Source, Options: rm.Options, } if err := m.Mount(rootfs); err != nil { return nil, errors.Wrapf(err, \u0026#34;failed to mount rootfs component %v\u0026#34;, m) } } s.mu.Lock() defer s.mu.Unlock() process, err := newInit( ctx, s.config.Path, s.config.WorkDir, s.config.RuntimeRoot, s.config.Namespace, s.config.Criu, s.config.SystemdCgroup, s.platform, config, rootfs, ) if err != nil { return nil, errdefs.ToGRPC(err) } if err := process.Create(ctx, config); err != nil { return nil, errdefs.ToGRPC(err) } // save the main task id and bundle to the shim for additional requests s.id = r.ID s.bundle = r.Bundle pid := process.Pid() s.processes[r.ID] = process return \u0026amp;shimapi.CreateTaskResponse{ Pid: uint32(pid), }, nil }总结# dockerd进程和containerd进程之间用GRPC进行通信，dockerd需要创建容器时发送RPC请求给containerd。 containerd执行containerd-shim命令并创建一个新进程。 新的containerd-shim进程调用runc在物理机上创建容器，然后启动一个TTRPC服务用于接受containerd发来的其他调用。 当用户需要在容器内执行其他任务时，containerd发送TTRPC请求给containerd-shim，containerd-shim执行Task模块在容器中创建新的进程。 关于# 作者：sycki\n阅读：112\n点赞：0\n创建：2020-03-13\n"},{"id":53,"href":"/kubernetes/docker-code-create-network.html","title":"DOCKER源码-创建网络","section":"容器开发","content":"DOCKER源码-创建网络# Daemon结构体# daemon/daemon.go\ntype Daemon struct { // ... 省略多行 ... EventsService *events.Events netController libnetwork.NetworkController volumes *volumesservice.VolumesService discoveryWatcher discovery.Reloader // ... 省略多行 ... }Daemon中的netController字段负责网络相关操作，它是一个接口，其接口的定义和实现都在vendor/github.com/docker/libnetwork/controller.go文件中。\ncontroller结构体# controller结构体实现了libnetwork.NetworkController接口，Daemon对象的对网络设备的增删改查都是调用controller对象的相关函数，其中创建网络的关键函数如下： vendor/github.com/docker/libnetwork/controller.go\nfunc (c *controller) NewNetwork(networkType, name string, id string, options ...NetworkOption) (Network, error) { // ... 省略多行 ... network := \u0026amp;network{ name: name, networkType: networkType, generic: map[string]interface{}{netlabel.GenericData: make(map[string]string)}, ipamType: defaultIpam, id: id, created: time.Now(), ctrlr: c, persist: true, drvOnce: \u0026amp;sync.Once{}, loadBalancerMode: loadBalancerModeDefault, } // ... 省略多行 ... err = c.addNetwork(network) // ... 省略多行 ... return network, nil }根据驱动创建网络# addNetwork()函数中先根据network对象的网络类型获取相应的驱动，然后调用该驱动对象的d.CreateNetwork()函数： vendor/github.com/docker/libnetwork/controller.go\nfunc (c *controller) addNetwork(n *network) error { d, err := n.driver(true) if err != nil { return err } // Create the network if err := d.CreateNetwork(n.id, n.generic, n, n.getIPData(4), n.getIPData(6)); err != nil { return err } n.startResolver() return nil }关键函数# 以bridge网络驱动为例，展开其CreateNetwork()函数，它主要对一些关键的配置信息做检查，比如验证IPv4和IPv6的地址是否有误： vendor/github.com/docker/libnetwork/drivers/bridge/bridge.go\nfunc (d *driver) CreateNetwork(id string, option map[string]interface{}, nInfo driverapi.NetworkInfo, ipV4Data, ipV6Data []driverapi.IPAMData) error { // ... 省略多行 ... if err = d.createNetwork(config); err != nil { return err } return d.storeUpdate(config) }分步骤创建网络设备# vendor/github.com/docker/libnetwork/drivers/bridge/bridge.go\nfunc (d *driver) createNetwork(config *networkConfiguration) (err error) { // ... 省略多行 ... for _, step := range []struct { Condition bool Fn setupStep }{ // 如果需要，在桥上启用IPv6。我们甚至对以前存在的桥这样做， // 因为它可能是在这里从以前的安装IPv6还不受支持，需要分配一个IPv6链接本地地址。 {config.EnableIPv6, setupBridgeIPv6}, // 我们确保在现有设备的情况下，网桥具有预期的dipv4和IPv6地址。 {bridgeAlreadyExists \u0026amp;\u0026amp; !config.InhibitIPv4, setupVerifyAndReconcile}, // 启用IPv6转发 {enableIPv6Forwarding, setupIPv6Forwarding}, // 设置环回地址路由 {!d.config.EnableUserlandProxy, setupLoopbackAddressesRouting}, // 设置 IPTables. {d.config.EnableIPTables, network.setupIPTables}, // 我们希望跟踪firewalld配置，以便在启动/重新加载时能够正确应用规则 {d.config.EnableIPTables, network.setupFirewalld}, // 设置IPv4的默认网关 {config.DefaultGatewayIPv4 != nil, setupGatewayIPv4}, // 设置IPv6的默认网关 {config.DefaultGatewayIPv6 != nil, setupGatewayIPv6}, // 添加网络间通信规则 {d.config.EnableIPTables, setupNetworkIsolationRules}, // 如果ICC关闭且启用了IPTables，则配置桥接网络过滤 {!config.EnableICC \u0026amp;\u0026amp; d.config.EnableIPTables, setupBridgeNetFiltering}, } { if step.Condition { bridgeSetup.queueStep(step.Fn) } } return bridgeSetup.apply() }上面for循环中定义了10个函数，每个函数负责一项配置，我为每一步加上了中文注释，这些步骤被放在bridgeSetup对象中，最后调用bridgeSetup.apply()执行里面的每一个函数。\n关于# 作者：sycki\n阅读：10\n点赞：0\n创建：2020-03-13\n"},{"id":54,"href":"/kubernetes/docker-code-start-dockerd.html","title":"DOCKER源码-启动流程","section":"容器开发","content":"DOCKER源码-启动流程# dockerd启动流程# 当用户执行命令systemctl start docker后，systemd检查docker配置文件/usr/lib/systemd/system/docker.service。\n因为docker配置文件中定义了BindsTo=containerd.service，所以containerd将先启动。\n如果containerd启动失败，那么dockerd也将启动失败。\ncontainerd启动成功后，dockerd二进制文件开始启动，开始执行main()函数。\ndockerd程序的main函数所在文件：cmd/dockerd/docker.go\nmain函数中的调用的newDaemonCommand()函数用于构建一个Command对象，该对象定义如下：\ncmd := \u0026amp;cobra.Command{ Use: \u0026#34;dockerd [OPTIONS]\u0026#34;, Short: \u0026#34;A self-sufficient runtime for containers.\u0026#34;, SilenceUsage: true, SilenceErrors: true, Args: cli.NoArgs, RunE: func(cmd *cobra.Command, args []string) error { opts.flags = cmd.Flags() return runDaemon(opts) }, DisableFlagsInUseLine: true, Version: fmt.Sprintf(\u0026#34;%s, build %s\u0026#34;, dockerversion.Version, dockerversion.GitCommit), } Command对象中的RunE字段就是用来启动dockerd的函数，它最终调用了DaemonCli.start()函数，DaemonCli定义： cmd/dockerd/daemon.go\ntype DaemonCli struct { *config.Config configFile *string flags *pflag.FlagSet api *apiserver.Server d *daemon.Daemon authzMiddleware *authorization.Middleware } 而DaemonCli实例中的start()函数是dockerd启动的主要逻辑，主要包含以下事项：\n从命令行中加载日志相关参数，并配置logrus日志对象 根据命令行参数判断是否打开Debug模式，是否打开实验特性 创建--data-root目录 创建PID文件 是否启用无root模式 加载API Server相关配置，包括证书 创建API Server，但现在不启动 创建一个协程与containerd保持心跳，如果连接containerd失败，则dockerd启动失败 监听信号量并设置hook函数用于释放DaemonCli对象 创建Daemon实例，创建Daemon的过程中创建了几个重要对象（这里只列出部分，后面会细讲）： containerd：负责容器相关操作，且依赖一个RPC客户端，大部分容器相关操作都通过RPC调用containerd进程来完成 volumes：负责volume的创建、查找、删除等 imageService：负责镜像相关操作 netController：负责网络设备的相关操作 将Daemon对象放入DaemonCli对象中 启动Metrics服务，默认地址：unix:/var/run/docker/metrics.sock 如果之前配置过swarm的话，则重新加入集群，并启动集群总的服务 构建API路由：用Daemon对象中包含的各核心功能与API Path对应起来 启动API Server，默认地址：unix:/var/run/docker.sock 阻塞直到API Server协程发生错误或完成退出后，结束各个协程 Daemon结构体# 以下是Daemon结构体的定义，在19.03.7版本中包含40个字段，下面我们只分析几个重要字段： daemon/daemon.go\ntype Daemon struct { // ... 省略多行 ... repository string containers container.Store containersReplica container.ViewDB execCommands *exec.Store imageService *images.ImageService netController libnetwork.NetworkController volumes *volumesservice.VolumesService discoveryWatcher discovery.Reloader root string graphDrivers map[string]string // By operating system containerdCli *containerd.Client containerd libcontainerdtypes.Client // ... 省略多行 ... }字段说明：\nroot：字符串，记录了--data-root选项的值\nrepository：字符串，记录了存储容器的目录：--data-root选项的值 + /containers\ncontainers：一个容器对象缓存器，方便其他需要频繁查询容器状态的模块使用\ncontainersReplica：目前是一个用基数树算法实现的内存式数据库，用于快速查找容器\nvolumes：负责volume的创建、查找、删除等，在使用默认驱动local的情况下，创建volume的工作由dockerd进程通过创建本地目录来完成，如果已存在同名的vlume，则直接返回该volume，关键代码：\nvolume/local/local.go\nfunc (r *Root) Create(name string, opts map[string]string) (volume.Volume, error) { // ... 省略多行 ... v, exists := r.volumes[name] if exists { return v, nil } path := r.DataPath(name) if err := idtools.MkdirAllAndChown(path, 0755, r.rootIdentity); err != nil { return nil, errors.Wrapf(errdefs.System(err), \u0026#34;error while creating volume path \u0026#39;%s\u0026#39;\u0026#34;, path) } // ... 省略多行 ... imageService：负责镜像相关操作，镜像的操作都是由dockerd进程在本地完成\nnetController：负责网络设备的相关操作，详见另一篇文章：DOCKER源码-创建网络\ncontainerd：负责容器相关操作，且依赖一个RPC客户端，大部分容器相关操作都通过RPC调用containerd进程来完成，详见另一篇文章：DOCKER源码-创建容器\n相关文章# DOCKER源码-创建网络 DOCKER源码-创建容器 关于# 作者：sycki\n阅读：57\n点赞：0\n创建：2020-03-13\n"},{"id":55,"href":"/kubernetes/docker-config-direct-lvm.html","title":"配置direct-lvm","section":"容器开发","content":"配置direct-lvm# 关于 Docker 存储引擎# 在 CentOS 上安装 Docker 时，默认的存储方式为 devicemapper，而 devicemapper 又有两种模式，默认为 loop-lvm，也就是挂载 loop 设备的方式，在安装 Doccker 后，它会挂载两个 loop 设备用作存储，这两个 loop 设备对应 Docker 安装目录下的两个文件，如下：\n[user@ser0 ~]$ sudo losetup -a /dev/loop0: [64770]:2164801071 (/var/lib/docker/devicemapper/devicemapper/data) /dev/loop1: [64770]:2164801072 (/var/lib/docker/devicemapper/devicemapper/metadata)看下它们的大小：\n[user@ser0 ~]$ sudo ls -hls /var/lib/docker/devicemapper/devicemapper/ total 52G 52G -rw-------. 1 root root 100G Jan 7 07:28 data 54M -rw-------. 1 root root 2.0G Jan 7 15:28 metadata它的优点是不用过多配置，开箱即用，但性能很差，默认可使用的空间是100G，在使用时有各种问题，官方不推荐这种方式用在生产中，另一种方式是direct-lvm，性能好且稳定，本文将介绍如何配置它。\n基本原理# 准备一块单独的磁盘给Docker存储用，逻辑物理均可，用lvm2工具将这块磁盘分为两个逻辑卷，再将这两个逻辑卷转换成thin pool类型，然后配置给Docker。\n开始配置# 查看当前存储引擎：\n[user@ser0 ~]$ sudo docker info ... Server Version: 1.12.1 Storage Driver: devicemapper Pool Name: docker-253:2-6442838537-pool # 这里表示当前是loop-lvm模式 Pool Blocksize: 65.54 kB Base Device Size: 53.69 GB Backing Filesystem: xfs Data file: /dev/loop0 Metadata file: /dev/loop1 Data Space Used: 55.5 GB Data Space Total: 107.4 GB # 默认只有100G可用空间，届时将无法创建容器 Data Space Available: 51.88 GB Metadata Space Used: 33.37 MB Metadata Space Total: 2.147 GB Metadata Space Available: 2.114 GB Thin Pool Minimum Free Space: 10.74 GB Udev Sync Supported: true ...查看磁盘使用情况\n[user@ser0 ~]$ sudo df -hT 文件系统 类型 容量 已用 可用 已用% 挂载点 /dev/mapper/centos-root xfs 50G 842M 50G 2% / devtmpfs devtmpfs 63G 0 63G 0% /dev tmpfs tmpfs 63G 0 63G 0% /dev/shm tmpfs tmpfs 63G 8.8M 63G 1% /run tmpfs tmpfs 63G 0 63G 0% /sys/fs/cgroup /dev/sda2 xfs 497M 123M 374M 25% /boot /dev/mapper/centos-home xfs 5.5T 33M 5.5T 1% /home tmpfs tmpfs 13G 0 13G 0% /run/user/0看来有一块磁盘是专门用来做数据分区的，即/dev/mapper/centos-home，而且来没有被使用，那我们就用它来配置 Docker 的 direct-lvm模式 了\n停掉 Docker服务，如果本机上有镜像的话，先push到 Docker Hub 或者自的私有仓库中，容器当然也一样。\nsudo systemctl stop docker卸载逻辑卷\nsudo umount /home删除逻辑卷，请一定备份好数据\n[user@ser0 ~]$ sudo lvdisplay ... --- Logical volume --- LV Path /dev/centos/home LV Name home VG Name centos LV UUID l0j4kt-eOu4-1fSh-WIqU-X4mN-SAmj-ThdaNO LV Write Access read/write LV Creation host, time localhost, 2017-01-05 00:35:37 +0800 LV Status available $ sudo open 1 LV Size 5.40 TiB Current LE 1416435 Segments 1 Allocation inherit Read ahead sectors auto - currently set to 256 Block device 253:2 ...这里看到它属于cetnos这个VG，然后删除它：\nsudo lvremove /dev/centos/home这时进入 /etc/fstab 查看 /dev/centos/home 这一行在不在了，如果还在，一定将 /dev/centos/home 这一行注释掉，否则系统将无法启动！！\nVG空间剩余量：\n[user@ser0 ~]$ sudo vgdisplay --- Volume group --- VG Name centos System ID Format lvm2 Metadata Areas 1 Metadata Sequence No 21 VG Access read/write VG Status resizable MAX LV 0 Cur LV 4 Open LV 3 Max PV 0 Cur PV 1 Act PV 1 VG Size 5.46 TiB PE Size 4.00 MiB Total PE 1430274 Alloc PE / Size 1430222 / 5.46 TiB Free PE / Size 1410256 / 5.30 TiB VG UUID W6gHFG-SVtL-wdck-amUk-SRHL-ct7h-Zv70Pd这里看到刚才那LV的容量都归centos这个VG了\n创建thin pool\n用centos中剩余的空间创建两个LV\nsudo lvcreate --wipesignatures y -n thinpool -l 90%VG centos sudo lvcreate --wipesignatures y -n thinpoolmeta -L 2G centos将两个 LV 合并为 thin pool\nsudo lvconvert -y --zero n -c 512K --thinpool centos/thinpool --poolmetadata centos/thinpoolmeta配置thin pool\n[user@ser0 ~]$ sudo vi /etc/lvm/profile/docker-thinpool.profile activation{ thin_pool_autoextend_threshold=80 thin_pool_autoextend_percent=20 }使配置生效\nsudo lvchange --metadataprofile docker-thinpool centos/thinpool查看 thinpool 是否能监视到状态\n[user@ser0 ~]$ sudo lvs -o+seg_monitor LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert data centos -wi-ao---- 469.00g root centos -wi-ao---- 50.00g swap centos -wi-ao---- 31.50g thinpool centos twi-a-t--- 4.91t 0.00 0.07 修改Docker配置\n[user@ser0 ~]$ sudo vi /usr/lib/systemd/system/docker.service ... ExecStart=/usr/bin/dockerd \\ --storage-driver=devicemapper \\ --storage-opt=dm.thinpooldev=/dev/mapper/centos-thinpool \\ --storage-opt=dm.use_deferred_removal=true \\ --storage-opt=dm.use_deferred_deletion=true ExecReload=/bin/kill -s HUP $MAINPID ...启动Docker服务，如果有以前的镜像数据必须删掉，所以先备份以前的数据：\nsudo mv /var/lib/docker /var/lib/docker.bak sudo systemclt start docker查看当前存储引擎：\n[user@ser0 ~]$ sudo docker info ... Server Version: 1.12.5 Storage Driver: devicemapper Pool Name: centos-thinpool # 这里是我们刚才创建的LV逻辑盘 Pool Blocksize: 524.3 kB Base Device Size: 10.74 GB Backing Filesystem: xfs Data file: Metadata file: Data Space Used: 19.92 MB Data Space Total: 5.399 TB # 总大小也变了 Data Space Available: 5.399 TB Metadata Space Used: 2.916 MB Metadata Space Total: 4.295 GB Metadata Space Available: 4.292 GB Thin Pool Minimum Free Space: 539.9 GB Udev Sync Supported: true ...到这里 direct-lvm 配置就完成了\n参考# LVM相关介绍\n关于# 作者：张佳军\n阅读：49\n点赞：1\n创建：2017-01-07\n"},{"id":56,"href":"/kubernetes/docker-dynamic-port-map.html","title":"动态映射端口","section":"容器开发","content":"动态映射端口# 虽然设计者们一再地强调在使用Docker时要遵循最佳实践，但很多情况下并不能完全做到最佳实践，就比如我们现在的情况，是把Docker当做虚拟机来用，每个容器中包含了很多服务，，，当然，这也带来了很多问题，就比如：要映射很多的端口到物理机。\n痛苦的映射问题# 在启动一个Docker容器时，可以指定-p参数来把容器内的端口映射到宿主机的IP端口上，这样可以很方便地从外界访问容器中的服务，一开始全都是用-p这种方式来做，如果容器中有10个端口需要映射到外面，那就指定10个-p选项，实际上可能更多，如下：\n[root@blog ~]# docker run \\ --name node1 \\ -p 10.0.82.43:22:22 \\ -p 10.0.82.43:8080:8080 \\ -p 10.0.82.43:6066:6066 \\ -p 10.0.82.43:7071:7071 \\ -p 10.0.82.43:111:111 \\ -p 10.0.82.43:5005:5005 \\ -p 10.0.82.43:6265:6265 \\ -p 10.0.82.43:7699:7699 \\ ... -tid node-base 然后当docker ps的时候会看到整个屏幕被-p参数给占满了，，，好吧，这个不是问题，问题是如果在使用了很久后发现有一个端口没有映射出来！或者是你需要通过Java API远程调用容器中的程序，而又恰好没有映射那个端口，那该怎么办？\n映射的实现# 其实Docker run命令中的-p选项，最终是通过宿主机上的iptables来实现的（也许你早就发现了，只是没有仔细研究），在Docker的宿主机上查看一下iptables的nat表，大概会看到下面这样：\n[root@blog ~]# iptables -t nat -nvL Chain PREROUTING (policy ACCEPT 1082K packets, 173M bytes) pkts bytes target prot opt in out source destination 637 37876 DNAT tcp -- * * 0.0.0.0/0 10.100.124.231 tcp dpt:80 to:10.100.124.231:5000 452K 27M DOCKER all -- * * 0.0.0.0/0 0.0.0.0/0 ADDRTYPE match dst-type LOCAL Chain INPUT (policy ACCEPT 267K packets, 29M bytes) pkts bytes target prot opt in out source destination Chain OUTPUT (policy ACCEPT 144K packets, 8644K bytes) pkts bytes target prot opt in out source destination 26032 1563K DOCKER all -- * * 0.0.0.0/0 !127.0.0.0/8 ADDRTYPE match dst-type LOCAL Chain POSTROUTING (policy ACCEPT 144K packets, 8666K bytes) pkts bytes target prot opt in out source destination 108K 7893K MASQUERADE all -- * !docker0 192.11.231.0/24 0.0.0.0/0 91402 6409K MASQUERADE all -- * !docker_gwbridge 192.12.231.0/24 0.0.0.0/0 0 0 MASQUERADE tcp -- * * 192.11.231.2 192.11.231.2 tcp dpt:5000 0 0 MASQUERADE tcp -- * * 192.11.231.8 192.11.231.8 tcp dpt:8080 0 0 MASQUERADE tcp -- * * 192.11.231.8 192.11.231.8 tcp dpt:5005 Chain DOCKER (2 references) pkts bytes target prot opt in out source destination 4 264 RETURN all -- docker0 * 0.0.0.0/0 0.0.0.0/0 9 540 RETURN all -- docker_gwbridge * 0.0.0.0/0 0.0.0.0/0 26315 1579K DNAT tcp -- !docker0 * 0.0.0.0/0 10.100.124.231 tcp dpt:5000 to:192.11.231.2:5000 0 0 DNAT tcp -- !docker0 * 0.0.0.0/0 10.100.124.115 tcp dpt:8080 to:192.11.231.8:8080 0 0 DNAT tcp -- !docker0 * 0.0.0.0/0 10.100.124.115 tcp dpt:5005 to:192.11.231.8:5005最主要最下面的DOCKER链，最后那三条规则就是在启动容器时指定了-p所导致的，那个5000端口你应该很熟悉，那是Register服务的端口号，我们可以通过它在局域网中上传和下载镜像，而且我在启动它的时候也只映射了这一个端口，如下：\n[root@blog ~]# docker ps -f name=repo CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 867106b006da registry:2 \u0026#34;/entrypoint.sh /etc/\u0026#34; 4 months ago Up 4 months 10.100.124.231:5000-\u0026gt;5000/tcp repo跟前面的提到的三条规则对比一下，是不是明白了？iptables把从10.100.124.231:5000这个端口过来的数据转发给了192.11.231.2:5000，其中192.11.231.2就是repo容器的IP。\n动态映射# 现在我们就用它来说明好了，假设在repo这个容器中还有一个httpd服务并且监听着8080端口，那我要怎样才能从外面访问它呢？其实很简单啊，在宿机上增加一条iptables规则就可以了，如下：\n[root@blog ~]# iptables -t nat -A DOCKER -d 10.100.124.231/32 ! -i docker0 -p tcp -m tcp --dport 8080 -j DNAT --to-destination 192.11.231.2:8080再看下iptables中的规则：\n[root@blog ~]# iptables -t nat -nvL ... #前面省略掉 Chain DOCKER (2 references) pkts bytes target prot opt in out source destination 4 264 RETURN all -- docker0 * 0.0.0.0/0 0.0.0.0/0 9 540 RETURN all -- docker_gwbridge * 0.0.0.0/0 0.0.0.0/0 26315 1579K DNAT tcp -- !docker0 * 0.0.0.0/0 10.100.124.231 tcp dpt:5000 to:192.11.231.2:5000 0 0 DNAT tcp -- !docker0 * 0.0.0.0/0 10.100.124.115 tcp dpt:8080 to:192.11.231.8:8080 0 0 DNAT tcp -- !docker0 * 0.0.0.0/0 10.100.124.115 tcp dpt:5005 to:192.11.231.8:5005 0 0 DNAT tcp -- !docker0 * 0.0.0.0/0 10.100.124.231 tcp dpt:8080 to:192.11.231.2:8080现在已经可以通过10.100.124.231:8080来访问repo容器内的httpd服务了，当然了，可以增加就可以删除啊，只要把前面那条增加命令中的-A换成-D就可以了。。\n换一种映射方式# 后来想，既然可以动态映射，那-p选项是不是就可以省略掉了？？So，后来在启动容器时，启动命令就变成这样了：\n[root@blog ~]# docker run \\ --name node1 \\ -p 10.0.82.43:22:22 \\ -tid node-base [root@blog ~]# con_ip=`docker inspect --format=\u0026#39;{{.NetworkSettings.Networks.bridge.IPAddress}}\u0026#39; node1` [root@blog ~]# iptables -t nat -A DOCKER -d 10.0.82.43/32 ! -i docker0 -p tcp -m tcp --dport 1025:62000 -j DNAT --to-destination $con_ip就是在启动容器的时候只映射一个端口出来，然后通过命令得到这个容器的IP，最后在宿主机上用iptables将容器的所有端口都映射出来，干净利落，又省去了很多麻烦。\n-End-\n关于# 作者：张佳军\n阅读：73\n点赞：0\n创建：2017-05-31\n"},{"id":57,"href":"/kubernetes/docker-multi-host-with-zk.html","title":"多主机通信之ZK","section":"容器开发","content":"多主机通信之ZK# 目前（2016.09.15）公司正在开发一个新产品，每次测试组在部署新版本时，需要先将 Linux 环境配好，卸载后又需要重新配置 Linux 环境，显然中间有很多重复性工作，所以为了加快产品版本迭代速度，减少重复性工作，我们利用Docker来做环境隔离和快速部署。 而为了测试产品性能，又须要运行在多个物理机上，那么Docker如何在多物理机间通信，就是首先要解决的问题。 上网找了很久的资料，大概有以下几种：\nSwarm模式：这是官方提供的Docker集群模式，且能够跨物理机通信，使用起简单方便，先看一个简单的例子，下面的命令在Swarm集群中创建一个Service：\ndocker service create \\ --constraint node.hostname==biyu-c3.com \\ --name sys \\ --replicas 1 \\ --network ingress \\ kxdmmr/centos6.8-ssh bash -c \u0026#39;/usr/sbin/sshd -D\u0026#39;而我们启动自已的容器可能要加一些别的参数，比如：\ndocker run --privileged=true --ip 19.19.19.19 --name hostname -h hostname -p 8080:8080 -p 80:80 -v /data/hadoop-data:/data --storage-opt size=30G -tid registry.io:5000/centos6.5如上，其中的选项：--privileged、--ip 在 Swarm 模式中是不提供的，目前也没有办法实现（当然你也可以自已把 Docker 重写了\u0026hellip;），所以不得不另寻它路。 自己修改网络配置，将 Dockre 网桥与物理机设在同一网段\u0026hellip;，这种方式一看我就头大了，非官方的，太麻烦，且没有保证。\n用第三方的服务发现工具，Docker 支持的工具有：Consul、Etcd、Zookeeper，官方给了一个列子，是在一个物理机创建多个虚拟机来模拟的，跟在物理机上还是有区别的，而且官方也没有给多更多资料，只好自已摸索了，好在最终用Zookeeper解决了问题。这种方式的好处是，我们在创建和使用容器时，跟以前没有任何区别，启动容器后，各物理机上的容器可以直接通信，且可以使用以前使用的任何选项。下面我们就动手实现一下。\n基本原理# 原理就是在物理机上搭建 Zookeeper，然后每个机器上的 Docker daemon 进程将网络配置和每个容器的ip等信息存储到 Zookeeper，具体细节较复杂，笔者没有彻底明白。\n环境描述# 硬件环境 机器数量：5\n100.0.8.27 node27.kxdmmr.com 100.0.8.28 node28.kxdmmr.com 100.0.8.29 node29.kxdmmr.com 100.0.8.30 node30.kxdmmr.com 100.0.8.31 node31.kxdmmr.com系统环境 系统版本：官方 CentOS 7 内核版本：3.10.0-327.el7.x86_64 Docker 环境 Docker版本：1.12.1 基础镜像拉取地址：daocloud.io/centos:6 基础镜像系统版本：centos 6.8\n开始构建# 搭建ZK服务# 本次我们搭建三个节点的ZK集群，分别为node27, node29, node30三台机器，从官网下载ZK到leap27，解压到 /opt/modules/\ntar -zxf zookeeper-3.4.6.tar.gz -C /opt/modules/修改ZK配置文件，如下：\ncd /opt/modules/zookeeper-3.4.6/ cp conf/zoo_sample.cfg conf/zoo.cfg vi conf/zoo.cfg 发送ZK到其它两台机器\nscp -r /opt/modules/zookeeper-3.4.6/ node29:/opt/modules/ scp -r /opt/modules/zookeeper-3.4.6/ node30:/opt/modules/ 为每个机器上的ZK创建myid文件，这里以node27为例\nmkdir /opt/modules/zookeeper-3.4.6/data echo 27 \u0026gt; /opt/modules/zookeeper-3.4.6/data/myid启动所有ZK服务，这里为node27为例\n/opt/modules/zookeeper-3.4.6/bin/zkServer.sh start查看每个ZK服务状态\n/opt/modules/zookeeper-3.4.6/bin/zkServer.sh status修改Docker配置# 打开Docker配置文件，在ExecStart=/usr/bin/dockerd后面追加内容：\n--cluster-store=zk://100.0.8.27:2181,100.0.8.29:2181,100.0.8.30:2181 --cluster-advertise=100.0.8.27:2376 如下：\nvi /usr/lib/systemd/system/docker.service重启Docker\nsystemctl daemon-reload; systemctl restart docker每台机器上的Docker配置均作以上修改并重启Docker服务。\n创建全局网络# 做完以上操作时，就可以在Docker内创建Overlay类型的网络了\ndocker network create --driver overlay --subnet=19.19.0.0/16 overlay然后就可以看到多了一个Docker网络，且每台机器上都能看到\ndocker network ls启动容器并测试# 在node27上启动一个容器\ndocker run -dt --name test1.kxdmmr.com -h test1 \\ --network overlay kxdmmr.io:5000/centos6.5-ks在node28上启动一个容器\ndocker run -dt --name test2.kxdmmr.com -h test2 \\ --network overlay kxdmmr.io:5000/centos6.5-ks进入容器test1\ndocker exec -ti test1.kxdmmr.com bash ping test2.kxdmmr.com能ping通当然就大功告成咯！！\n问题总结# --bip 11.11.0.1/16：如果 Docker 自动分配的 ip 与本机 ip 冲突，可以在配置内修改Docker默认网关\n--graph=/data/docker：修改 Docker 默认存储位置，包括镜像，容器等，在配置内加此参数\n关于# 作者：张佳军\n阅读：34\n点赞：0\n创建：2017-01-01\n"},{"id":58,"href":"/kubernetes/docker-registry-deploy-manage.html","title":"私有仓库的搭建与管理","section":"容器开发","content":"私有仓库的搭建与管理# Docker Registry 简介# 怎样在几个机器之间使用同一个镜像？难道要先上传到官方的仓库，另几个机器去下载吗？当然不是了，最好的方法就是搭建一个本地的仓库。 Docker Registry 就是官方提供的搭建本地仓库的方案，它有两种搭建方式， http 与 https ，如果你只简单的在本地使用，第一种就足够了，如果考虑到安全方面，就用 https ，这种方式当然就需要为你的仓库服务器申请证书了，或者自建一个证书机构，自已为自已颁发证书。本文只介绍 http 方式，并介绍仓库的上传、下载、查询、删除等常用操作。\n环境描述# 假设我有三台机器，他它们的/etc/hosts文件包含如下内容：\n[user@ser1 ~]$ cat /etc/hosts 10.100.100.101 ser1.node.com registry.io 10.100.100.102 ser2.node.com 10.100.100.103 ser3.node.com很显然，我要在第一台机器上部署 Docker Registry ，并且这三台机器都已安装好了Docker服务\nDocker Registry 搭建# 官方提供的私有仓库是以镜像像方式提供的，也就是说你只要把这个镜像下载下来启动就可以了，当然还要自已定制一些运行参数。 关于所有可配置项，可以去官网查看，具体地址在本文尾部。\n在第一台机器上运行官方提供的 Registry 容器\nsudo mkdir -p /data/docker-registry docker run -dt \\ --name registry \\ -p 10.100.100.101:5000:5000 \\ --restart=always \\ -v /data/docker-registry:/var/lib/registry \\ -e REGISTRY_STORAGE_DELETE_ENABLED=true \\ -e REGISTRY_STORAGE_DELETE_REDIRECT=true \\ registry:2参数说明： -p 10.100.100.101:5000:5000 这是把容器的端口映射到物理机，这样在其它机器直接访问物理机就相当于访问仓库了 --restart=always 这个参数可以让容器在发生意外的时候还可以自已启动，比如当我重启 Docker 服务的时候 -v /data/docker-registry:/var/lib/registry 找一个容量大的挂载点映射给容器，用来存放上传的镜像 -e 最后这两个是让我们可以远程删除已经上传的镜像\n看下启动了没有\n[user@ser1 ~]$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 867106b006da registry:2 \u0026#34;/entrypoint.sh /etc/\u0026#34; 10 seconds ago Up 10 seconds 10.100.100.101:5000-\u0026gt;5000/tcp registry候改其它每台机器上的 Docker 配置文件，使得其它机器信任此仓库，在其配置文件中找到ExecStart一行，在后面追加内容 \u0026ndash;insecure-registry registry.io:5000 这里以 CentOS7 为例，如下：\n[user@ser1 ~]$ sudo vi /usr/lib/systemd/system/docker.service ... [Service] ExecStart=/usr/bin/dockerd --insecure-registry registry.io:5000 ExecReload=/bin/kill -s HUP $MAINPID ...重新启动 Docker 服务，使配置生效\nsudo systemctl daemon-reload ; sudo systemctl restart docker我们在第二台机器上，上传一个镜像试一下\n[user@ser2 ~]$ docker images # 看一下本地有哪些镜像 paulczar/glusterfs latest dc2e1257bba4 2 years ago 277 MB [user@ser2 ~]$ docker tag paulczar/glusterfs:latest registry.io:5000/glusterfs # 修改镜像名 [user@ser2 ~]$ docker push registry.io:5000/glusterfs # 上传到自已的仓库 2e92feb4cbe1: Pushing [==============================\u0026gt; ] 189.1 MB/277 MBOK!!，顺利上传至我们的私有仓库，而且速度很快\nDocker Registry REST API 简介# 有了自已的仓库，那我们想看一下仓库中已经上传了哪些镜像，或者想删除自已上传的镜像，怎么办？难道我要去看 /data/docker-registry 目录下的内容？？不用担心，官方已经提供了方法， Registry 本身对外提供了 HTTP 操作接口，很方便。 先看一个例子，下面这条命令是测试 registry 版本是不是2.x版本的，这里我们在ser2这台机器上执行\n[user@ser2 ~]$ curl -i -X GET registry.io:5000/v2/ HTTP/1.1 200 OK Content-Length: 2 Content-Type: application/json; charset=utf-8 Docker-Distribution-Api-Version: registry/2.0 X-Content-Type-Options: nosniff Date: Sat, 21 Jan 2017 14:13:25 GMT注意看输出信息的第一行，响应码200表示请求成功，也就是说我们的 registry 版本确实是2.x版本\n查看仓库中的镜像# 那我们再来一条命令，看一下我们已经上传的的有镜像\n[user@ser2 ~]$ curl -H \u0026#34;Docker-Distribution-API-Version: registry/2.0\u0026#34; -X GET registry.io:5000/v2/_catalog {\u0026#34;repositories\u0026#34;:[\u0026#34;glusterfs\u0026#34;]}它返回了一个Json串，里面包含了我们刚刚上传的镜像，是不是很棒？？但聪明的你也许已经发现，这个返回的信息里应该包含镜像标签的啊，后面不是应该有个:latest吗？ 不着急，再来一条命令，用来查询某一镜像的所有标签（或者说版本）\n[user@ser2 ~]$ curl -H \u0026#34;Docker-Distribution-API-Version: registry/2.0\u0026#34; -X GET registry.io:5000/v2/glusterfs/tags/list {\u0026#34;name\u0026#34;:\u0026#34;glusterfs\u0026#34;,\u0026#34;tags\u0026#34;:[\u0026#34;latest\u0026#34;]}看到了吧，如果有多个tag的话，这里会全部列出来\n删除仓库中的镜像# 删除要稍麻烦一点，分两步\n先获取某镜像的 digest (返回的消息头中的 Docker-Content-Digest 后面部分)\n[user@ser2 ~]$ curl -H \u0026#34;Authorization: Basic \u0026lt;hash_here\u0026gt;\u0026#34; -i -X GET \\ -H \u0026#34;Accept: application/vnd.docker.distribution.manifest.v2+json\u0026#34; \\ http://registry.io:5000/v2/glusterfs/manifests/v1 HTTP/1.1 200 OK Content-Length: 745 Content-Type: application/vnd.docker.distribution.manifest.v2+json Docker-Content-Digest: sha256:97de138832d894bf87af449c012538fe1c94dd078a2dc8941f0790f498738c4a Docker-Distribution-Api-Version: registry/2.0 Etag: \u0026#34;sha256:97de138832d894bf87af449c012538f31c94dd078a2dc8941f0790f498738c4a\u0026#34; X-Content-Type-Options: nosniff Date: Sat, 21 Jan 2017 14:41:32 GMT { \u0026#34;schemaVersion\u0026#34;: 2, \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.docker.distribution.manifest.v2+json\u0026#34;, \u0026#34;config\u0026#34;: { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.docker.container.image.v1+json\u0026#34;, \u0026#34;size\u0026#34;: 1232, \u0026#34;digest\u0026#34;: \u0026#34;sha256:78f3c04c6b8654dd86d0e09148dac08f34eaddd008aeb2926cd6a3a74j84f3d5\u0026#34; }, \u0026#34;layers\u0026#34;: [ { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.docker.image.rootfs.diff.tar.gzip\u0026#34;, \u0026#34;size\u0026#34;: 1300037181, \u0026#34;digest\u0026#34;: \u0026#34;sha256:065b2f8d16ae85886f19afb12a9ed3eaad2k04484a6143f218ebb39eac43a61d\u0026#34; }, { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.docker.image.rootfs.diff.tar.gzip\u0026#34;, \u0026#34;size\u0026#34;: 5600560669, \u0026#34;digest\u0026#34;: \u0026#34;sha256:f074337a2c9c17aaf3ce1ed9edcb8b08cbc5e3f730032b721917f24a8920a653\u0026#34; } ] }然后用上面返回的消息头中的 Docker-Content-Digest 后面的值，带入下一条命令\n[user@ser2 ~]$ curl -H \u0026#34;Accept: application/vnd.docker.distribution.manifest.v2+json\u0026#34; -i -X DELETE \\ http://registry.io:5000/v2/glusterfs/manifests/sha256:97de138832d894bf87af449c012538fe1c94dd078a2dc8941f0790f498738c4a HTTP/1.1 200 OK ...返回码200表示删除成功了，但是！！这里注意一下，此时镜像并不一定是真的被从仓库中的物理机上删除了，大部分情况下只是让我们看不到了而已，为什么说不一定呢？这要从镜像的组成原理说起，一个镜像通常是由多个层组成的，理论上，只有我们删除的这个镜像的所有依赖的层都被删了，它才可能被真正删除，而在我多次实验中，即使把所有的层都删除了，镜像也没有从物理机上消失，不知为何，感兴趣的朋友可以试下。\n参考资料# Registry 所有可能配置项 Docker REST API 使用 关于# 作者：张佳军\n阅读：24\n点赞：0\n创建：2017-01-21\n"},{"id":59,"href":"/kubernetes/docker-with-glusterfs.html","title":"数据持久化之GlusterFS","section":"容器开发","content":"数据持久化之GlusterFS# 在用 Docker 搭建集群时，遇到一些数据持久化的问题，如果容器内的数据量大的话就不能将数据保存在容器内，否则不方便迁移，而如果使用 Volume 这样的方式挂载，那么假如容器挂掉以后，会在别的物理机上重新启起来（如 Swarm 模式），这样 Volume 方式就不生效了。所以容器内的数据的持久化是个必须解决的问题，相信很多人也遇到了这样的需求，本文就此问题介绍一些解决方法。\n关于 GlusterFS# GlusterFS 是一种开源的分布式文件系统，易于横向扩展，性能高，可直接用 Linux 系统的 mount 命令从远程挂载到本地来使用，比如下面一条命令：\nmount -t glusterfs glusterfs.server.com:/src-volume /mnt说明： -t glusterfs 是指挂载格式为 glusterfs glusterfs.server.com 是指 glusterfs 服务所在的机器是址，这里换成IP当然也是可以的啦 /mnt 是一个本地目录 这样就将 glusterfs 服务器上的一个卷挂载到本地了，然后就可以直接对 /mnt 下的文件进行存取，是不是感觉很方便？那怎么把它跟 Docker 结合起来呢？有两种方式， Docker 的 plugin 功能和容器内部挂载方式\n第一种：Docker 的 plugin 功能# Docker 本身提供了一个 plugin 的功能模块，允许用户安装各种插件来扩展 Docker 的功能，而 GlusterFS 就是被其支持的插件之一，安装此插件后，就可以在启动容器的时候挂载 GlusterFS 类型的 Volume ，那么具体怎么做呢？\n部署 GlusterFS 服务，教程网上多的是，这里提供一篇： http://navyaijm.blog.51cto.com/4647068/1258250 我们假设有三台主机部署了 GlusterFS ，分别为：gfs-1、gfs-2、gfs-3\n在你安装有 Docker 的主机上，安装 go 语言环境，如已安装请跳过此步，安装方法请转到： https://github.com/astaxie/build-web-application-with-golang/blob/master/zh/01.1.md\n用go命令下载 GlusterFS 插件\ngo get github.com/calavera/docker-volume-glusterfs启动 GlusterFS 插件\nsudo docker-volume-glusterfs -servers gfs-1:gfs-2:gfs-3启动容器\nsudo docker run --volume-driver glusterfs --volume datastore:/data alpine touch /data/helo这样就将GlusterFS上的一个名为datastore的卷挂载到了你启动的容器中，这样即使你在另一台机器上再次启动容，数据也不会丢失\n第二种：容器内部挂载方式# 这次我们用容器的方式安装GlusterFS服务，这样就容易多了\n[user@ser0 ~]$ docker run -tid --name glusterfs -h glusterfs \\ --network overlay --privileged=true -v /home/data/src:/data \\ paulczar/glusterfs glusterd --pid-file=/app/gluster.pid --log-file=- --no-daemon [user@ser0 ~]$ docker exec -ti glusterfs bash # 进入 glusterfs 容器 root@glusterfs:/app# gluster volume create mydata glusterfs:/data force # 创建一个名为mydata的卷 root@glusterfs:/app# gluster volume start mydata # 启动卷 root@glusterfs:/app# gluster volume info # 查看一下所有卷的情况 Volume Name: mydata Type: Distribute Volume ID: 24b67e3c-c343-4b07-b65c-5b9481c396b7 Status: Started Number of Bricks: 1 Transport-type: tcp Bricks: Brick1: glusterfs:/data/mydata参数说明：\n–network overlay.com 这里指定了一个自定义网络，可以让不同物理机上的容器互相通信，如果你还不知道怎样实现，请看这篇教程：Docker 多主机通信之ZK -v /home/data/src:/data 用物理机上的一个目录来存储 GlusterFS 服务上的数据 启动你的容器\n[user@ser0 ~]$ docker run -tid -h centos --network overlay centos:6.8 bash # 容器也要加入 overlay 这个网络 [user@ser0 ~]$ docker exec -ti centos bash [root@centos /]# yum install glusterfs glusterfs-fuse -y # 在容器内安装GlusterFS客户端 [root@centos /]# mount -t glusterfs glusterfs:/your-gluster-volume /mnt 这样就将glusterfs上的卷挂载到了容器内的 /mnt 目录下，容器挂掉后，数据不丢失，再次启动容器，再挂载即可\n参考资料# go语言下载与安装 Docker插件-GlusterFS安装 Gluster在物理机上的安装与使用 容器版的GlusterFS的下载与使用 关于# 作者：张佳军\n阅读：195\n点赞：0\n创建：2017-01-15\n"},{"id":60,"href":"/kubernetes/k8s-calico.html","title":"calico网络","section":"容器开发","content":"calico网络# 什么是Calico# Calico是一个为k8s而设计的基于路由的容器网络解决方案，它的核心思想是把Linux节点当做路由器，用边界网关协议（BGP）将当前节点的路由信息通知给其它节点，来完成各个节点上的路由自动配置。\n有关BGP的作用和原理请看另一篇文章，白话BGP协议。\nCalico的组成# Calico通常以Daemonset的方式安装到k8s集群中的每个子节点上，Pod的默认名字为calico-node，Pod中包含几个组件：\nBGP Client：BGP客户端，负责将本机的路由信息通知给其它所有节点，同时接收其它节点发来的信息，端口：179。BGP Client引用了一个开源的BGP项目，你可以在本文最后找到项目地址 calico-felix：负表将将其它节点发来的路由信息设置到物理机上 BGP Route Reflector：另一个BGP客户端，在集群节点较多时启用此模式，比BGP Client节省资源 工作原理# Calico启动后，所有节点上的BGP客户端将自己所在节点的主机名、IP地址、AS号等信息注册到ETCD中，这样所有节点的BGP就可以通过ETCD中的信息找到其它BGP，这时每个BGP定期将自已节点上的路由信息通知给其它所有BGP，其它节点收到信息后进行分析和筛选，将有用的信息通过calico-felix设置到本地Linux路由表中。\n通过以下命令查看BGP注册在ETCD中的信息：\ncurl -s localhost:2379/v2/keys/calico/bgp/v1/host | jq数据流程# 有A和B两台机器，每个机器上各有一个容器X和Y，第个容器中的虚拟网卡eth0分别对应了物理机上的一个虚拟网卡，如下图中所示，容器X中的eth0网卡对应了物理机上的103，这种成对出现的设备叫做veth设备，是Linux系统专门为Namespace（也就是容器技术）设计的，当有数据从一端进入以后会被自动转发到另一端，这样就可以在容器和物理之间方便地交换数据，当我们在k8s中创建一个Pod时，kubelet组件会调用CNI插件来为Pod添加虚拟网卡，veth设备就是在这个时候被创建的。\n当X访问Y时，报文先由容器中的eth0@if103到达物理机上的103: cali103，然后物理机上会有一条路由（通过命令ip route查看），告诉系统将目标为2.2.0.0/16这个段的数据包全部转发到5.5.5.3，数据到了B以到，机器B上会有一条路由把目标为2.2.2.2的数据包转发到203网卡上，203同样是一个veth设备，它直接转发给容器中的eth0@if203网卡，A和B上的两条路由就是由Calico来设置的。\n参考与引用# Calico官方文档 白话BGP协议 使用GO语言实现的BGP项目 关于# 作者：sycki\n阅读：239\n点赞：2\n创建：2018-07-21\n"},{"id":61,"href":"/kubernetes/k8s-cni.html","title":"k8s CNI","section":"容器开发","content":"k8s CNI# CNI（container network interface）# 为了将网络功能插件化，k8s要1.5提出了CNI标准，现在我们常用的网络如：flannel、calico等都是基于CNI标准开发的。\nkubelet配置CNI# --network-plugin=cni --cni-bin-dir=/opt/cni/bin --cni-conf-dir=/etc/cni/net.d /opt/cni/bin：CNI插件的存放目录 /etc/cni/net.d：插件的配置文件的存放目录 接口定义# vendor/github.com/containernetworking/cni/libcni/api.go\ntype CNI interface { AddNetworkList(net *NetworkConfigList, rt *RuntimeConf) (types.Result, error) DelNetworkList(net *NetworkConfigList, rt *RuntimeConf) error AddNetwork(net *NetworkConfig, rt *RuntimeConf) (types.Result, error) DelNetwork(net *NetworkConfig, rt *RuntimeConf) error }接口实现# 下面以calico-plugin为例说明CNI的实现，calico-plugin与calico容器网络之间没有直接关系，我们通常说的calico指的是这两个东西的组合，而calico容器网络是解决不同物理机上容器之间的通信，而calico-plugin是在k8s创建Pod时为Pod设置虚拟网卡，也就是容器中的eth0和lo网卡，calico-plugin是由两个静态的二进制文件组成，由kubelet以命令行的形式调用，这两个二进制的作用如下：\ncalico-ipam：分配IP，维护IP池，需要依赖etcd。 calico：通过调用系统API来修改namespace中的网卡信息。 文件在物理机上的位置：\n/opt/cni/ ├── bin │ ├── calico │ ├── calico-ipam └── net.d └── 10-calico.confcalico-plugin在ETCD中维护的信息：\ncurl -s localhost:2379/v2/keys/calico/ipam/v2/assignment/ipv4/block | jqcalico插件配置\nvim /opt/cni/net.d/10-calico.conf { \u0026#34;name\u0026#34;: \u0026#34;calico-k8s-network\u0026#34;, \u0026#34;cniVersion\u0026#34;: \u0026#34;0.1.0\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;calico\u0026#34;, \u0026#34;etcd_endpoints\u0026#34;: \u0026#34;http://127.0.0.1:2379\u0026#34;, \u0026#34;log_level\u0026#34;: \u0026#34;info\u0026#34;, \u0026#34;ipam\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;calico-ipam\u0026#34; }, \u0026#34;kubernetes\u0026#34;: { \u0026#34;kubeconfig\u0026#34;: \u0026#34;/opt/rainbond/kubernetes/kubecfg/admin.kubeconfig\u0026#34; } }calico-plugin工作原理# kubelet在创建一个Pod时，会先启动puase容器，然后为这个容器添加设置网络，也就是添加网卡，这里会通过CNI调起文件系统中的/opt/cni/bin/calico，并将Pod信息通过标准输入(stdin)传递给calico进程，calico通过修改系统中Namespace达到为容器添加网卡的目的。\ncalico-plugin源码分析# /opt/cni/bin/calico的入口函数定义在calico.go文件中，这个文件中有三个主要函数：\nmain()：入口函数，主要负责从进程的标准输入(stdin)中读取kubelet传进来的json数据，然后调用cmdAdd()或是cmdDel()。 cmdAdd()：为Pod添加网卡 cmdDel()：为Pod删除网卡 我们看cmdAdd()函数的实现，cmdAdd()是个通用函数，它会根据wepIDs.Orchestrator字段的值调用calico针对k8s的实现：calico.go:190\nif wepIDs.Orchestrator == api.OrchestratorKubernetes { if result, err = k8s.CmdAddK8s(ctx, args, conf, *wepIDs, calicoClient, endpoint); err != nil { return err }其中k8s.CmdAddK8s()函数的主要工作如下：\n调用/opt/cni/bin/calico-ipam分配一个IP：k8s/k8s.go:171\nipamResult, err := ipam.ExecAdd(conf.IPAM.Type, args.StdinData) 创建endpoint：k8s/k8s.go:243\nendpoint.Name = epIDs.WEPName endpoint.Namespace = epIDs.Namespace endpoint.Labels = labels endpoint.GenerateName = generateName endpoint.Spec.Endpoint = epIDs.Endpoint endpoint.Spec.Node = epIDs.Node endpoint.Spec.Orchestrator = epIDs.Orchestrator endpoint.Spec.Pod = epIDs.Pod endpoint.Spec.Ports = ports endpoint.Spec.IPNetworks = []string{} 创建一对veth设备，在Linux中，veth设备是两张虚拟网卡，一个在Linux的物机机上，一个在容器中，用于容器与物理机之间的通信：k8s/k8s.go:280\n_, contVethMac, err := utils.DoNetworking(args, conf, result, logger, hostVethName) 将endpoint信息写入到Pod中：k8s/k8s.go:299\nif _, err := utils.CreateOrUpdate(ctx, calicoClient, endpoint); err != nil { 将网卡信息写入到容器对应的Namespace中：utils/network.go:41\nerr = ns.WithNetNSPath(args.Netns, func(hostNS ns.NetNS) error { veth := \u0026amp;netlink.Veth{ LinkAttrs: netlink.LinkAttrs{ Name: contVethName, Flags: net.FlagUp, MTU: conf.MTU, }, PeerName: hostVethName, } 将endpoint信息写入ETCD：k8s/k8s.go:229\nif _, err := utils.CreateOrUpdate(ctx, calicoClient, endpoint); err != nil { logger.WithError(err).Error(\u0026#34;Error creating/updating endpoint in datastore.\u0026#34;) releaseIPAM() return nil, err } 写入ETCD的逻辑引用了另一个项目：github.com/projectcalico/libcalico-go\n相关资料# calico-plugin项目 kubelet源码分析 k8s创建Pod的流程 关于# 作者：sycki\n阅读：127\n点赞：4\n创建：2018-07-21\n"},{"id":62,"href":"/kubernetes/k8s-code-apiserver-start.html","title":"k8s源码分析-apiserver","section":"容器开发","content":"k8s源码分析-apiserver# 本系列文章是基于kubernetes1.7版本的。\nmain函数# apiserver的入口定义在cmd/kube-apiserver/apiserver.go文件中：\nfunc main() { rand.Seed(time.Now().UTC().UnixNano()) // 创建一个默认配置对象 s := options.NewServerRunOptions() // 构建命令行对象 s.AddFlags(pflag.CommandLine) // 解析命令行参数 flag.InitFlags() logs.InitLogs() defer logs.FlushLogs() // 如果指定了-version选项，则打印版本号然后退出 verflag.PrintAndExitIfRequested() // 启动服务，并从管道中监听停止信号，该通道可能永远不会写入数据 if err := app.Run(s, wait.NeverStop); err != nil { fmt.Fprintf(os.Stderr, \u0026#34;%v\\n\u0026#34;, err) os.Exit(1) } }这个main函数看起来是比较直观的，中文的注释是我加上去的，便于阅读。\n初始化配置# 在main函数中的第一行是为随机数产生器用当前时间提供了一个基数，避免多次启动产生相同的值，说明在这个组件中可能有些功能依赖了随数机。\ns := options.NewServerRunOptions()显然是在创建一个配置对象，不过这个对象中只包含一些默认参数。\ns.AddFlags(pflag.CommandLine)是构建一个命令行对象，怎么构建的呢，其实就是s在pflag.CommandLine这个对象中加了很多选项，并把自己的在很多字段以指针的方式注入到了pflag.CommandLine中，这些选项是经过分组的，如ETCD相关配置、apiserver相关的请求超时时间等。\n紧接着flag.InitFlags()解析所有命令行参数，并通过指针把值放到配置对象s中。\n其中flag这个包是k8s对开源项目github.com/spf13/pflag的封装，而这个项目又是基于golang标准库中的flag包开发的，并且封装了一些实用的函数，可以让你快速生成自己的命令行选项。\n准备资源# 然后调用到了Run()函数，定义在cmd/kube-apiserver/app/server.go文件中，这里代码较多就不全部贴出来了，为了保持文章的可读性，我会尽量减少函数展开的层数。\nnodeTunneler, proxyTransport, err := CreateNodeDialer(runOptions)上面代码判断是否安装在云主机上，如果是，并且指定了密钥文件，也是就--ssh-keyfile选项，则安装key到所有云主机的实例中，然后使用该密钥文件创建一个连接器nodeTunneler，用来访问其它节点。\nkubeAPIServerConfig, sharedInformers, versionedInformers, insecureServingOptions, serviceResolver, err := CreateKubeAPIServerConfig(runOptions, nodeTunneler, proxyTransport)这句是利用最初的配置对象runOptions创建用于启动apiserver的配置对象，与runOptions不同的是，它还包括了启动apiserver所需的资源。\n该函数中做了以下几件事：\n根据用户指定的--admission-control列表加载相应的admission插件，这些插件的作用是用来过滤API请求的，可以用来作请求的合法性检测，比如我们对所有get csr的请求进行用户验证，如果该用户权限太低则拒绝该请求；也可以修改某个请求，比如为所有create deployments的请求加上scale=1等等。默认不加载任何规则。 如果有必要的配置没有设置则为其设置默认值，如Service IP段、序列化缓存大小等。 验证必要配置项，如果在此阶段有错误配置项，则所有错误信息将被打包返回。 如果验证通过则开始连接ETCD，并启动同步信息的进程，该进程会一直监听apiserver的数据并同步到ETCD集群中。 最后创建master.Config对象，也就是kubeAPIServerConfig变量，然后返回。 kubeAPIServer, err := CreateKubeAPIServer(kubeAPIServerConfig, apiExtensionsServer.GenericAPIServer, sharedInformers, apiExtensionsConfig.CRDRESTOptionsGetter)这行代码是比较核心的，它的作用如下：\n首先对kubeAPIServerConfig做一个配置信息的检查，如果有空缺的字段则为其补全。 安装以/api/v1开头的的REST API，安装API的函数是pkg/master/master.go文件内的InstallLegacyAPI()函数，而把RUI和Handler对应起来的是pkg/registry/core/rest/storage_core.go文件内的NewLegacyRESTStorage()函数，我们贴出来一小段： func (c LegacyRESTStorageProvider) NewLegacyRESTStorage(restOptionsGetter generic.RESTOptionsGetter) (LegacyRESTStorage, genericapiserver.APIGroupInfo, error) { ... restStorageMap := map[string]rest.Storage{ \u0026#34;pods\u0026#34;: podStorage.Pod, \u0026#34;pods/attach\u0026#34;: podStorage.Attach, \u0026#34;pods/status\u0026#34;: podStorage.Status, \u0026#34;pods/log\u0026#34;: podStorage.Log, \u0026#34;pods/exec\u0026#34;: podStorage.Exec, ... \u0026#34;replicationControllers\u0026#34;: controllerStorage.Controller, \u0026#34;replicationControllers/status\u0026#34;: controllerStorage.Status, \u0026#34;services\u0026#34;: serviceRest.Service, \u0026#34;services/proxy\u0026#34;: serviceRest.Proxy, \u0026#34;services/status\u0026#34;: serviceStatusStorage, ...可见在k8s中，把Handler封装成了Storage，其实就是一个Storage对象负责一个REST API的增删改查。 3. 然后把最终的启动逻辑作为钩子，注册到kubeAPIServer中，但这时还没有启动，注册钩子的代码如下：\nkubeAPIServer.GenericAPIServer.AddPostStartHook(\u0026#34;start-kube-apiserver-informers\u0026#34;, func(context genericapiserver.PostStartHookContext) error { sharedInformers.Start(context.StopCh) return nil })我们来说一下这个钩子函数，可以看到这里做了一个启动操作，这个sharedInformers是个集合，它包含了多个SharedInformer对象，也说是说这个启动操作其实是启动所有的SharedInformer，这个SharedInformer又是什么呢？它其实是一个缓存器，其它如kube-scheduler、kubelet等组件跟apiserver通信时都是通过缓存间接通信的，而这个SharedInformer一方面负责收集客户端和其它组件发来的请求，一方面负责通知该事件的关注者，使关注者可以做出相应的动作，还有就是它会定其把缓存中的数据同步到ETCD中。\n以下是SharedInformer接口的定义，在shared_informer.go文件中：\ntype SharedInformer interface { AddEventHandler(handler ResourceEventHandler) AddEventHandlerWithResyncPeriod(handler ResourceEventHandler, resyncPeriod time.Duration) GetStore() Store GetController() Controller Run(stopCh \u0026lt;-chan struct{}) HasSynced() bool LastSyncResourceVersion() string }前两个函数都是向该事件添加一个监听器，也就是前面说的关注者，只不过使用第二个函数可以自定义同步数据的周期间隔时间。 GetStore()用来获取该事件类型的处理器，有新事件时该Store对应的逻辑会被调起。 Run()就是启动该SharedInformer的各个进程了。\n启动服务# 此时我们还在app.Run()函数中，在函数的最后才是真正启动Server的逻辑：\naggregatorServer.GenericAPIServer.PrepareRun().Run(stopCh)我们展开句尾的.Run(stopCh)看一下，这正是API服务启动的关键代码，它定义在： kubernetes/vendor/k8s.io/apiserver/pkg/server/serve.go\nfunc (s *GenericAPIServer) serveSecurely(stopCh \u0026lt;-chan struct{}) error { secureServer := \u0026amp;http.Server{ Addr: s.SecureServingInfo.BindAddress, Handler: s.Handler, MaxHeaderBytes: 1 \u0026lt;\u0026lt; 20, TLSConfig: \u0026amp;tls.Config{ NameToCertificate: s.SecureServingInfo.SNICerts, MinVersion: tls.VersionTLS12, NextProtos: []string{\u0026#34;h2\u0026#34;, \u0026#34;http/1.1\u0026#34;}, }, } if s.SecureServingInfo.Cert != nil { secureServer.TLSConfig.Certificates = []tls.Certificate{*s.SecureServingInfo.Cert} } // 中间省略... s.effectiveSecurePort, err = RunServer(secureServer, s.SecureServingInfo.BindNetwork, stopCh) } 函数的开头就是定义一个Server，这在golang中是一个标准的Server. 其中的s.Handler是所有REST API对应的handler，golang中的handler相当于java中的servlet，或者是Spring VMC中的controller. 读取配置中提供的证书、CA、私钥等，注入到Server对象中。 最后一行是启动Server，里面是非阻塞式启动的，逻辑较简单，这里就不再展开了。 调用刚才注册的启动钩子函数，启动各个SharedInformer。 监听stopCh通道并阻塞主进程。 到这里就是apiserver启动的全部流程了，下次细讲一下apiserver的工作流程，以及它如何处理Client发来的创建、更新、删除等请求。 关于# 作者：张佳军\n阅读：188\n点赞：4\n创建：2017-12-23\n"},{"id":63,"href":"/kubernetes/k8s-code-kubelet.html","title":"k8s源码分析-kubelet","section":"容器开发","content":"k8s源码分析-kubelet# 本文以k8s v1.10为例，分析kubelet组件的工作原理。\n入口# main函数定义在cmd/kubelet/kubelet.go，主要任务如下：\n创建命令行对象，包括解析用户指定的参数，生成配置对象等 执行命令行对象，最后进入启动kubelet的逻辑，调用了kubelet的RunKubelet()函数 创建kubelet# 以下是创建kubelet的关键函数，它创建了docker客户端、网络插件等重要组件：pkg/kubelet/kubelet.go:321\nfunc NewMainKubelet(kubeCfg *kubeletconfiginternal.KubeletConfiguration,初始化容器运行时服务端（CRI）# 在1.5以前的版本中，k8s依赖于dokcer，为了支持不同的容器运行时，比如rkt、containerd，kubelet从1.5开始加入了CRI标准，CRI是一组rpc接口，只要是实现了这组接口都可以作为kubelet的运行时，而且在k8s内部将之前的Pod抽象为一种更为通用的SandBox。\n调用过程如下：\nkubelet -\u0026gt; remote -\u0026gt; CRI -\u0026gt; dockershim -\u0026gt; docker_client -\u0026gt; docker_daemon提出了CRI标准以后，意味着在新的版本里需要使用新的连接方式与docker通信，为了兼容以前的版本，k8s提供了针对docker的CRI实现，也就是kubelet包下的dockershim包，dockershim是一个rpc服务，监听一个端口供kubelet连接，dockershim收到kubelet的请求后，将其转化为REST API请求，发送给物理机上的docker daemon，以下是创建和启动dockershim的代码：pkg/kubelet/kubelet.go:NewMainKubelet():622\n// 创建dockershim ds, err := dockershim.NewDockerService(kubeDeps.DockerClientConfig, crOptions.PodSandboxImage, streamingConfig, ... // 启动rpc服务 if err := server.Start(); err != nil {创建Docker客户端的逻辑是在创建dockershim的过程中，关键代码：pkg/kubelet/dockershim/libdocker/client.go:100\nclient, err := getDockerClient(dockerEndpoint)client对象就是docker的客户端，包含了我们常用的docker run，docker images等所有操作，dockerEndpoint就是--container-runtime-endpoint选项的值，默认是unix:///var/run/docker.sock。\n初始化容器运行时客户端# dockershim是rpc的服务端，pkg/kubelet/remote包是rpc客户端的实现，此包下的函数由kubelet组件调用，创建remote的代码：pkg/kubelet/kubelet.go:657\nruntimeService, imageService, err := getRuntimeAndImageServices(remoteRuntimeEndpoint, remoteImageEndpoint, kubeCfg.RuntimeRequestTimeout)初始化网络插件（CNI）# 创建CNI实例的逻辑是在创建dockershim的过程中，用来在容器中创建和删除网络设备，关键代码：pkg/kubelet/dockershim/docker_service.go:NewDockerService():232\ncniPlugins := cni.ProbeNetworkPlugins(pluginSettings.PluginConfDir, pluginSettings.PluginBinDirs)初始化卷管理器# 卷管理器的作用是检查容器需要的卷是否已挂载，需要卸载的卷是否已经卸载，关键代码：pkg/kubelet/kubelet.go:NewMainKubelet():817\nklet.volumePluginMgr, err = NewInitializedVolumePluginMgr(klet, secretManager, configMapManager, tokenManager, kubeDeps.VolumePlugins, kubeDeps.DynamicPluginProber)初始化Pod处理器# 创建一个叫worker的对象，它持有处理Pod的入口函数，也是就klet.syncPod：pkg/kubelet/kubelet.go:NewMainKubelet():854\nklet.podWorkers = newPodWorkers(klet.syncPod, kubeDeps.Recorder, klet.workQueue, klet.resyncInterval, backOffPeriod, klet.podCache)启动kubelet# 启动kubelet的入口函数是：pkg/kubelet/kubelet.go:1370\nfunc (kl *Kubelet) Run(updates \u0026lt;-chan kubetypes.PodUpdate) {启动Pod处理器# pkg/kubelet/kubelet.go:1413\nkl.syncLoop(updates, kl)其中syncLoop()函数是处理Pod事件的入口函数，定义如下：pkg/kubelet/kubelet.go:1781\nfunc (kl *Kubelet) syncLoop(updates \u0026lt;-chan kubetypes.PodUpdate, handler SyncHandler)处理Pod事件# 当从kube-apiserver中监听到需要处理的事件后，将事件交给相应的Pod处理器：pkg/kubelet/kubelet.go:1926\nhandler.HandlePodSyncs(podsToSync)分配一个worker并开始处理事件的函数：pkg/kubelet/kubelet.go:1959\nfunc (kl *Kubelet) dispatchWork(pod *v1.Pod, syncType kubetypes.SyncPodType, mirrorPod *v1.Pod, start time.Time)执行worker中处理Pod事件的函数，也就是前面说到的入口函数：pkg/kubelet/pod_workers.go:170\nerr = p.syncPodFn(syncPodOptions{ mirrorPod: update.MirrorPod, pod: update.Pod, podStatus: status, killPodOptions: update.KillPodOptions, updateType: update.UpdateTy ...同步Pod# 入口函数定义：pkg/kubelet/kubelet.go:1450\nfunc (kl *Kubelet) syncPod(o syncPodOptions) error调用容器运行时的同步Pod接口：pkg/kubelet/kubelet.go:1649\nresult := kl.containerRuntime.SyncPod(pod, apiPodStatus, podStatus, pullSecrets, kl.backOff)在1.11.x版本中该接口只有一个实现：pkg/kubelet/kuberuntime/kuberuntime_manager.go:567\nfunc (m *kubeGenericRuntimeManager) SyncPod(pod *v1.Pod, _ v1.PodStatus, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, backOff *flowcontrol.Backoff) (result kubecontainer.PodSyncResult) {与docker的衔接# 这个SyncPod()函数函数是创建容器的关键逻辑，在这里可以一览创建Pod的全部流程，所有关于容器的操作都是调用remote模块最后到达docker daemon中，比如以下创建容器的代码：pkg/kubelet/kuberuntime/kuberuntime_container.go:SyncPod():118\ncontainerID, err := m.runtimeService.CreateContainer(podSandboxID, containerConfig, podSandboxConfig)其中runtimeService字段就是在启动kubelet阶段创建的remote对象。\nkubelet创建Pod的过程# 这里列出SyncPod()函数的工作流程，也就是创建Pod的过程。\n检查Pod信息是否有更改，如果是则杀死现有的Pod，包括Pod中的所有容器。\n创建SandBox（沙箱），其实就是启动pause容器，这个过程中包含了一些重要操作：\n创建SandBox的函数：pkg/kubelet/dockershim/docker_sandbox.go:79\nfunc (ds *dockerService) RunPodSandbox(ctx context.Context, r *runtimeapi.RunPodSandboxRequest) (*runtimeapi.RunPodSandboxResponse, error) { 拉取pause容器的镜像。\n启动pause容器。\n写入resolv.conf文件，覆盖docker默认创建的resolv.conf文件：pkg/kubelet/dockershim/docker_sandbox.go:140\ncontainerInfo, err := ds.client.InspectContainer(createResp.ID) 设置Pod网络，也就是调用CNI来为容器添加网卡：pkg/kubelet/dockershim/docker_sandbox.go:163\nerr = ds.network.SetUpPod(config.GetMetadata().Namespace, config.GetMetadata().Name, cID, config.Annotations) 启动init容器。\n最后启动所有业务容器：pkg/kubelet/kuberuntime/kuberuntime_manager.go:712\nfor _, idx := range podContainerChanges.ContainersToStart { if msg, err := m.startContainer(podSandboxID, podSandboxConfig, container, pod, podStatus, pullSecrets, podIP, kubecontainer.ContainerTypeRegular); err != nil { ... } 至此Pod就算创建完成了，如果创建了相应的service，那么proxy组件会为service在物理机上通过iptables工具创建数据转发规则，将集群内的请求Pod的数据转发到相应的Pod中，如果Pod有多个副本，则以负载均衡的方式分别转发给这个Pod。\n关于# 作者：sycki\n阅读：293\n点赞：4\n创建：2018-03-06\n"},{"id":64,"href":"/kubernetes/k8s-code-pod-create.html","title":"k8s源码分析-创建Pod流程","section":"容器开发","content":"k8s源码分析-创建Pod流程# 本文从源码层面解释kubernetes从接收创建Pod的指令到实际创建Pod的整个过程。\n1.1 监听用户请求# 监听的任务是由kube-apiserver这个组件来完成的，它实际上是一个WEB服务，一般是以双向TLS认证方式启动的，所以在启动时需要提供证书、私钥、客户端的CA证书和CA私钥，当然也支持HTTP的方式，启动后就开始监听用户请求了。\n1.2 对请求分类# kube-apiserver中的WEB服务在启动时注册了很多的Handler，golang中的Handler相当于Java中的servlet或者是Spring中的Controller，是对某一业务逻辑的封装，通俗点说，一个Handler负责对一个URI请求的处理，而在kube-apiserver中，Handler被封装成了一个叫Store的对象，怎么封装的呢？比如/api/v1/namespaces/{namespace}/pods这个URI对应了一个叫PodStorage的Store，这个Store中包含了对/api/v1/namespaces/{namespace}/pods的多个Handler，这些Handler有的是处理创建请求，有的是处理删除请求等等，代表了对一种资源的操作集。\n我们来看看这个Store的定义： staging/src/k8s.io/apiserver/pkg/registry/generic/registry/store.go\ntype Store struct { CreateStrategy rest.RESTCreateStrategy AfterCreate ObjectFunc UpdateStrategy rest.RESTUpdateStrategy AfterUpdate ObjectFunc DeleteStrategy rest.RESTDeleteStrategy AfterDelete ObjectFunc ... } func (e *Store) Create(ctx genericapirequest.Context, obj runtime.Object, includeUninitialized bool) (runtime.Object, error) { func (e *Store) Update(ctx genericapirequest.Context, name string, objInfo rest.UpdatedObjectInfo) (runtime.Object, bool, error) { ...因为每种资源所需要的操作不一样，所以Store中只包含了基本的通用的操作，作为一个基础类。\n1.3 处理请求# 在Store的定义中有一个Storage storage.Interface字段，Store中的创建、更新、删除等操作，比如上面的Create()函数中会调用这个对象中的Create()方法，也就是说这个Storage对象包含了一组更低级的操作，可以看作是数据的持久化层，这些操作都是通用的，而Store可以用Storage中的功能组合出具有不同功能的控制层对象，也就是Store对象啊，，好吧我们距离真相又进了一步，那这个Storage storage.Interface对象又是怎样实现的呢？\n1.4 数据存储到ETCD# Storage字段是一个叫Interface的类型，里面定义了一些数据持久层的操作，这里就不贴出来了，我们更关心它的实现，我们先来看看Interface实例的创建吧，它的创 建工作是由一个工厂类负责的： staging/src/k8s.io/apiserver/pkg/storage/storagebackend/factory/factory.go\nfunc Create(c storagebackend.Config) (storage.Interface, DestroyFunc, error) { switch c.Type { case storagebackend.StorageTypeETCD2: return newETCD2Storage(c) case storagebackend.StorageTypeUnset, storagebackend.StorageTypeETCD3: return newETCD3Storage(c) default: return nil, nil, fmt.Errorf(\u0026#34;unknown storage type: %s\u0026#34;, c.Type) } }好吧，看到这里就彻底明白了，根据配置中指定的存储服务创建不同的Storage对象，并且目前只支持ETCD2和ETCD3两种存储服务，持久层所做的增删改查就是对ETCD中 数据的增删改查，总结一下Storage对象也是就Interface被创建的过程：\n根据配置中提供的存储服务名称选择进入相应的创建函数，也就是上面的逻辑。 从配置对象中取出我们启动kube-apiserver时指定的ETCD的证书证书、私钥和CA证书（如果开启了双向证），用于apiserver和ETCD通信，用这些信息生成一个配置对象。 用配置对象创建一个ETCD的客户端。 创建一个k8s.io/apiserver/pkg/storage/etcd3.store对象并让其持有这个ETCD客户端 将这个对象作为Interface返回。 1.5 调用ETCD客户端# 那么这个k8s.io/apiserver/pkg/storage/etcd3.store对象就成了关键啦，因为它现了Interface的所有接口函数，我们看看它的Create()函数据是怎样实现的： vendor/k8s.io/apiserver/pkg/storage/etcd3/store.go\n// Create implements storage.Interface.Create. func (s *store) Create(ctx context.Context, key string, obj, out runtime.Object, ttl uint64) error { if version, err := s.versioner.ObjectResourceVersion(obj); err == nil \u0026amp;\u0026amp; version != 0 { return errors.New(\u0026#34;resourceVersion should not be set on objects to be created\u0026#34;) } data, err := runtime.Encode(s.codec, obj) if err != nil { return err } key = path.Join(s.pathPrefix, key) opts, err := s.ttlOpts(ctx, int64(ttl)) if err != nil { return err } newData, err := s.transformer.TransformToStorage(data, authenticatedDataString(key)) if err != nil { return storage.NewInternalError(err.Error()) } txnResp, err := s.client.KV.Txn(ctx).If( notFound(key), ).Then( clientv3.OpPut(key, string(newData), opts...), ).Commit() if err != nil { return err } if !txnResp.Succeeded { return storage.NewKeyExistsError(key, 0) } if out != nil { putResp := txnResp.Responses[0].GetResponsePut() return decode(s.codec, s.versioner, data, out, putResp.Header.Revision) } return nil }其中txnResp, err := s.client.KV.Txn(ctx).If(这一行就是调用ETCD客户端了。\n2.1 调度Pod# 调度工作是由kube-scheduler负责的，而且不受kube-apiserver控制，kube-scheduler通过kube-apiserver的REST API不断地检查是否有新的且还没有被调度的Pod，如果如有则根据配置中的调度算法为其绑定到某个节点，绑定的过程也是通过REST API将信息写入到kube-apiserver中的，对应的REST API定义如下： pkg/registry/core/rest/storage_core.go\nfunc (c LegacyRESTStorageProvider) NewLegacyRESTStorage(restOptionsGetter generic.RESTOptionsGetter) (LegacyRESTStorage, genericapiserver.APIGroupInfo, error) { ... restStorageMap := map[string]rest.Storage{ ... \u0026#34;pods/binding\u0026#34;: podStorage.Binding, \u0026#34;bindings\u0026#34;: podStorage.Binding, ...\u0026quot;pods/binding\u0026quot;和\u0026quot;bindings\u0026quot;两个API就是了。\n下面是REST API/api/v1/namespaces/{my-ns}/pods对应的Store对象的定义： pkg/registry/core/pod/storage/storage.go\nfunc NewStorage(optsGetter generic.RESTOptionsGetter, k client.ConnectionInfoGetter, proxyTransport http.RoundTripper, podDisruptionBudgetClient policyclient.PodDisruptionBudgetsGetter) PodStorage { ... return PodStorage{ Pod: \u0026amp;REST{store, proxyTransport}, Binding: \u0026amp;BindingREST{store: store},其中的Binding字段就是关于绑定逻辑的Store了。\n3.1 创建Pod# 最后的创建工作由各个节点上的kubelet组件负责，工作原理同kube-scheduler一样，通过REST API从kube-apiserver循环监听是否有新创建的并且已经被绑定到自己节点的Pod，如果有则在自己的节点上创建相应的Docker容器并设置容器的网络、端口转发、内存和CPU限制等，然后把结果用REST API通知给kube-apiserver。\nkubelet创建Pod的过程，主要逻辑在k8s源码的pkg/kubelet/kuberuntime/kuberuntime_manager.go文件的SyncPod()函数中：\n检查Pod信息是否有更改，如果是则杀死现有的Pod，包括Pod中的所有容器。\n创建SandBox（沙箱），其实就是启动pause容器，这个过程中包含了一些重要操作：\n创建SandBox的函数：pkg/kubelet/dockershim/docker_sandbox.go:79\nfunc (ds *dockerService) RunPodSandbox(ctx context.Context, r *runtimeapi.RunPodSandboxRequest) (*runtimeapi.RunPodSandboxResponse, error) { 拉取pause容器的镜像。\n启动pause容器。\n写入resolv.conf文件，覆盖docker默认创建的resolv.conf文件：pkg/kubelet/dockershim/docker_sandbox.go:140\ncontainerInfo, err := ds.client.InspectContainer(createResp.ID) 设置Pod网络，也就是调用CNI来为容器添加网卡：pkg/kubelet/dockershim/docker_sandbox.go:163\nerr = ds.network.SetUpPod(config.GetMetadata().Namespace, config.GetMetadata().Name, cID, config.Annotations) 启动init容器。\n最后启动所有业务容器：pkg/kubelet/kuberuntime/kuberuntime_manager.go:712\nfor _, idx := range podContainerChanges.ContainersToStart { if msg, err := m.startContainer(podSandboxID, podSandboxConfig, container, pod, podStatus, pullSecrets, podIP, kubecontainer.ContainerTypeRegular); err != nil { ... } 至此Pod就算创建完成了，如果创建了相应的service，那么proxy组件会为service在物理机上通过iptables工具创建数据转发规则，将集群内的请求Pod的数据转发到相应的Pod中，如果Pod有多个副本，则以负载均衡的方式分别转发给这个Pod。\n相关资料# kubelet源码分析 关于# 作者：张佳军\n阅读：486\n点赞：5\n创建：2017-12-30\n"},{"id":65,"href":"/kubernetes/k8s-code-scheduler-start.html","title":"k8s源码分析-scheduler","section":"容器开发","content":"k8s源码分析-scheduler# 本文以kubenetes v1.7为例，说明kube-scheduler组件的启动流程与工作原理。\n入口# scheduler的main函数定义在plugin/cmd/kube-scheduler/scheduler.go中，main函数代码也比较清晰：\n创建默认配置对象 将配置对象的指针传给命令行解析器，然后命令行解析器把解析到的各选项的值写入到配置对象中 如果用户指定了version选项则打印版本信息并退出 将配置对象传给Run()函数，然后就开始启动Scheduler了 创建客户端# kubeClient, leaderElectionClient, err := createClients(s)Run()函数一开始先创建了一个kubernetes的客户端，用来连接kube-apiserver组件以获取集群信息，这个客户端对象会被包含在Scheduler对象中。\n创建缓存更新器# informerFactory := informers.NewSharedInformerFactory(kubeClient, 0) podInformer := factory.NewPodInformer(kubeClient, 0)然后又根据客户端创建出来两个Informer对象，它跟客户端在一个包中，其作用是允许用户提供一些事件监听器（watcher），然后它有一个Run方法，启动以后会一直循环从kube-apiserver中查询我们想要的信息，比如节点状态、新增Pod等等，如果有变化就会触发我们注册的相应的监听器对应的动作，然后本地有一个缓存对象，用来存放这些查询到的信息，这时只是创建，它们的Run方法还没有被调用。\n创建Scheduler# sched, err := CreateScheduler(Scheduler对象的创建与另外两个对象密切相关，一个是Config，它与Scheduler定义在一个文件中：plugin/pkg/scheduler/scheduler.go，另一个是ConfigFactory，定义在plugin/pkg/scheduler/factory/factory.go，它们的关系大概为：\nConfigFactory的主要工作是维护本地已缓存调度资源，比如等待调度的Pod、已调度的Pod、集群节点列表、PV/PVC列表等，并由Informer循环地从apiserver中把资源更新到本地，当然还包括向队列增删改查的函数，这些函数由Informer提供。\nScheduler是对ConfigFactory的高级抽象，相对包含的函数少一些，因为它封装出了更高级的功能，使用起来更简单。\n而Config是Scheduler中的一个字段，Config没有函数只有一些字段，主要的作用是包含了Scheduler运行时需要的资源，这个Config对象包含的元素如下：\ntype Config struct { SchedulerCache schedulercache.Cache Ecache *core.EquivalenceCache NodeLister algorithm.NodeLister Algorithm algorithm.ScheduleAlgorithm Binder Binder PodConditionUpdater PodConditionUpdater PodPreemptor PodPreemptor NextPod func() *v1.Pod WaitForCacheSync func() bool Error func(*v1.Pod, error) Recorder record.EventRecorder StopEverything chan struct{} } 其中Algorithm字段自然就是调度算法了，NodeLister字段表示集群中所有节点的列表，Binder用来将指定Pod绑定到某一主机上。\n默认调度算法# 默认的调度算法对象在pkg/scheduler/factory/factory.go:CreateFromKeys()函数中被创建：\nalgo := core.NewGenericScheduler( c.schedulerCache, c.equivalencePodCache, c.podQueue, predicateFuncs, predicateMetaProducer, priorityConfigs, priorityMetaProducer, extenders, c.volumeBinder, c.pVCLister, c.alwaysCheckAllPredicates, c.disablePreemption, )所以调度一个Pod的具体实现就在这个genericScheduler结构体中定义了，关键函数是它的Schedule()函数了：\n// Schedule tries to schedule the given pod to one of the nodes in the node list. // If it succeeds, it will return the name of the node. // If it fails, it will return a FitError error with reasons. func (g *genericScheduler) Schedule(pod *v1.Pod, nodeLister algorithm.NodeLister) (string, error) { ...Schedule()函数完成的工作如下：\n先过滤掉不合法的Node，如果过滤后只剩一个Node，就把Pod调度到该Node并结束本次调度工作：\nfilteredNodes, failedPredicateMap, err := g.findNodesThatFit(pod, nodes) 通过该算法对象中指定的打分函数，并行地为每个节点打分。不管是指定的哪个打分函数，这些打分函数预期设置为0-10，0是最低优先级得分（最不喜欢的节点），10是最高，每个优先级函数也可以有 将自身权重，优先级函数返回的节点得分乘以权重得到加权得分，最后将所有得分合并（加）得到所有节点的总加权得分：\npriorityList, err := PrioritizeNodes(pod, g.cachedNodeInfoMap, metaPrioritiesInterface, g.prioritizers, filteredNodes, g.extenders) 打分过程比较复杂，上面函数中的g.prioritizers字段是一个函数集，这些函数提供了不同的规则为Node打分，它们是在集群启动时被注删到factory对象中，而genericScheduler对象在创建时引用了这些函数，以下是注册所有默认算法的代码：\npkg/scheduler/algorithmprovider/defaults/defaults.go\nregisterAlgorithmProvider(defaultPredicates(), defaultPriorities()) 为所有Node打分后得到一个以分数排序的节点列表，如果有多个第一名，就从这些第一名中以循环方式选择一个节点：\nreturn g.selectHost(priorityList) 调度器中几个重要的对象的创建大概就这么多，后面就开始启动了。\n启动Metrics服务# go startHTTP(s)Metrics是一个用于查询调试信息和运行状态信息的REST API Server，它定义如下：\nfunc startHTTP(s *options.SchedulerServer) { mux := http.NewServeMux() healthz.InstallHandler(mux) if s.EnableProfiling { mux.HandleFunc(\u0026#34;/debug/pprof/\u0026#34;, pprof.Index) mux.HandleFunc(\u0026#34;/debug/pprof/profile\u0026#34;, pprof.Profile) mux.HandleFunc(\u0026#34;/debug/pprof/symbol\u0026#34;, pprof.Symbol) mux.HandleFunc(\u0026#34;/debug/pprof/trace\u0026#34;, pprof.Trace) ... mux.Handle(\u0026#34;/metrics\u0026#34;, prometheus.Handler()) ...服务的默认端口是10251，所以我们可以这样查看Scheduler的运行时信息：\ncurl -i localhost:10251/metrics启动缓存更新器# go podInformer.Informer().Run(stop) informerFactory.Start(stop)启动Scheduler# sched.Run()终于看到启动Schduler的代码了，看看它是怎么启动的： plugin/pkg/scheduler/scheduler.go\nfunc (sched *Scheduler) Run() { if !sched.config.WaitForCacheSync() { return } go wait.Until(sched.scheduleOne, 0, sched.config.StopEverything) }调度Pod# Scheduler启动后，Scheduler对象的scheduleOne()函数会被循环调用，每调用一次就完成一次调度Pod的操作：\n执行sched.config.NextPod()获取下一个需要被调度的Pod，sched.config就是我们前面说到的Config对象，这时候就它发挥作用的时候了。\n通过启动时指定的调度算法（如果不指定，则默认使用我们上面讲过的算法）得出该Pod应该被调度到哪节点上，关键代码：\nhost, err := sched.config.Algorithm.Schedule(pod, sched.config.NodeLister) 绑定Pod和主机\nerr := sched.bind(assumedPod, \u0026amp;v1.Binding{ ObjectMeta: metav1.ObjectMeta{Namespace: assumedPod.Namespace, Name: assumedPod.Name, UID: assumedPod.UID}, Target: v1.ObjectReference{ Kind: \u0026#34;Node\u0026#34;, Name: suggestedHost, }, }) 将绑定信息写回到kube-apiserver，这一步是在上面的sched.bind()这个函数上完成的，关键代码是err := sched.config.Binder.Bind(b)，Bind()是一个接口函数，它的实现在ConfigFactory对象中，定义如下：\n// Bind just does a POST binding RPC. func (b *binder) Bind(binding *v1.Binding) error { glog.V(3).Infof(\u0026#34;Attempting to bind %v to %v\u0026#34;, binding.Name, binding.Target.Name) return b.Client.CoreV1().Pods(binding.Namespace).Bind(binding) } 最后将本次的操作信息写入到Metrics服务中。\n总结# 从调度器的实现上来看，它与apiserver之间的解耦合做的非常彻底，完全没有依赖，我们甚至可以通过REST API手动来完成Pod与主机的绑定。文中提到客户端是一个单独的项目在Github上，你可以把它引用在自己的项目中来完成与k8s集群的交互，当然可以模仿调度器用Informer的方式实现更高级的功能。\n以上就是Scheduler组件启动的全总流程，希望你读完本文后对Scheduler有一个比较清晰的认识，以更好的使用你的k8s集群。 关于# 作者：sycki\n阅读：72\n点赞：0\n创建：2018-01-14\n"},{"id":66,"href":"/kubernetes/k8s-cri.html","title":"k8s CRI","section":"容器开发","content":"k8s CRI# CRI（container runtime interface）# 在1.5以前的版本中，k8s依赖于dokcer，为了与docker解耦并支持更多的容器运行时，比如rkt、containerd，kubelet从1.5开始加入了CRI，作为k8s和容器运行时通信的标准，CRI是一组rpc接口，也就是说只要是实现了这组接口都可以作为kubelet的运行时，另外在k8s内部将之前的Pod抽象为一种更为通用的SandBox。\n接口定义# v1.7 pkg/kubelet/apis/cri/v1alpha1/runtime v1.11 pkg/kubelet/apis/cri/runtime/v1alpha2 接口实现# 提出了CRI标准以后，意味着在新的版本里需要使用新的连接方式与docker通信，也就是说docker端需要按CRI的标准实现一个rpc的服务端，所以为了兼容以前的版本，不改变用户习惯，k8s提供了针对docker的CRI实现，也就是k8s源码中kubelet包下的dockershim包，dockershim是一个rpc服务，监听一个端口供kubelet连接，dockershim收到kubelet的请求后，将其转化为REST API请求，发送给物理机上的docker daemon，以下是创建和启动dockershim的代码（k8s v1.11版本）：\npkg/kubelet/kubelet.go:NewMainKubelet():617\n// 创建dockershim ds, err := dockershim.NewDockerService(kubeDeps.DockerClientConfig, crOptions.PodSandboxImage, streamingConfig, ... // 启动rpc服务 if err := server.Start(); err != nil {创建Docker客户端的逻辑是在创建dockershim的过程中，关键代码：\npkg/kubelet/dockershim/libdocker/client.go:100：\nclient, err := getDockerClient(dockerEndpoint)client对象就是docker的客户端，包含了我们常用的docker run，docker images等所有操作，其中dockerEndpoint变量就是kubelet启动参数--container-runtime-endpoint选项的值，默认是unix:///var/run/docker.sock。\nk8s调用容器运行时过程如下：\nkubelet -\u0026gt; remote -\u0026gt; CRI -\u0026gt; dockershim -\u0026gt; docker_client -\u0026gt; docker_daemon在Github上阅读完整的源码：\nCRI客户端实现（remote包）： pkg/kubelet/remote，与容器相关的逻辑主要在remote_runtime.go文件中。 CRI服务端实现（dockershim包）： pkg/kubelet/dockershim，与容器相关的逻辑主要在docker_container.go文件中。 dockershim调用Docker的逻辑： pkg/kubelet/dockershim/libdocker kubelet相关参数# //指定资源管理驱动 --runtime-cgroups cgroupfs //指定容器运行时 --container-runtime docker //指定docker daemon的地址 --docker-endpoint unix:///var/run/docker.sock //创建dockershim服务，供kubelet连接 --container-runtime-endpoint unix:///var/run/dockershim.sock --image-service-endpoint unix:///var/run/dockershim.sock相关资料# kubelet源码分析 k8s创建Pod的流程 关于# 作者：sycki\n阅读：64\n点赞：2\n创建：2018-07-21\n"},{"id":67,"href":"/kubernetes/k8s-dashboard-with-heapster.html","title":"k8s 仪表盘与性能指标","section":"容器开发","content":"k8s 仪表盘与性能指标# 安装Heapster# 需要注意的地方就是镜像，如果官方的不能下载，可以选择国内的，共三个：\ndocker pull kxdmmr/heapster-influxdb-amd64:v1.3.3 docker pull kxdmmr/heapster-grafana-amd64:v4.4.3 docker pull kxdmmr/heapster-amd64:v1.4.2然后去官方下载三个yaml文件：\nhttps://github.com/kubernetes/heapster/tree/master/deploy/kube-config/influxdb\n还有一个关于RBAC的：\nhttps://github.com/kubernetes/heapster/tree/master/deploy/kube-config/rbac\n修改文件内的镜像地址，然后通过kubectl create一次性创建它们。\n[root@node10 dashboard]# kubectl -n kube-system get pods NAME READY STATUS RESTARTS AGE heapster-7f776d4686-8nfz2 1/1 Running 0 12d monitoring-grafana-64768ccd78-4tgmd 1/1 Running 0 12d monitoring-influxdb-84774b9644-z9m28 1/1 Running 0 12d安装Dashboard# 下载yaml# 下载官方的yaml文件：\nmkdir dashboard \u0026amp;\u0026amp; cd dashboard curl -O https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml修改镜像地址# 如果你不能下载官方的镜像，可以选择国内镜像，将yaml文件内image: k8s.gcr这一行改为image: kxdmmr/kubernetes-dashboard-amd64:v1.8.1\n挂载证书到pod中# 在默认的启动参数中指定了自动生成证书--auto-generate-certificates，但实际并不能正常提供https服务，我们可以指定为自己的证书。\n我们打算将证书以Secret的方法挂载到Pod中，所以先将证书和私钥编码成base64，至于证书和私钥的选择，你可以专门为dashboard生成一对，也可以用已经存在的，这里我用安装集群时的一对密钥作为示例：\nless /etc/kubernetes/ssl/kubernetes.pem | base64 -w 0 less /etc/kubernetes/ssl/kubernetes-key.pem | base64 -w 0然后将得到的两个base64码放入yaml文件中的Secret中，给它增加一个data字段，如下：\napiVersion: v1 kind: Secret metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-certs namespace: kube-system type: Opaque data: certfile: EZ6VVcwZWx0NC94Z.........(省略) keyfile: LRVktLNCS0tLE9Qo=.........(省略)引用证书# 这里需要修改dashboard的启动参数，也就是Deployment部分: Deployment.spec.template.spec.containers.args：\nargs: # - --auto-generate-certificates - --heapster-host=http://heapster.kube-system.svc.cluster.local.:80 - --tls-cert-file=certfile - --tls-key-file=keyfile说明：\n先将--auto-generate-certificates这个选项去掉 指定heapster服务的访问方式，hostname和ip的方式都可以，如果不指定是默认自动获取的，但我在使用的时候发现日志里一直报错，说访问heapster时连接超时。 指定我们自己的证书和私钥，因为这个Secret默认已经被以Volume的方式挂载到了/certs这个位置，所以我们可以直接指定，注意不用加/certs这个前缀。 暴露端口# 将配置文件中的Service改为NodePort类型：\nkind: Service apiVersion: v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-system spec: type: NodePort externalIPs: - \u0026#34;10.100.100.101\u0026#34; ports: - port: 443 targetPort: 8443 selector: k8s-app: kubernetes-dashboard创建# kubectl create -f kubernetes-dashboard.yaml现在你可以通过https://10.100.100.101/来访问你的仪表盘了，如果显示了要求登录的界面直接点skip即可，并且可以看到每个namespace的资源使用情况。\n需要注意的问题# 如果遇到任何问题请先查看相应pod的运行日志，比如dashboard不能启动，应该先用kubectl describe查看apiserver创建pod时发生了什么，然后是通过kubectl log查看dashboard的启动日志，它可以看到dashboard所有工作日志。\n关于# 作者：sycki\n阅读：94\n点赞：0\n创建：2018-01-17\n"},{"id":68,"href":"/kubernetes/k8s-expose-service.html","title":"k8s 暴露服务","section":"容器开发","content":"k8s 暴露服务# 在k8s集群中向外暴露服务目前官方支持两种方式：NodePort和Ingress，前者简单粗暴，而后者更高级优雅，那是不是前者就可以被抛弃了？至少目前来看不是的，它们都各有优势和不足，通常情况下需要两者配合使用来满足所有场景，必要时还需要修改自己的业务模块来支持k8s平台，下面我们从使用方法上和优缺点方面细说这两种方案。\nNodePort# 原理# 这种方法其实类似于docker run命令中的-p选项，只不过在Kubernetes中用kube-proxy组件代替了Docker的-p的功能，并且是定义在service中，其原理是通过操作系统的iptables来将物理机上指定端口的数据转发到对应的Pod内。\n使用# 下面是定义一个NodePort类型的Service：\napiVersion: v1 kind: Service metadata: name: spark-driver spec: type: NodePort externalIPs: - \u0026#34;10.100.100.100\u0026#34; ports: - port: 7070 name: master - port: 4040 name: appui selector: app: spark-driver-pod10.100.100.100是k8s集群中某节点的物理IP地址，然后我们就可以通过10.100.100.100:4040访问到app: spark-driver-pod这个Pod的4040端口。\n限制# 很快你就会发现这种方式会有一个问题，当我们在k8s中创建了多个一样的服务，并且都需要访问它们的4040端口和7070端口时，我们需要为每个服务都绑定一个物理IP，而集群中的节点是有限的，很快就没有IP可以用了，这时候有两种变通的方法：\n把上面yaml文件定义中的4040改为其它端口，比如：4041、4042，由于每个服务映射出来的端口不一样，这时需要我们通过其它手段把这些信息记录下来，用户才知道怎样访问自己创建的服务，做这些事是需要工作量的，而且端口也并不是无限的。 第二方法是增加IP，虽然节点是有限的，但如果你的集群使用的IP段是16位的话，还是有很多IP可以用的，方法就是在其中一个k8s子节点上增加子IP，比如几百个、几千个、几万个，，这个方法其实已经在我的前几篇文章中提到过了，具体操作可以参考我的这篇文章。 Ingress# 上面说的两种变通方法其实有点“旁门左道”的味道了，而k8s官方在1.1及以上版本中提供了更优雅的方式来解决这个问题，那就是Ingress，它是以插件的方式存在的，并且默认是没有安装的。\n原理# Ingress是通过在k8s集群中启动一个或多个nginx服务，然后通过k8s的REST API从kube-apiserver中监控Endpoint的变化来动态修改这个nginx的配置文件，将不同的请求转发给相应的Service来完成数据转发，这样说起来可能比较抽象，下面我们将用一个demo来说明它的工作原理。\n安装# Ingress的安装还是比较简单的，官方的安装文档在这里，大概可以分为三步：\n1、下载yaml文件\nmkdir ingress \u0026amp;\u0026amp; cd ingress curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/namespace.yaml curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/default-backend.yaml curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/configmap.yaml curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/tcp-services-configmap.yaml curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/udp-services-configmap.yaml curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/rbac.yaml curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/with-rbac.yaml上面每个命令都涉及一个yaml文件，其中default-backend.yaml和with-rbac.yaml两个文件中涉及到两个在国外的镜像，你可能下载不下来，这时需要改为国内镜像：\nsed -i \u0026#39;s|image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller|image: kxdmmr/nginx-ingress-controller|g\u0026#39; * sed -i \u0026#39;s|image: gcr.io/google_containers/defaultbackend|image: kxdmmr/defaultbackend|g\u0026#39; *2、开始安装\nkubectl create -f namespace.yaml kubectl apply .3、创建服务 有了Nginx后我们得能访问到才行，这样它才能帮我们做数据转发啊，这里我选择把这个Nginx的端口映射到物理机上，这样访问起来方便：\ncat \u0026lt;\u0026lt;EOF \u0026gt; service-nodeport.yaml apiVersion: v1 kind: Service metadata: name: ingress-nginx namespace: ingress-nginx spec: type: NodePort externalIPs: - \u0026#34;10.100.100.100\u0026#34; ports: - name: http port: 80 targetPort: 80 protocol: TCP - name: https port: 443 targetPort: 443 protocol: TCP selector: app: ingress-nginx EOF注意把10.100.100.100换成你k8s集群中某子节点的IP地址，然后创建它：\nkubectl create -f service-nodeport.yaml检查一下吧，如果Pod都启动了就没问题了：\nkubectl -n ingress-nginx get all 使用# 假设我们集群里有两个服务，我们看看怎样通过Ingress来访问到它们，这两个服务如下：\napiVersion: v1 kind: Service metadata: name: spark-driver-1 spec: ports: - port: 4040 name: appui selector: app: spark-driver-pod-1 --- apiVersion: v1 kind: Service metadata: name: spark-driver-2 spec: ports: - port: 4040 name: appui selector: app: spark-driver-pod-2通过路径访问# 通过不同路径访问不同服务，现在我们只能在集群内访问到这两个服务，下面我们创建一个Ingress（也可以分成两个）：\necho \u0026#39; apiVersion: extensions/v1beta1 kind: Ingress metadata: name: ingress-dashboard namespace: kube-system annotations: nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: - http: paths: - path: /service/s1/ backend: serviceName: spark-driver-1 servicePort: 4040 - path: /service/s2/ backend: serviceName: spark-driver-2 servicePort: 4040 \u0026#39; | kubectl create -f -现在我们可以通过10.100.100.100这个IP来访问这两个服务：\ncurl -i 10.100.100.100/service/s1/ curl -i 10.100.100.100/service/s2/这种方式有个弊端，比如上例中的两个Spark服务，它的4040端口其实是一个WEB UI，我们在浏览器中访问10.100.100.100/service/s1/时会发现页面中的js、css等资源加载不出来，因为它页面中的静态资源使用了绝对路径，比如：\n\u0026lt;script src=\u0026#34;/static/table.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;然后浏览器在访问这个资源的时候把它拼接成了这样：\n10.100.100.100/static/table.js这样当然访问不到了，这时候需要把这些资源改为相对路径的方式才能正常访问，比如Dashboard中的资源就是用相对路径的方式，所以用Path方式完全没问题。\n通过域名访问# 更为通用的方式是通过不同域名访问不同服务，这种方式是通过不同的域名访问Ingress的IP（也就是10.100.100.100），Ingress中的Nginx通过请求头中的域将请求转发给不同的后端服务：\necho \u0026#39; apiVersion: extensions/v1beta1 kind: Ingress metadata: name: ingress-dashboard namespace: kube-system spec: rules: - host: s1.bar.com http: paths: - backend: serviceName: spark-driver-1 servicePort: 80 - host: s2.bar.com http: paths: - backend: serviceName: spark-driver-2 servicePort: 80 \u0026#39; | kubectl create -f -以下两个请求将分别访问到spark-driver-1和spark-driver-2：\ncurl -H \u0026#39;host: s1.bar.com\u0026#39; 10.100.100.100 curl -H \u0026#39;host: s2.bar.com\u0026#39; 10.100.100.100为了可以从任意地方都能访问到Nginx，你可能需要将以上的域名*.bar.com泛解析到Nginx的IP，这样就可以通过s1.bar.com、s2.bar.com直接访问到后面的服务。\n小结# 这两种方式可以根据自己的业务需要选其中一种，相信看完本文后你已经心里有数，当然也可以相互配置使用。 关于# 作者：张佳军\n阅读：236\n点赞：2\n创建：2018-01-07\n"},{"id":69,"href":"/kubernetes/k8s-principle.html","title":"k8s 基本原理","section":"容器开发","content":"k8s 基本原理# k8s原理解析# kubernetes由多个模块组成，一般情况下，除去插件和依赖我们需要部署5个组件和一个静态工具，分别为：kube-apiserver、kube-scheduler、kube-controller-manager、kubelet、kube-proxy、kubectl，我们逐一讲解它们的作用及原理。\nkube-apiserver# 一个WEB服务，运行在主节点，它向外提供了很多的REST API，比如：新删改Pod、Service、Deployments，添加节点，提供对REST API的访问权限控制等。\n一般运行一个WEB服务都会依赖一个数据库，用来持久化WEB的状态，apiserver也不例外，它的运行需要依赖一个键值对存储服务，用来存储集群中所有信息，如：Pod状态、Node状态、已创建的Service、已创建的Endpoint，创建的Deployments、Statefulsets等，目前(v1.7.2)它只支持ETCD2、ETCD3两种存储服务，所以在部署apiserver前需要先部署一个ETCD集群，且默认地、它与apiserver之间的通信是TLS协议的，所以在apiserver和ETCD的启动参数中都会指定一对证书和密钥，其它所有组件与apiserver进行通信时也都是TLS方式。\nkube-scheduler# 一个提供Pod调度功能和调度算法的组件，运行在主节点，它的工作原理是不断地从apiserver监听是否有新创建的Pod，或者说已创建但还没有进行调度的Pod，如果有则将该Pod放入本地队列，然后用指定的算法为集群中所有Node进行打分，将Pod绑定到得分最高的Node上，绑定其实就是将Node的Hostname写入到Pod对象对应的字段中，然后将Pod信息写回到apiserver中，至此调度就算完成了。\nscheduler启动时可以指定调度算法，默认的算法名为\u0026quot;default-scheduler\u0026quot;，原理是遍历所有Node，用Node的剩余内存和CPU作为权重，计算出每个Node的分数，如果有多个第一名则随机选其中一个第一名。\nkube-controller-menager# 负责管理其它scale-controller，运行在主节点，我们创建Deployments、Statefulsets等资源的时候会有一个相应的scale-controller被创建出来，用来监控该资源的副本数是否与预期数量相同。\nkubelet# 做实际部署工作的组件，运行在每个子节点上，原理是所有kubelet不断从apiserver检测已绑定主机但还未部署的Pod，如果这个Pod是绑这定在自己主机上的则将其部署，然后通过REST API将Pod状态更新到apiserver中。除此之外它还要发送心跳给apiserver并汇报自身状态。\nkube-proxy# 与kubelet一起被部署在每个子节点上，负责从api-server中监听service和endpoint资源，并在物理机上通过iptables或ipvs为Pod设置端口转发、负载均衡以及从service到Pod的数据转发。\nkubectl# 一个k8s客户端，可以部署在任何地方，它的运行需要依赖一个配置文件，用来提供apiserver的地址、端口、证书等信息，以下是一个创建Deployments的示例：\nvim spark.yaml\napiVersion: apps/v1beta1 kind: Deployment metadata: name: spark-executor spec: replicas: 10 template: metadata: labels: app: spark-executor spec: containers: - name: spark-executor image: registry.io:5000/leap/spark:latest imagePullPolicy: Always resources: requests: memory: \u0026#34;2G\u0026#34; cpu: \u0026#34;5\u0026#34; limits: memory: \u0026#34;2G\u0026#34; cpu: \u0026#34;5\u0026#34; ports: - containerPort: 8081 name: port-1 env: - name: DEBUG value: \u0026#34;__DEBUG\u0026#34;kubectl -n p48-u26-jiajun2 create spark.yml\n参考# 阅读创建Pod的完整流程文章更有助于理解所有组件的原理和它们之间的协作。 关于# 作者：sycki\n阅读：54\n点赞：3\n创建：2018-07-14\n"},{"id":70,"href":"/kubernetes/kubeedge-code.html","title":"KubeEdge源码分析","section":"容器开发","content":"KubeEdge源码分析# 本文以1.2.1版本为例，项目结构说明：kubeedge。\n边缘端模块# EdgeHub# 模块位置：edge/pkg/edgehub 该模块用于与CloudHub通信，目前支持两种方式与CloudHub建立连接：QUIC协议和WebSocket协议，无论使用哪种方式都会使用TLS加密。\n使用QUIC方式# 关键代码quicclient.go:75：\nfunc (qcc *QuicClient) Init() error { // ... 省略多行 ... client := qclient.NewQuicClient(option, exOpts) // ... 省略多行 ... }上面的NewQuicClient()函数其实是调用第三方包（quic-go）建立多路复用的UDP连接与CloudHub端通信client.go:86：\nfunc DialAddrContext(ctx context.Context, addr string, tlsConf *tls.Config, config *Config,) (Session, error) { // ... 省略多行 ... udpConn, err := net.ListenUDP(\u0026#34;udp\u0026#34;, \u0026amp;net.UDPAddr{IP: net.IPv4zero, Port: 0}) return dialContext(ctx, udpConn, udpAddr, addr, tlsConf, config, true) }使用WebSocket方式# 关键代码websocket.go:75：\nfunc (qcc *QuicClient) Init() error { // ... 省略多行 ... client := \u0026amp;wsclient.Client{Options: option, ExOpts: exOpts} // ... 省略多行 ... }关于# 作者：sycki\n阅读：21\n点赞：0\n创建：2020-03-27\n"},{"id":71,"href":"/kubernetes/kubernetes-quick-deloy-17.html","title":"k8s 快速部署 1.7","section":"容器开发","content":"k8s 快速部署 1.7# 最近（2017.07.30）k8s 又发布了新版本，这个版本中增加了两种持久化存储方案，StorygeOS 与 Local ，为了一探究竟，我部署了一套最新版的 k8s 集群，虽然我在半年前部署过一次，并写过一篇 k8s-v1.5 版本部署的文篇，但那次的经验已经不再适用于新版本，这让我又体验了一次 k8s 的部署过程，这简直是种折磨，我现在新重新整理出来，希望给读者带来帮助。\n环境说明# 这次同样是用的 k8s 官方提供的快速安装工具 kubeadm，这种方式依然被官方视为实验性功能，并不建议用在生产环境之中，先说明一下我的环境。\n服务器三台：node1.docker.com, node2.docker.com, node3.docker.com\n操作系统：CentOS 7.2-1511 64位 Docker 版本：1.12.6\nKubernetes 版本：1.7.0\n部署方式：官方提供的 kubeadm 工具\n准备 Linux# 关于系统，我用的是 CentOS7.2，官方说的是只要 CentOS7 就可以，如果是 Ubuntu 的话，版本要在 Ubuntu 16.04 或以上，或者是 HypriotOS v1.0.1+ 系统，不过我没用过这个系统。。\n安装 Docker# 所有节点都需要做这个步骤，这里要提醒大家一下，如果你按照 Docker 官网给出的方式，通过官方提供的 repo 源安装了最新版的 Docker，最好卸载掉，然后重新安装 1.12.x 版本的 Docker，否则后导致后面初始化 K8s 时失败，我已经在这个环节上浪费掉很多时间了，，k8s 官方说 Docker-1.13.x 也是可以的，但我没有试过，，所以正确的安装姿势是，用 CentosOS 7.2 自带的 repo 源来安装，如果你的系统中有 /etc/yum.repos.d/docker.repo 这个文件的话，请移除，然后执行：\nyum install docker-1.12.6 -y如果你的系统自带的源中没有找到 docker，那可以尝试从 Docker 官网安装，但一定要是 1.12.x 的 Docker 版本，请参考Docker 官方文档。\n安装完成后先不启动。\n关于网络# k8s 在初次启动时会下载很多镜像，不过这只是针对 kubeadm 这种安装方式，它把 k8s 的各个模块全部以镜像的方式运行起来，如果全部用二进制码安装的话，是不需要这些镜像的。。那么下载镜像会有什么问题呢？问题大了，这些镜像是在 google 服务器上的，国内连接不到，这就需要读者自行搭梯子了，一般来讲这是必须的，因为每次使用 kubeadm 创建集群时它都会下载最新版本的镜像，所以提前把镱像下载这种方法并不凑效，搭梯子具体方法这里就不讨论了，我就假设你已经有自己的代理服务器了，它的址是：1.2.3.4:8000，这时需要在你的 ~/.bashrc 中加入三个变量，如下：\necho \u0026#39;export HTTP_PROXY=http://1.2.3.4:8000\u0026#39; \u0026gt;\u0026gt; ~/.bashrc echo \u0026#39;export HTTPS_PROXY=http://1.2.3.4:8000\u0026#39; \u0026gt;\u0026gt; ~/.bashrc echo \u0026#39;export no_proxy=\u0026#34;127.0.0.1,node1.docker.com\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.bashrc source .bashrc使用 ping google.com 测试一下，如果能连接到 google 的话，恭喜你！已经成功一半了，，接下来是为 Docker 设置代理，光设置上面那些对 Docker 不生效的，方法很简单：\nmkdir -p /etc/systemd/system/docker.service.d #先创建一个目录 cat \u0026gt; /etc/systemd/system/docker.service.d/http-proxy.conf \u0026lt;\u0026lt;-EOF [Service] Environment=\u0026#34;HTTP_PROXY=http://10.100.124.236:8118\u0026#34; \u0026#34;HTTPS_PROXY=https://10.100.124.236:8118\u0026#34; \u0026#34;NO_PROXY=127.0.0.1,docker.io\u0026#34; EOF这时就可以启动 Docker 了：\nsystemctl start docker \u0026amp;\u0026amp; docker info安装 kubectl# 执行以下两条命令来安装 kubectl\ncurl -LO https://storage.googleapis.com/kubernetes-release/release/v1.7.0/bin/linux/amd64/kubectl chmod +x ./kubectl \u0026amp;\u0026amp; cp ./kubectl /usr/local/bin/kubectl安装 kubeadm 和其它组件# 这一步没有什么特别要注意的地方，唯一可能失败的原因就是代理没配置好，\ncat \u0026lt;\u0026lt;EOF \u0026gt; /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg EOF setenforce 0 yum install kubelet-1.7.0 kubeadm-1.7.0 kubectl-1.7.0 kubernetes-cni-0.5.1 -y systemctl enable kubelet \u0026amp;\u0026amp; systemctl start kubelet使用 kubeadm 初始化主节点# 这一步是关键，成败就在此一举了，开始之前还是先试一下网络是否 OK ，然后就开始初始化主节点吧：\n[root@blog ~]# kubeadm init [kubeadm] WARNING: kubeadm is in beta, please do not use it for production clusters. [init] Using Kubernetes version: v1.7.0 [init] Using Authorization modes: [Node RBAC] [preflight] Running pre-flight checks [preflight] Starting the kubelet service [certificates] Generated CA certificate and key. [certificates] Generated API server certificate and key. [certificates] API Server serving cert is signed for DNS names [kubeadm-master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 10.138.0.4] [certificates] Generated API server kubelet client certificate and key. [certificates] Generated service account token signing key and public key. [certificates] Generated front-proxy CA certificate and key. [certificates] Generated front-proxy client certificate and key. [certificates] Valid certificates and keys now exist in \u0026#34;/etc/kubernetes/pki\u0026#34; [kubeconfig] Wrote KubeConfig file to disk: \u0026#34;/etc/kubernetes/admin.conf\u0026#34; [kubeconfig] Wrote KubeConfig file to disk: \u0026#34;/etc/kubernetes/kubelet.conf\u0026#34; [kubeconfig] Wrote KubeConfig file to disk: \u0026#34;/etc/kubernetes/controller-manager.conf\u0026#34; [kubeconfig] Wrote KubeConfig file to disk: \u0026#34;/etc/kubernetes/scheduler.conf\u0026#34; [apiclient] Created API client, waiting for the control plane to become ready [apiclient] All control plane components are healthy after 16.502136 seconds [token] Using token: \u0026lt;token\u0026gt; [apiconfig] Created RBAC rules [addons] Applied essential addon: kube-proxy [addons] Applied essential addon: kube-dns Your Kubernetes master has initialized successfully! To start using your cluster, you need to run (as a regular user): mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a pod network to the cluster. Run \u0026#34;kubectl apply -f [podnetwork].yaml\u0026#34; with one of the options listed at: http://kubernetes.io/docs/admin/addons/ You can now join any number of machines by running the following on each node as root: kubeadm join --token c09068.cc1f3e78a73d9de6 1.2.3.4:6443如果看到以上输出就表示成功了，这时先把最后一行复制到其它地方记下来。\n这时很多人是在 [apiclient] Created API client, waiting for the control plane to become ready 这一步卡住了，第一个原因是就上面说到的网络，第二个原因就是 Docker 版本跟 k8s 不兼容，所以最好直接安装 1.12.6 版本。还有如果安装失败需要再次执行 kubeadm init 的话，需要先执行 kubeadm reset 来清理一下环境。\n上面输出中提示需要我们手动做一些操作：\nmkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config还有一条命令，它可以让主节点即作为主节点也作为子节点，也一起执行一下吧：\nkubectl taint nodes --all node-role.kubernetes.io/master-安装 pod 网络# 这一步是为集群搭建一个全局的网络环境，这样所有的 Docker 容器就可以跨物理机相互通信了，k8s 支持很多种全局网络，刚开始我用的是 flannel，拆腾半天还是不行，后来换成 weave 网络很容易就搭建成功了，命令如下：\nkubectl apply -n kube-system -f \u0026#34;https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d \u0026#39;\\n\u0026#39;)\u0026#34;这时需要等一会，因为它需要下载几个相关镜像，通过以下命令来看它是否安装成功：\n[root@blog ~]# kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system etcd-node1.docker.com 1/1 Running 0 1h kube-system kube-apiserver-node1.docker.com 1/1 Running 0 1h kube-system kube-controller-manager-node1.docker.com 1/1 Running 0 1h kube-system kube-dns-2425271678-56l1g 0/3 Pending 0 1h kube-system kube-proxy-s3vfd 1/1 Running 0 1h kube-system kube-scheduler-node1.docker.com 1/1 Running 0 1h kube-system weave-net-pwm3d 2/2 Running 0 38s这时如果看到 weave-net-* 这一行后面是 2/2 就表示创建成功了！如果等了很久还没有成功，可以通过以下命令查看原因：\nkubectl describe -f \u0026#34;https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d \u0026#39;\\n\u0026#39;)\u0026#34;加入其它节点# 这里需要注意的是，先确认其它两台机器，也要主同节点一样，安装好 Docker、kubectl、kubelet、kubeadm 等等，然后要把主节点上的所有镜像同步到其它机器上，可以自己搭一个私有镜像仓库来同步，或者将主节点上的所有镜像通过 docker save 命令导出，然后通过 scp 发通到其它机器，再通过 docker load 命令导入。\n镜像同步完成后，在 node2.docker.com 和 node3.docker.com 主机上执行命令刚才记下来那条命令，就可以将其它机器与主节点组成一个集群了，如下：\nkubeadm join --token c09068.cc1f3e78a73d9de6 1.2.3.4:6443执行完毕后，在主节点上执行以下命令查看集群中的节点：\n[root@blog ~]# kubectl get node NAME STATUS AGE VERSION node1.docker.com Ready 8d v1.7.0 node2.docker.com Ready 8d v1.7.0 node3.docker.com Ready 8d v1.7.0然后查看 k8s 所有模块是否都运行正常：\n[root@blog ~]# kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system etcd-node1.docker.com 1/1 Running 1 8d kube-system kube-apiserver-node1.docker.com 1/1 Running 1 8d kube-system kube-controller-manager-node1.docker.com 1/1 Running 1 8d kube-system kube-dns-2425271678-56l1g 3/3 Running 3 8d kube-system kube-proxy-7s7bg 1/1 Running 0 8d kube-system kube-proxy-s3vfd 1/1 Running 1 8d kube-system kube-proxy-zv4l6 1/1 Running 0 8d kube-system kube-scheduler-node1.docker.com 1/1 Running 1 8d kube-system weave-net-pwm3d 2/2 Running 3 8d kube-system weave-net-tp1dz 2/2 Running 0 8d kube-system weave-net-zz2fm 2/2 Running 0 8d这时应该看到三个 kube-proxy 与三个 weave-net 相关的 POD，并且 READY 那一列是全部在运行状态才对。\n安装 Dashboard# 官方提供的安装方式有点问题，我们先把要用到的 yaml 文件下载下来，对其做一点改动：\nwget https://git.io/kube-dashboard这时会得到一个 kubernetes-dashboard.yaml 文件，我们打开它，在最后的 Service 定义里，将其 9090 端口映射出来：\n[root@blog ~]# vim kubernetes-dashboard.yaml ... --- kind: Service apiVersion: v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-system spec: externalIPs: - \u0026#34;1.2.3.4\u0026#34; type: NodePort ports: - port: 9090 targetPort: 9090 selector: k8s-app: kubernetes-dashboard ...然后安装它：\nkubectl create -f kubernetes-dashboard.yaml然后用浏览器访问：http://1.2.3.4:9090\n到这里 k8s 的部署就全部完成了，接下来你可以定义自己的各种 yaml 文件来创建服务。\n-End-\n关于# 作者：张佳军\n阅读：112\n点赞：4\n创建：2017-07-15\n"},{"id":72,"href":"/kubernetes/kubernetes-quick-deloy.html","title":"k8s 快速部署 1.5","section":"容器开发","content":"k8s 快速部署 1.5# 本篇文章可能已不再适用于最新版本的 k8s，您可以看我的另一篇文章：Kubernetes - 快速部署(1.7)。\nk8s 即 kubernetes，它是一个由谷歌开源的容器管理框架，提供完善的容器管理功能，如：Docker 容器编排，服务发现，状态监视等，据说它融合了谷歌多年的容器运营经验，所以目前为止，在容器管理界它是最成熟的，但它并不容易使用，比起 Docker 自带的容器管理框架 Swarm 要复杂的多，有人称 Kubernetes 的集群部署是地狱级的，这并不夸张。 Kubernetes 在 1.5 版本以后，谷歌简化了它的部署流程，小编看了官网的介绍，只需要在各个机器上执行一两条命令就可以搭建一个 k8s 集群，所以赶紧小试了一把，然而实际上并没那么容易，今天就记录在此。下面我们就来搭建一个 k8s 集群，并安装一个 WEB UI 应用（Dashboard）做为示例。\n环境描述# 三台机器：node1.docker.com, node2.docker.com, node3.docker.com 操作系统：CentOS 7.2 k8s 版本：v1.5.1 好多地方需要用 root 权限，所以笔者在这里直接用 root 用户了 在每台机器上安装 k8s# 这一步的难点在于相关资 源的下载，这些镜像都是在谷歌的服务器上，国内不能下载，这里有几个选择，一是用加速器，二是在网上找一下，看有没有其它人共享下载好的 k8s 与相关镜像，有的话最好，三是购买几台国外的服务器来部署 k8s，当然还有其它方式，相信这难不倒诸位，笔者推荐第一种。\n以下是本文用到的所有镜像，如果你用上述的第二种方式，请提前下载好这些镜像：\n[root@node1 ~]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE docker.io/weaveworks/weave-npc 1.9.2 6d47d7ef52cf 3 days ago 58.23 MB docker.io/weaveworks/weave-kube 1.9.2 c187d4ccbf10 3 days ago 163.2 MB gcr.io/google_containers/kube-proxy-amd64 v1.5.3 932ee3606ada 2 weeks ago 173.5 MB gcr.io/google_containers/kube-scheduler-amd64 v1.5.3 cb0ce9bb60f9 2 weeks ago 54 MB gcr.io/google_containers/kube-controller-manager-amd64 v1.5.3 25304c6f1bb2 2 weeks ago 102.8 MB gcr.io/google_containers/kube-apiserver-amd64 v1.5.3 93d8b30a8f27 2 weeks ago 125.9 MB gcr.io/google_containers/kubernetes-dashboard-amd64 v1.5.1 1180413103fd 7 weeks ago 103.6 MB gcr.io/google_containers/etcd-amd64 3.0.14-kubeadm 856e39ac7be3 3 months ago 174.9 MB gcr.io/google_containers/kubedns-amd64 1.9 26cf1ed9b144 3 months ago 47 MB gcr.io/google_containers/dnsmasq-metrics-amd64 1.0 5271aabced07 4 months ago 14 MB gcr.io/google_containers/kube-dnsmasq-amd64 1.4 3ec65756a89b 5 months ago 5.126 MB gcr.io/google_containers/kube-discovery-amd64 1.0 c5e0c9a457fc 5 months ago 134.2 MB gcr.io/google_containers/exechealthz-amd64 1.2 93a43bfb39bf 5 months ago 8.375 MB gcr.io/google_containers/pause-amd64 3.0 99e59f495ffa 10 months ago 746.9 kB解决了下载问题就开始安装 k8s 各组件了，因为笔者是 CentOS 系统，执行以下命令安装：\ncat \u0026lt;\u0026lt;EOF \u0026gt; /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=http://yum.kubernetes.io/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg EOF setenforce 0 yum install -y docker kubelet kubeadm kubectl kubernetes-cni systemctl enable docker \u0026amp;\u0026amp; systemctl start docker systemctl enable kubelet \u0026amp;\u0026amp; systemctl start kubelet如果是 Ubuntu 或 HypriotOS 系统，执行以下命令安装：\napt-get update \u0026amp;\u0026amp; apt-get install -y apt-transport-https curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/apt/sources.list.d/kubernetes.list deb http://apt.kubernetes.io/ kubernetes-xenial main EOF apt-get update # Install docker if you don\u0026#39;t have it already. apt-get install -y docker.io apt-get install -y kubelet kubeadm kubectl kubernetes-cni初始化主节点# 官网说如果在后面打算用 flannel 做为 pod 网络，那么需要在 kubeadm init 后面加上 \u0026ndash;pod-network-cidr 10.244.0.0/16 这个参数，刚开始笔者就是 flannel 做为 pod 网络的，后来发现网络总是不通，原因还没搞明白，而且好多人也遇到了同样的问题，导致后面安装 WEB UI 应用也会出问题，所有这里就不用加这个参数了。 如果镜像没有提前下载好，这一步也需要连接谷歌服务器，不然会停在 \u0026ldquo;Created API client\u0026rdquo; 这一步。\n[root@node1 ~]# kubeadm init [kubeadm] WARNING: kubeadm is in alpha, please do not use it for production clusters. [preflight] Running pre-flight checks [init] Using Kubernetes version: v1.5.3 [tokens] Generated token: \u0026#34;f69e65.6dffddf74bd6f4a6\u0026#34; [certificates] Generated Certificate Authority key and certificate. [certificates] Generated API Server key and certificate [certificates] Generated Service Account signing keys [certificates] Created keys and certificates in \u0026#34;/etc/kubernetes/pki\u0026#34; [kubeconfig] Wrote KubeConfig file to disk: \u0026#34;/etc/kubernetes/kubelet.conf\u0026#34; [kubeconfig] Wrote KubeConfig file to disk: \u0026#34;/etc/kubernetes/admin.conf\u0026#34; [apiclient] Created API client, waiting for the control plane to become ready [apiclient] All control plane components are healthy after 23.557702 seconds [apiclient] Waiting for at least one node to register and become ready [apiclient] First node is ready after 3.504181 seconds [apiclient] Creating a test deployment [apiclient] Test deployment succeeded [token-discovery] Created the kube-discovery deployment, waiting for it to become ready [token-discovery] kube-discovery is ready after 4.503198 seconds [addons] Created essential addon: kube-proxy [addons] Created essential addon: kube-dns Your Kubernetes master has initialized successfully! You should now deploy a pod network to the cluster. Run \u0026#34;kubectl apply -f [podnetwork].yaml\u0026#34; with one of the options listed at: http://kubernetes.io/docs/admin/addons/ You can now join any number of machines by running the following on each node: kubeadm join --token=f69e65.6dffddf74bd6f4a6 10.100.124.236如果成功了，上面会输出一个口令，用这个口令可以让其它机器加入到这个集群，也就是最后那条命令，先记下来，后面会用到。\n安装 Pod 网络# 官方文档上给出来 6 种网络，因为已经碰过壁，这里就直接选 Weave Net，它的安装方式简单致极：\nkubectl apply -f https://git.io/weave-kube然后来看一下安装状态：\n[root@node1 ~]# kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system dummy-2088944543-fnrc3 1/1 Running 0 2d kube-system etcd-leap236.lenovo.com 1/1 Running 0 2d kube-system kube-apiserver-node1.docker.com 1/1 Running 0 2d kube-system kube-controller-manager-leap236.lenovo.com 1/1 Running 0 2d kube-system kube-discovery-1769846148-17vgk 1/1 Running 0 2d kube-system kube-dns-2924299975-92p6j 4/4 Running 0 2d kube-system kube-proxy-d1mb3 1/1 Running 0 2d kube-system kube-scheduler-node1.docker.com 1/1 Running 0 2d kube-system kubernetes-dashboard-3203831700-1tk4p 1/1 Running 0 1d等到 kube-dns 那一行后面变成 4/4 就算安装成功了。\n将其它机器加入集群# 刚才 kubeadm init 那一步最后输出一行命令，现在复制它，并且在其它两台机器上执行：\n[root@node2 ~]# kubeadm join --token=f69e65.6dffddf74bd6f4a6 10.100.124.236 [kubeadm] WARNING: kubeadm is in alpha, please do not use it for production clusters. [preflight] Running pre-flight checks [preflight] Starting the kubelet service [tokens] Validating provided token [discovery] Created cluster info discovery client, requesting info from \u0026#34;http://10.100.124.236:9898/cluster-info/v1/?token-id=f69e65\u0026#34; [discovery] Cluster info object received, verifying signature using given token [discovery] Cluster info signature and contents are valid, will use API endpoints [https://10.100.124.236:6443] [bootstrap] Trying to connect to endpoint https://10.100.124.236:6443 [bootstrap] Detected server version: v1.5.3 [bootstrap] Successfully established connection with endpoint \u0026#34;https://10.100.124.236:6443\u0026#34; [csr] Created API client to obtain unique certificate for this node, generating keys and certificate signing request [csr] Received signed certificate from the API server: Issuer: CN=kubernetes | Subject: CN=system:node:node2.docker.com | CA: false Not before: 2017-03-02 12:15:00 +0000 UTC Not After: 2018-03-02 12:15:00 +0000 UTC [csr] Generating kubelet configuration [kubeconfig] Wrote KubeConfig file to disk: \u0026#34;/etc/kubernetes/kubelet.conf\u0026#34; Node join complete: * Certificate signing request sent to master and response received. * Kubelet informed of new secure connection details. Run \u0026#39;kubectl get nodes\u0026#39; on the master to see this machine join.然后在 node1 上执查看所有节点：\n[root@node1 ~]# kubectl get nodes NAME STATUS AGE node1.docker.com Ready,master 2d node2.docker.com Ready 2d node3.docker.com Ready 2d安装一个 Dashboard Dashboard 是一个很好的 Kubernetes 集群管理工具，它的安装同样简单：\n[root@node1 ~]# kubectl create -f https://rawgit.com/kubernetes/dashboard/master/src/deploy/kubernetes-dashboard.yaml deployment \u0026#34;kubernetes-dashboard\u0026#34; created service \u0026#34;kubernetes-dashboard\u0026#34; created看一下安装情况：\n[root@node1 ~]# kubectl get pods --all-namespaces kube-system dummy-2088944543-fnrc3 1/1 Running 0 2d kube-system etcd-node1.docker.com 1/1 Running 0 2d kube-system kube-apiserver-node1.docker.com 1/1 Running 0 2d kube-system kube-controller-manager-leap236.lenovo.com 1/1 Running 0 2d kube-system kube-discovery-1769846148-17vgk 1/1 Running 0 2d kube-system kube-dns-2924299975-92p6j 4/4 Running 0 2d kube-system kube-proxy-54h8v 1/1 Running 0 2d kube-system kube-proxy-5gqwb 1/1 Running 0 2d kube-system kube-proxy-d1mb3 1/1 Running 0 2d kube-system kube-scheduler-node1.docker.com 1/1 Running 0 2d kube-system kubernetes-dashboard-3203831700-1tk4p 1/1 Running 0 1d kube-system weave-net-f9xvq 2/2 Running 0 2d kube-system weave-net-px4pt 2/2 Running 0 2d这时它又会下载需要的镜像，需要等一会，如果你看到 kubernetes-dashboard 那一行 一直是 ContainerCreating 状态，那么用以下方式可以查看它的运行日志，以便排错：\n[root@node1 ~]# kubectl describe -n kube-system pods kubernetes-dashboard-3203831700-1tk4p ... QoS Class: BestEffort Tolerations: \u0026lt;none\u0026gt; Events: FirstSeen LastSeen Count From SubObjectPath Type Reason Message --------- -------- ----- ---- ------------- -------- ------ ------- 54s 54s 1 {default-scheduler } Normal Scheduled Successfully assigned leap-n3 to leap238.lenovo.com \u0026lt;invalid\u0026gt; \u0026lt;invalid\u0026gt; 1 {kubelet leap238.lenovo.com} spec.containers{leap3} Normal Started Started container with docker id c8d 804eea4 \u0026lt;invalid\u0026gt; \u0026lt;invalid\u0026gt; 1 {kubelet leap238.lenovo.com} spec.containers{leap2} ...启动 Dashboard# 这里也需要注意一下，官方说启动命令是这样：\nkubectl proxy但这样启动起来以后只允许本地访问，例如：curl localhost:8001/ui ，但也许你需要在其它主机上的浏览器中访问。\n那就需要加一些启参数了，另外，这个命令是阻塞式的，我们需要它在后台去运行，下面是最终命令：\nkubectl proxy --address=\u0026#39;10.100.124.236\u0026#39; --accept-hosts=\u0026#39;.+\u0026#39; \u0026amp;如果不加 –address=’10.100.124.236′ –accept-hosts=’.+’ 这两个参数，在访问的时候会得到一个 Unauthorized 这样的页面。 在访问的时候，记得在 url 后面不能少了 /ui 。\n好，现在可以打开你的浏览器访问了：http://10.100.124.236:8001/ui 。\n问题总结# 笔者在安装过程中其实还遇到了很多问题，但最困难的还是墙的问题，最彻底的办法还是用加速器，先把所需的镜像下载下来以后，再建一个私有仓库，把镜像同步到所有机器上，这样就不需要每个机器都下载镜像，启动时会快很多。\n还有就是在安装 Dashboard 后，明明我把代理就停了，也执行了unset http_proxy https_proxy ftp_proxy no_proxy，但访问时说因为代理问题无法访问，然后我把所有服务都停了（包括：删除 Dashboard 与 pod 网络，kubeadm reset 各个主机），最后代理相关的东西全停了（因为所有东西已经下载完了，不需要代理了），再重新装网络与 Dashboard ，问题就解决了。\n参考资料# 官方文档 weave net 网络 关于# 作者：张佳军\n阅读：58\n点赞：0\n创建：2017-03-05\n"},{"id":73,"href":"/kubernetes/popular-bgp-protocol.html","title":"白话BGP协议","section":"容器开发","content":"白话BGP协议# 什么是BGP# 引用维基百科对BGP的描述:\n边界网关协议（英文：Border Gateway Protocol, BGP）是互联网上一个核心的去中心化自治路由协议。 它通过维护IP路由表或\u0026rsquo;前缀\u0026rsquo;表来实现自治系统（AS）之间的可达性，属于矢量路由协议。\nBGP是一种通信协议、一组规范、一种解决方案，通俗的说，它用来在多个路由器之间共享彼此的路由表信息，代替了人工维护各个路由器上的路由表，这在大的网络拓扑结构中是很有用的。\n为什么用BGP# 我们通过一个例子来看看BGP到底解决了什么问题。\n图中有两个路由器R1和R2，为了方便讨论，我将每个IP写成了1.1、2.2的形式，你也可以把它想象成1.1.1.1和2.2.2.2的形式，其中5.5是一个公网IP，其它均是公司内部规划的私有IP，红色的字母代表路由器上的接口，每个路由器旁边有一张表，表示了它们当前的路由信息，并且这几条路由信息是它们默认生成的。\n当主机2.2访问主机1.2时，报文先到达R2，这时由于R2的路由表中没有1.2的路由信息，所以报文会被转发到默认网关1.1，然后数据包从R2的A接口发出并到达R1，由于R1中有1.2的路由，所以报文可以到过1.2主机。\n当主机1.2访问主机2.2时，由于R1中没有2.2的路由，报文会被送到默认网关5.1，也就是Internet网，结果可想而知，数据包最终会因为TTL耗尽而被丢弃。\n要解决1.2不能访问2.2的问题，我们需要在R1上加一条路由，如下：\n目标 网关 接口 2.2 C 这样当报文到了R1以后，就会被R1从C接口发送到R2，R2再将报文转发给2.2。\n但是在网络拓扑非常复杂的情况下，会有很多的路由器以及成千上万的主机（这里不限于同一个公司内部的路由器和主机，更实际的情况是多个公司、多个网络提供商、多个自治网络(AS)组合起来的复杂网络），靠人工为每台路由器配置和维护路由表就会变得不实现。\n那能不能自动化完成这些事情呢？答案是可以的，例如上面我们为R1增加的路由信息的操作，实际上可以在所有路由器上各自启动一个服务序程序，让它们将自己的路由表通过TCP连接共享给其它所有路由器，其它路由器收到信息后进行分析，将有用的信息添加到自己的路由表中。这样的服务程序就是BGP，它的目标就是解决大型网络中的可达性信息的共享和管理问题。\n实际上，BGP协议被称为最复杂的网络协议之一，实现一个可用的BGP协议需要很多知识，在这里我也只能粗略说一下它的作用和基本原理，让你快速了解到BGP到底是什么，如果需要深入BGP可以查阅相关书籍，如《BGP设计与实现》。\n怎样实现BGP# 我接触过一个使用BGP实现的容器网络通信方案Calico，它把Linux节点当做路由器，在多个节点之间使用BGP协议共享路由信息，这个Calico项目是开源的，这篇Calico的文章在这里：Calico网络\n关于# 作者：sycki\n阅读：139\n点赞：4\n创建：2018-07-21\n"},{"id":74,"href":"/linux/linux-centos7-multi-ip.html","title":"Centos7增加子IP","section":"Linux","content":"Centos7增加子IP# 假设你的 Linux 主机上运行了多了 Docker 容器，而这些容器都需要映射同样的端口号到物理机，以便向外提供服务。这时你可能想到把这些容器的端口映给物理机的不同端口上，如下：\ndocker run \\ -p 8080:8080 \\ -p 8081:8081 \\ -p 8082:8080 \\ ...但这样很容易记不清每个端口对应的服务，本文将介绍一种更优雅的方式解决这个问题。\n基本原理# 通过修改 Linux 配置文件，为 Linux 主机创建多个子 IP ，为每个容器中的端口映射到不同的 IP 上即可。\n实现步骤# 查看当前网卡状态\n[suer@ser0 ~]$ ip a 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: enp8s0f0: \u0026lt;NO-CARRIER,BROADCAST,MULTICAST,UP\u0026gt; mtu 1500 qdisc mq state DOWN qlen 1000 link/ether 70:e2:84:0e:5a:87 brd ff:ff:ff:ff:ff:ff 3: enp10s0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 70:e2:84:0e:5a:86 brd ff:ff:ff:ff:ff:ff inet 10.100.124.231/24 brd 10.100.124.255 scope global enp10s0 valid_lft forever preferred_lft forever 4: enp8s0f1: \u0026lt;NO-CARRIER,BROADCAST,MULTICAST,UP\u0026gt; mtu 1500 qdisc mq state DOWN qlen 1000 link/ether 70:e2:84:0e:5a:88 brd ff:ff:ff:ff:ff:ff我们看到第三个网卡 IP 地址为：10.100.124.231，也就是目前系统的主 IP 地址，它的名字为：enp10s0\n修改网络配置文件，方法很简单，假设我要增加三个子 IP，如下：\n[suer@ser0 ~]$ sudo vi /etc/sysconfig/network-scripts/ifcfg-enp10s0 TYPE=Ethernet BOOTPROTO=none DEFROUTE=yes PEERDNS=yes PEERROUTES=yes IPV4_FAILURE_FATAL=no IPV6INIT=yes IPV6_AUTOCONF=yes IPV6_DEFROUTE=yes IPV6_PEERDNS=yes IPV6_PEERROUTES=yes IPV6_FAILURE_FATAL=no NAME=enp10s0 UUID=32fbf024-507c-4864-842d-36fec758cad7 DEVICE=enp10s0 ONBOOT=yes IPADDR=10.100.124.231 NETMASK=255.255.255.0 GATEWAY=10.100.124.254 DNS1=10.96.1.18 DNS2=10.96.1.19 IPADDR1=10.100.124.111 PREFIX1=24 IPADDR2=10.100.124.112 PREFIX2=24 IPADDR3=10.100.124.113 PREFIX3=24重启网络\nsudo service network restart查看结果\n[suer@ser0 ~]$ ip a 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: enp8s0f0: \u0026lt;NO-CARRIER,BROADCAST,MULTICAST,UP\u0026gt; mtu 1500 qdisc mq state DOWN qlen 1000 link/ether 70:e2:84:0e:5a:87 brd ff:ff:ff:ff:ff:ff 3: enp10s0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 70:e2:84:0e:5a:86 brd ff:ff:ff:ff:ff:ff inet 10.100.124.231/24 brd 10.100.124.255 scope global enp10s0 valid_lft forever preferred_lft forever inet 10.100.124.111/24 brd 10.100.124.255 scope global secondary enp10s0 valid_lft forever preferred_lft forever inet 10.100.124.112/24 brd 10.100.124.255 scope global secondary enp10s0 valid_lft forever preferred_lft forever inet 10.100.124.113/24 brd 10.100.124.255 scope global secondary enp10s0 valid_lft forever preferred_lft forever 4: enp8s0f1: \u0026lt;NO-CARRIER,BROADCAST,MULTICAST,UP\u0026gt; mtu 1500 qdisc mq state DOWN qlen 1000 link/ether 70:e2:84:0e:5a:88 brd ff:ff:ff:ff:ff:ff是的，就这么简单，现在不访试试从其它主机上 ping 一下这些子 IP，理论上只要能 ping 通主 IP，就能 ping 通子 IP。 关于# 作者：张佳军\n阅读：18\n点赞：0\n创建：2017-02-05\n"},{"id":75,"href":"/linux/linux-parallel-progress.html","title":"炫酷的并行进度条","section":"Linux","content":"炫酷的并行进度条# 你知道怎样写一个并行的进度条吗，如果是在WEB页上或是GUI编程中这很简单，如果是用Shell写呢？或者Python？也许哪天你需要写一个这样的程序，也许这时的你正一筹莫展，那么恭喜你看到本文。 我们先来看一个例子： 这是一个普通的软件安装程序，图中的两个箭头指向两个不同的进度条，其中上方是总进度条，右下角那个是一个不停转圈圈的进度条，表示该步骤正在进行中。没错，这是用Python脚本实现的，用Shell也是一样，而且可能比Python还要简单。\nPython实现# 在Python中，线程是假线程，它与C或者Java中的线程不同，为了完美，博主在实现的时候用了多进程，也就是multiprocessing模块。 在主进程中启动两个子进试，分别负责两个进度条（当然也可以是任意多个），总进度条需要一个全局变量，以在不同的步骤中为其赋不同的值，小进度条在每步完成的时候向下移动一行。 关于光标控制，这是一个关键点，因为在任何情况下，屏幕上只有一个光标，且所有输出都会落在光标处，那上例中的总进度条怎么会出现在光标的上方呢？这就需要移动光标了，在Python中，它没有这个能力，只能借助第三方库来实现，这样就为程序的跨平台带来麻烦，而Shell本身提供了这个功能，而且简单到不行，假设我要在屏幕的第三十行第五十列打印一个“hello”，只需这样： echo -e \u0026#34;\\033[30;50Hhello\u0026#34;或者：\nprintf \u0026#34;\\033[%d;%dH%s\u0026#34; 30 50 hello 接下来就是对光标进一步控制，因为在上一步中将光标移到了总进度条的位置，当打印完指定的字符后还需要移动到小进度条的位置，博主在网上搜索良久后终于找到了完美的方法，也是简单到不行：tput sc 与 tput rc 两条命令，前者用于暂时保存光标位置，然后不管将光标称动到哪，后者都能将位置还原，现在我们将上步骤中的打印命令改良一下： tput sc ;printf \u0026#34;\\033[%d;%dH%s\\n\u0026#34; 30 50 hello ;tput rc 最后就是同步问题，因为两个进度条是并行执行的，这意味着两个进程可能会在同一时刻执行打印命令，导致打印在屏幕上的字符可能错乱，所以我们要保证在同一时刻只有一个进程在调用print命令，这就用到了多进程锁，而multiprocessing模块已经提供了这样的锁 综上所述，这里给出一个简单的示例：\n#!/usr/bin/python # -*- coding: utf-8 -*- import os,time from multiprocessing import Process, Manager, Lock # 打印info在终端的指定位置 def writer(info, y = 0, x = 0): with lock: print_cmd = \u0026#34;tput sc \u0026gt;\u0026amp;2 \u0026amp;\u0026amp; echo -e \u0026#39;\\033[%s;%sH%s\u0026#39; \u0026gt;\u0026amp;2 \u0026amp;\u0026amp; tput rc \u0026gt;\u0026amp;2\u0026#34; % (y, x, info) os.system(print_cmd) # 根据pbar_data内的数据打印进度条，bar_length为进度条总长度 def progress_bar(pbar_data, y = 0, x = 0, bar_length = 80): percent = float(bar_length) / 100 while pbar_data[0] \u0026lt; bar_length: time.sleep(0.05) current = int(float(pbar_data[1]) * percent) if pbar_data[0] != current: for i in range(pbar_data[0], current + 1): time.sleep(pbar_data[2]) info = \u0026#34;\\033[1;32;40m Progress: [\u0026#34; + (\u0026#34;=\u0026#34; * (i - 1) + \u0026#34;\u0026gt;\u0026#34;).ljust(bar_length,\u0026#34; \u0026#34;) + \u0026#34;] \u0026#34; + str(int(i / percent)) + \u0026#34;% \\033[0m\u0026#34; writer(info, y) pbar_data[0] = current \u0026#39;\u0026#39;\u0026#39; 创建全局变量，用于将主进程信息传递给子进程 data_a[0]: 已完成进度的百分比 data_a[1]: 目前最新进度的百分比 data_a[2]: 进度条变化速度 \u0026#39;\u0026#39;\u0026#39; # pbar_a的数据 data_a = Manager().list() data_a.append(0) data_a.append(2) data_a.append(0.5) # pbar_b的数据 data_b = Manager().list() data_b.append(0) data_b.append(2) data_b.append(0.5) # 创建进程锁 lock = Lock() # 创建总进度条进程 pbar_a = Process(target = progress_bar, args=(data_a, 10, 0)) pbar_a.daemon = True # 创建其它进度条进程 pbar_b = Process(target = progress_bar, args=(data_b, 12, 0)) pbar_b.daemon = True # 隐藏光标 os.system(\u0026#34;printf \u0026#39;\\33[2J\\33[?25l\\r\u0026#39;\u0026#34;) # 开始其它步骤并打印进度条 pbar_a.start() pbar_b.start() # Step 1 data_a[1] = 10 data_b[1] = 30 time.sleep(5) # Step 2 data_a[1] = 70 data_a[2] = 0.1 time.sleep(5) # Step 3 data_a[1] = 100 data_b[1] = 100 data_b[2] = 0.05 # 等待进度条执行完毕 pbar_a.join() pbar_b.join() # 显示光标 os.system(\u0026#34;printf \u0026#39;\\33[?25h\\n\u0026#39;\u0026#34;)-End-\n关于# 作者：张佳军\n阅读：99\n点赞：0\n创建：2017-05-19\n"},{"id":76,"href":"/linux/linux-pipe-redirect.html","title":"管道与重定向详解","section":"Linux","content":"管道与重定向详解# Unix 哲学# 在 Unix 系统中，任何程序都可以实现三个接口，即：标准输入（stdin）、标准输出（stdout）、标准错误输出（stderr），你应该注意到，这三个东西前面都有标准两个字，是的，正是这种标准，使得 Unix 系统中的所有独立的程序可以相互传递数据而没有任何限制，这种机制给用户带来了极大的方便，这正是 Unix 哲学！\n什么是管道# 在 Unix/Linux 中，有一个常用的符号，叫做管道符，写做：\u0026quot;|\u0026quot;，举个例子：\nls / | grep usrls 和 grep 是两个独立的程序，但是 ls 的输出数据可以传给 grep 继续处理，而管道符\u0026quot;|\u0026ldquo;在中间起到了数据传输的作用，正如其名，它就像一根橡胶水管，这根水管的两端可以连接任意程序，因为这些程序都向外提供了一组一模一样的接口，这样我们就可以将任意数量的不同功能的程序随意组合起来使用。\n什么是重定向# 在上例中，管道符的一端连接到 ls 的标准输出，另一端连接到 grep 的标准输入，那我们能不能将其中一端连接到一个文件呢？这样我们就可以将结果数据保存下来，或者将一个文件输入到某个程序中去。当然可以，这时就用到重定向了，下面以 Bash Shell 为例，解释常用重定向符号的意义及其用法。\n重定向符号# 0\u0026lt; 标准输入重定向，0可省略 1\u0026gt; 标准输出重定向，1可省略 2\u0026gt; 标准错误输出重定向 它们的用法都是左边给定一个程序，右边给定一个文件，\u0026gt; 表示覆盖，\u0026gt;\u0026gt; 表示追加。 输出重定向# 如果我想把一个程序的标准输出追加到 o.txt 文件中，而错误输出覆盖到 e.txt 文件中，可以这样写：\nls \u0026gt;\u0026gt; o.txt 2\u0026gt; e.txt把标准输出与错误输出都写入到 o.txt：\nls \u0026amp;\u0026gt; o.txt或者像下面这样写\nls \u0026gt; o.txt 2\u0026gt;\u0026amp;1这两种写法是等价的，但明显第一种更为简洁。\n但不能写成下面这样：\nls 2\u0026gt;\u0026amp;1 \u0026gt; o.txt它不会像你期望的那样执行，这行命令中有两个重定向操作，所有操作符会被从左到右依次解释，首先错误输出中的数据被重定向到标准输出流中，这时标准输出指向的是终端，然后第二个重定向操作将标准输出重定向到了 o.txt，但这次操作只影响了标准输出，并没有影响错误输出的指向。\n上面的解释有些牵强，因为博主实在不知道该怎样翻译，官方解释如下：\ndirects only the standard output to file dirlist, because the standard error was duplicated from the standard output before the standard output was redirected to dirlist.\n输入重定向# 将 i.txt 输入到 cat:\ncat \u0026lt; i.txt将一段文本重定向到 cat，文本中包含的代码被正常执行：\ncat \u0026lt;\u0026lt; EOF hello The current time: `date` EOF将一段文本重定向到 cat，文本中所有内容均被视为普通字符串：\ncat \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; hello The current time: `date` EOF将一段文本重定向到 cat，文本中所有内容均被视为普通字符串，且忽略所有前导制表符（也就是date前面的空白部分）：\ncat \u0026lt;\u0026lt;-\u0026#39;EOF\u0026#39; hello The current time: `date` EOF还有一种不常见的输入重定向：\ngrep Sep \u0026lt;\u0026lt;\u0026lt; `date`\u0026lt;\u0026lt;\u0026lt; 后面的字符中如果包含代码，会被正常执行，然后再重定向给 grep。\n命名管道# 前面讲的管道\u0026rdquo;|\u0026ldquo;是没有名字的，这将导致它不能在两个独立的进程间传递数据，这时可以创建一个命名管道来解决。\nmkfifo /tmp/fifo ll /tmp/fifo prw-r--r-- 1 root root 0 Sep 21 15:59 /tmp/fifo等待读取数据，如果管道里没有数据，会一直阻塞：\ncat /tmp/fifo另一个进程写入数据：\necho hello \u0026gt; /tmp/fifo这时cat命令会后收到echo命令输出的数据，但当在cat命令后再执行ctrl + c命令时，会导致echo命令退出，如果想随时查看管道中的数据而不影响echo命令，可以使用下面的方式。\n创建文件描述符(File Descriptors)# 我们还可以自定义一个文件描述符9，并将其绑定到一个管道文件：\nmkfifo /tmp/fifo exec 9\u0026lt;\u0026gt;/tmp/fifo然后向管道写入数据：\necho `date` \u0026gt;\u0026amp;9从管道读取数据：\nread -u 9 var echo $var解除绑定：\nexec 9\u0026lt;\u0026amp;- exec 9\u0026gt;\u0026amp;--End-\n关于# 作者：张佳军\n阅读：51\n点赞：3\n创建：2017-09-21\n"},{"id":77,"href":"/linux/linux-read-file-by-line.html","title":"逐行读取文件的几种方法","section":"Linux","content":"逐行读取文件的几种方法# shell编程也是一门学问，虽然功能有限，但要完全掌握它并不容易，甚至比JAVA这样的编程语言更难掌握，因为它考虑了太多的特殊情况，在编写的时候有更多的不确定性。\n有一次同事问我，shell里怎样逐行读取一个文件，以处理它，没想到他会被这样的问题给难住。原来他写了一个while语句（从网上搜的）来逐行处理文件，发现这个语句没有像预期的那样执行，写法如下：\n[user@node1 ~]$ vim test.sh #!/bin/bash index=1 while read line do echo \u0026#34;$((index++)): $line\u0026#34; ssh localhost \u0026#34;echo $line \u0026gt;\u0026gt; b.txt\u0026#34; done \u0026lt; a.txt执行后，只打印出了a.txt中的第一行，当时以为写错了，结果在这个小问题上折腾了很久，再次上网搜索发现原来已很多人也遇到了同样的问题，而且有人已经给出来解释与解决方法。\n分析原因# 首先是跟ssh这个命令有关系，因为把ssh那一个行去掉以后就一切正常了。\n后来看到有网友说为ssh加上-n选项就可以了，所以特地看了下ssh命令的文档，找到了如下描述：\n[user@node1 ~]$ man ssh ... -n Redirects stdin from /dev/null (actually, prevents reading from stdin). This must be used when ssh is run in the background. A com- mon trick is to use this to run X11 programs on a remote machine. For example, ssh -n shadows.cs.hut.fi emacs \u0026amp; will start an emacs on shadows.cs.hut.fi, and the X11 connection will be automatically forwarded over an encrypted channel. The ssh program will be put in the background. (This does not work if ssh needs to ask for a password or passphrase; see also the -f option.) ...看到这里应该明白了，主要是因为ssh会主动从输入流中读取数据。\n现在我们来完整地分析一下这个语句执行的过程，首先a.txt的内容会全部重定向到stdin中等待其它程序读取，这时所有内容中的\\n还是存在的，在调用read命令时会以\\n为结束符，每次读取一行，这时echo命令把这一行内容给打印出来，而到了ssh这一行的时候，ssh命令会主动从stdin中读出所有内容（就像read命令一样），但它不把\\n当做结束符，所以就读取了剩下的所有数据，所以到while第二次循环时，stdin中已经没有数据了，这时跳出循环，执行结束。\n为ssh加上-n选项并不是一个完美的解决方法，因为假如while中还有其它类似的命令呢？又要被坑一次，那用for语句行不行呢？就像下面这样：\n[user@node1 ~]$ vi test.sh #!/bin/bash index=1 for line in `cat a.txt` do echo \u0026#34;$((index++)): $line\u0026#34; ssh -n localhost \u0026#34;cat\u0026#34; done这样还是不行的，因为for line in语句每次读取数据是以系统IFS为结束符的，而IFS包括空格，这意味着如果a.txt中有空格，for语句也就不能逐行读取了。\n几种可行的方法# 如果只是对文件做一些简单的操作，为ssh加上-n选项就是一个很好的解决方法，因为大多数人对while的用法足够了解：\n[user@node1 ~]$ vi test.sh #!/bin/bash index=1 while read line do echo \u0026#34;$((index++)): $line\u0026#34; ssh -n localhost \u0026#34;echo $line \u0026gt;\u0026gt; b.txt\u0026#34; done \u0026lt; a.txt如果要处理的文件不是很大，也可以用sed命令来处理，看起来更简单，如果文件太大，性能当然会有损耗了：\n[user@node1 ~]$ vi test.sh #!/bin/bash count=`cat a.txt | wc -l` for i in `seq 1 $count` do line=`sed -n \u0026#34;${i}p\u0026#34; a.txt` echo $line done还可以head命令加tail命令，如果文件太大，这种方式的性能损耗会更明显，不过几百行的小文件还是看不出损耗的啦：\n[user@node1 ~]$ vi test.sh #!/bin/bash count=`cat a.txt | wc -l` for i in `seq 1 $count` do line=`head -n $i | tail -n 1` echo $line done还可以用强大的awk命令，好像有点大材小用了，=_=!\n[user@node1 ~]$ vi test.sh #!/bin/bash count=`cat a.txt | wc -l` for i in `seq 1 $count` do line=`awk \u0026#34;NR==$i\u0026#34; a.txt` echo $line done当然还有xargs命令啦，性能会比sed和awk强些，不过它后面一般只接一个命令，如果操作比较复杂的话，可能满足不了你：\ncat a.txt | xargs -I _LINE ssh localhost \u0026#34;echo _LINE \u0026gt; b.txt\u0026#34;对上面的方法稍加修改，可以在xargs后面接多个命令，不过依然不太适合复杂的操作：\ncat a.txt | xargs -I _LINE bash -c \u0026#39;line=\u0026#34;_LINE\u0026#34; ; ssh hostname1 \u0026#34;echo $line \u0026gt; b.txt\u0026#34; ; ssh hostname2 \u0026#34;echo $line \u0026gt; b.txt\u0026#34; \u0026#39;好吧，先就写这么多，如果以后发现其它更好的方法再写上来。\n关于# 作者：张佳军\n阅读：60\n点赞：2\n创建：2017-04-09\n"},{"id":78,"href":"/linux/linux-resolve-iptables.html","title":"理解iptables","section":"Linux","content":"理解iptables# iptables是Linux系统中的一个命令，也是Linux系统上最常用的防火墙，我们常见的Linux发行版如：CentOS、Ubuntu等都是用iptables作为默认的防火墙，它的强大之处在于它可以控制所有进出系统的数据包。然而，对于初次接触它的人来说，这是个比较难用的命令，不止一个人问过我怎样写一条阻止某IP的命令，因为用它增加一条防火墙规则可能需要写一条很长的命令，如果你没有了解它的工作原理，是写不出一条完整的iptables命令的。本文将剖析它的工作原理并结合例子让你更容易使用它。\n前提工作# 要了解iptables就先要了解数据包，这里说的数据包指的是在网络中传输的数据包，又名：报文，如果你了解过TCP协议并写过相关的代码，那么这个词应该不会陌生，如果你不了也没关系，我们这就来谈一谈这个数据包的概念。\n其实这个数据包的结构又要用到七层协议的概念，真是一环扣一环，这里我就且当你了解七层协议的概念，如果真的不懂可以先去看看《鸟哥的Linux私房菜》，不过我会尽量讲的通俗一些，希望这不会影响到你阅读下文。\n网络传输# 设有两台电脑A与B，A通过网络发送一个文件给B，这个文件会被切分成一个一个的小数据包，每个数据包大小约 1KB 左右，然后转成二进制码按顺序发出去，在网络中以电讯号的形式传输给B，B接收到以后再将这些二进制码转成数据包，最后组合成一个文件。\n封包表头# 数据包也称为封包，一个完整的封包，除了我们需要发送的内容以外还包含了很多额外信息，也就是封包表头，主要用来标记这个数据包“从哪里来到哪里去”，在上例中，如果AB相距较远的话，中间一般会经过多个路由器和交换机，当然还有网卡网线等。。。当一个封包经过途中每个设备时，这些设备都需要读出这些封包的表头，才能知道这个封包要去哪里，然后再发给对应的设备，下一个设备收到这个封包也是一样，最终到达了B，可以说封包表头与数据本身同等重要。\n封包格式# 实际上，每个数据包外面套了三层封包，每层都是不同的格式，用来给不同的设备读取。如果以TCP作为传输协议，这三层封包分别为：MAC封包、IP封包、TPC封包，用下图来说明它们的关系： 图中只是简单的列出每个封包表头中较关键的信息，其实还有很多其它信息，比如TCP封包表头中的序列号、状态码，每种封包的详细格式，这里就不展开说明了，不然的话就有点跑题了。如果想深入了解的话网上有很多相关资料。\niptables中的表（table）# 终于说到正点了，，iptables的原理正是分析每个封包的表头，然后根据我们设定的防火墙规则作出相应的动作。 在iptables中有表的概念，使用的时候一般用默认的表，需要的话也可以自己建表，在CentOS6.5中默认有四张表：filter、nat、mangle、raw，每张表有不同的作用:\nfilter 表的作用就是防火墙 nat 表是用来作数据转发 mangle 表专门对特定数据包进处理 raw 表也是在特殊情况下用的，可以减少iptabls对性能的影响 前两个表是常用的表，后两个用的很少，而且博主一直没有用过，本文只对前两个表进行介绍。\n表中的链（chains）# 上面的每张表中都有多个链，链可以理解为小表，每个小表用来处理不同的情况，在filter表中，默认有三个小表：\nINPUT 进入本机的封包要经过此链 OUTPUT 从本机发出的封包要经过此链 FORWARD 经过nat表后目标不是本机的封包会经过此链 在nat表中，默认也有三个小表：\nPREROUTING 封包进入本机时要先经过此链 POSTROUTING 封包从本机发送出去之前要经过此链 OUTPUT 经filter表过滤后的封包会经过此链 由此可以看出，数据包在经过这些链时，是有先后顺序的，而且每张表也是有关联的，如果我们只考虑filter表和nat表，那么封包在通过iptables时的顺序大致如下： 上图中需要注意的是，nat表中的PREROUTING链，nat是用来做数据转发的，它可以修改封包表头中的内容，假设我在PREROUTING链中写入一条规则：将目标为本机的封包转发到 137.137.0.200 这个IP上，这样当封包从PREROUTING链出来后，就不走图中的第二步，而是会经上图中的两条黄线，直接被送出本机。\n链中的规则（rule）# 每个小表中可以有多条规则，经过此小表的封包要符合此小表上的每条规则，最后才能通过，以filter表为例：\n[root@blog ~]# iptables -t filter -n -L Chain INPUT (policy DROP) target prot opt source destination REJECT all -- 191.96.249.0/24 0.0.0.0/0 reject-with icmp-port-unreachable ACCEPT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:22 ACCEPT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:80 Chain FORWARD (policy ACCEPT) target prot opt source destination Chain OUTPUT (policy ACCEPT) target prot opt source destination 命令说明：\n-t 指定对哪个表进行操作，如果不指定，默认filter表 -n 不要将IP地址转换为主机名 -L 显示该表中的所有链与链中的规则 上例中的命令列出来filter表中的详细情况，它有三个链：INPUT、FORWARD、OUTPUT，其中INPUT链中有7条规则，INPUT后面的括号中policy DROP表示默认不通过，当有数据包经过INPUT链时，会从第一条（从上往下的顺序）规则开始匹配，其中第一条表示拒绝所有来源为 191.96.249.0 这个网段的封包，当第一条不通过以后就不会再往下走了。\niptables命令# 要理解iptables，那么了解它的各个选项与参数也是很重要的。好了，我们来看一条命令：\n[root@blog ~]# iptables -t filter -I INPUT -m mac --mac-source 6d:f5:27:8c:e2:7d -s 137.137.0.200 -d kxdmmr.com -p tcp --sport 1:65535 --dport 22 -j ACCEPT上面只是一个例子，一般不会写这么长啊，，，只是为了说明各参数的意义：\n-t 对指定表进行操作，如：filter、nat -I INPUT 表示插入到INPUT链，对指定链进行插入操作，本条规则会被插入到INPUT链的最前面，也就是第一条，还有-A表示追加，-D表示删除，等等 -m mac \u0026ndash;mac-source 指定来源MAC地址 -s 指定来源IP地址 -d 指定目标IP地址 -p 指定通信协议，如TCP、UDP、ICMP等， --sport 指定来源端口 --dport 指定目标端口 -j 指定动作，ACCEPT表示同意通过，DROP表示丢弃此封包，REJECT表示拒绝此封包 所以，上面一条命令意义就是，在filter表中的INPUT链中最前面插入一条规则，作用是，让来源MAC地址为 6d:f5:27:8c:e2:7d 的，且来源IP为 137.137.0.200 的，且目标IP为本机的，且以TCP协议连接过来的，且来源端口是任意端口的，且目标端口为22的封包允许通过。 再来看一条数据转发的命令：\n[root@blog ~]# iptables -t nat -A PREROUTING -d 137.137.0.100/32 ! -i lo -p tcp --dport 80 -j DNAT --to-destination 192.168.0.200:8080命令说明：\n-A 将本条规则追加在PREROUTING链后面 -i 指定网卡，本例中指：只匹配从lo这个网卡过来的封包 ! 表示非，本例中指：匹配lo这个网卡之外的所有其它网卡 --dport 匹配目标端口为60030的封包 -j 指定动作，DNAT表示修改此封包的目标IP，后面要跟 –-to-destination 参数。SNAT表示修改来源IP，后面要跟 –-to-source 参数 --to-destination 将封包表头中的目标IP修改为此值 所以，这条命令的意思是，在nat表中的PREROUTING链最后面追加一条规则，作用是，将目标IP为 137.137.0.100 的，且不是从lo这个网卡传进来的，且通信协议为tcp的，且目标端口为80的封包，转发到 192.168.0.200 这个IP的8080端口上。\n通过上面的例子，你应该对它有一个比较完整的概念，并且很容易使用它。iptables还有更多参数可以用，这样看来，iptables貌似可以分析封包表头中的所有信息，让我们可以比较精准的对数据包进行控制。\n-End-\n关于# 作者：张佳军\n阅读：63\n点赞：1\n创建：2017-05-30\n"},{"id":79,"href":"/linux/linux-send-mail.html","title":"发送邮件的两种方式","section":"Linux","content":"发送邮件的两种方式# 很多情况下，我们需要在服务器上向外发送邮件，用来获得任务报告、安全监控等等，到这里有人会说了：这还不简单！我一条命令分分种就可以搞定。 而实际上，还是有点难度的，难在哪？我想一定有人在Linux下发邮件的时候碰到过下面这样的情况 我们先来发一份邮件，Linux下发送邮件很简单：\necho \u0026#34;hello\u0026#34; | mail -s \u0026#34;title\u0026#34; mybox@163.com -a nginx.log这样发出去有两种结果，要么提示你发送失败，要么石沉大海，然后你各种尝试，，各种搜资料，，各种失败，， 说到底，主要是因为国内垃圾邮件太多导致的，在国内很多人利用邮箱向他们所谓的“客户”（那些并没有订阅的人）或其它使用邮箱的人群发送大量的广告，以此牟利，甚至还发往国外，使得各大邮箱服务商不得不想办法来过滤这些垃圾邮件，像上面那封邮件也会被当成是垃圾邮件，然后让我们这些用邮箱干正事的人也跟着受罪，无知的人们哪。。那我们该怎么办？本文将介绍两种可行的方法。\n在Linux下发送邮件，按照发送原理大概分为两类# 一种是把 Linux 本身当做邮件服务器，由它来直接发送给其它发件人，但这种方式有个弊端，假设我要给 abc@163.com 发一份邮件，那么邮件到达 163 服务时，它可能把我们的邮件当成垃圾邮件，导致邮件被拒收，本文不介绍此种方式。 还有一种方式是通过第三方服务器发送，假设我要给 abc@163.com 发一份邮件，我可以先把邮件交给 qq 邮件服务器，qq 服务器再转发给 163 服务器，这样就会降低被拒收的概率，但这种方式需要先通过 qq 邮件服务器的登录认证才行，这也是本文重点介绍的方式。 先引用两句名言# 勿以恶小而为之，勿以善小而不为\n地势坤，君子以厚德载物\n两种可行的方法# 方法一，利用mailx命令# 这个方法相对简单，但只支持smtp协议，以CentOS7.2为例，如果有防火墙，请将25端口加入白名单，sendmail服务可以关闭 先安装mailx\nsudo yum install mailx -y修改配置文件\nsudo echo \u0026#34;set from=${user}@163.com smtp=smtp.163.com smtp-auth-user=$user \\ smtp-auth-password=$passwd smtp-auth=login\u0026#34; \u0026gt;\u0026gt; /etc/mail.rc 发送邮件测试\necho -e \u0026#39;任务序号：0001\u0026#39; | mail -s \u0026#39;任务成功：0001\u0026#39; my_mail@163.com如果发送成功的话，不需要等太久，一般一分种足够了，然后去检查一下自己的邮箱吧，， 如果你的25端口没开，或者因为其它原因导致连不到服务器，它会有提示告诉你连接超时，像下面这样：\ncould not connect: 连接超时 \u0026#34;/home/user/dead.letter\u0026#34; 13/344 . . . message not sent.如果两分种以上还没收到邮件，且没有任何提示，那可能是被服务器拒收了，而且笔者也没有找到mail的日志在哪。\n方法二，利用python脚本# 用puthon脚本的好处是，我们可以自定义很多参数，这些参数是通过邮箱服务器认证的重要因素 这种方式同时支持smtp和smtps协议，且需要启动sendmail服务，如果有防火墙，需放行25、465端口。\n下面给出一个例子\n[user@node1 ~]$ vi send_mail.py #!/usr/bin/env python # -*- coding: utf-8 -*- import sys import smtplib from email.mime.text import MIMEText from email.header import Header from email.mime.multipart import MIMEMultipart mailto_list=[\u0026#34;user_name@163.com\u0026#34;] mail_host=\u0026#34;smtp.163.com\u0026#34; mail_user=\u0026#34;user_name\u0026#34; mail_pass=\u0026#34;passwd\u0026#34; mail_postfix=\u0026#34;163.com\u0026#34; def send_mail_att(to_list,sub,content,att_file): # 下面几个参数是通过认证的关键因素 me=\u0026#34;splash_update_apks\u0026#34;+\u0026#34;\u0026lt;\u0026#34;+mail_user+\u0026#34;@\u0026#34;+mail_postfix+\u0026#34;\u0026gt;\u0026#34; msg = MIMEMultipart() msg[\u0026#39;Subject\u0026#39;] = Header(sub, \u0026#39;utf-8\u0026#39;) msg[\u0026#39;From\u0026#39;] = me msg[\u0026#39;To\u0026#39;] = \u0026#34;;\u0026#34;.join(to_list) # 邮件正文内容 msg.attach(MIMEText(content, \u0026#39;plain\u0026#39;, \u0026#39;utf-8\u0026#39;)) # 构造附件 att1 = MIMEText(open(att_file, \u0026#39;rb\u0026#39;).read(), \u0026#39;base64\u0026#39;, \u0026#39;utf-8\u0026#39;) #att1[\u0026#34;Content-Type\u0026#34;] = \u0026#39;application/octet-stream\u0026#39; att1[\u0026#34;Content-Disposition\u0026#34;] = \u0026#39;attachment; filename=\u0026#34;run.log\u0026#34;\u0026#39; msg.attach(att1) try: server = smtplib.SMTP() server.connect(mail_host) server.login(mail_user,mail_pass) server.sendmail(me, to_list, msg.as_string()) server.close() return True except Exception, e: print str(e) return False if __name__==\u0026#34;__main__\u0026#34;: import time time_for_now = str(time.strftime(ISOTIMEFORMAT, time.localtime())) send_mail_att(mailto_list, sys.argv[1], sys.argv[2], sys.argv[3])发送邮件测试\npython send_mail.py titel context /var/log/log.att同上，如果发送成功，很快就能收到邮件，如果服务器拒收，则立即会输出提示信息 如是迟迟收不到邮件，用以下命令可以查看没有发送出去的邮件，并且可以看到该邮件发送失败的原因\nsendmail -bp-End-\n关于# 作者：张佳军\n阅读：75\n点赞：0\n创建：2017-03-12\n"},{"id":80,"href":"/network/tcp-theory.html","title":"TCP原理","section":"网络编程","content":"TCP原理# 字段说明# 序号（SEQ）# 序号类似于自增ID，用于标识当前数据包在所有数据包中的位置，它的自增规则如下：\n发送一个序号字段不为空的数据包时，该数据包本身占一个序号，故响应包的确认号 = 发送方序号+data_length+1 发送一个序号字段为空的数据包时（如纯ACK响应），该数据包本身不占序号，故响应包的确认号 = 发送方序号+data_length 确认号（ACK）# 用于接收方通知发送方，前面的包已经收到无误，且下次希望收到序号为几的包。\n首部长度字段# 表示TCP头部一共占几个32bit，也就是说，TCP头部的最大长度为 = (2^4-1) * (32/8) = 15 * 4 = 60字节。\n窗口# 用于接收方通知发送方，自己的接受缓存区剩余字节数，最大为2^16-1=65535，当需要更大值时，通常加入窗口缩放因子字段（window scale），例如窗口字段为2048，缩放因子为8，则实际窗口大小为：2048 \u0026laquo; 8。\n标志位# URG 紧急指针（u rgent pointer）有效 。 ACK 确认序号有效。 PSH 接收方应该尽快将这个报文段交给应用层。 RST 重建连接。 SYN 同步序号用来发起一个连接。 FIN 发端完成发送任务。\n连接过程# A\u0026gt;B 随机一个序号A_SEQ：100，（意义：告诉B，A的初始序号） 确认号：空 在控制区设置SYN：1 在选项区添加MSS:1460，（意义：告诉B，A能接受的最大数据区大小，通常为1460：MAC包1514-MAC头14-IP头20-TCP头20）\nA\u0026lt;B 随机一个序号B_SEQ：200，（意义：告诉A，B的初始序号） 确认号：101（A_SEQ+data_length+1），（意义：告诉A，B想要序号为几的包，上一条SYN本身占一个序号） 在控制区设置SYN：1 在控制区设置ACK：1 在选项区添加MSS:1460，（意义：告诉A，B能接受的最大数据区大小，通常为1460：MAC包1514-MAC头14-IP头20-TCP头20）\nA\u0026gt;B（纯ACK响应） 序号A_SEQ：空 确认号：201（B_SEQ+data_length+1），（意义：告诉B，A想要序号为几的包，上一条SYN本身占一个序号） 在控制区设置ACK：1 在选项区添加最大序列大小字段，通常为1460（MAC包1514-IP头20-TCP头20）MSS：1460\n发送数据过程# A\u0026gt;B（发送三个字节abc） 序号A_SEQ：101，（意义：告诉B，这是序号为几的包） 确认号：201（B_SEQ+data_length+1），（意义：告诉B，A想要序号为几的包） 在控制区设置PUS：1 在控制区设置ACK：1 选项区：空\nA\u0026lt;B（纯ACK响应） 序号B_SEQ：空 确认号：101+3+1（A_SEQ+data_length+1），（意义：B想要序号为几的包） 在控制区设置ACK：1 选项区：空\n断开过程# A\u0026gt;B 序号A_SEQ：105，（意义：告诉B，这是序号为几的包） 确认号：201（B_SEQ+data_length+1），（意义：告诉B，A想要序号为几的包） 在控制区设置FIN：1 在控制区设置ACK：1 选项区：空\nA\u0026lt;B（纯ACK响应） 序号B_SEQ：空 确认号：106（A_SEQ+data_length+1），（意义：告诉A下次从第几字节开始） 在控制区设置ACK：1 选项区：空\nA\u0026lt;B 序号B_SEQ：201，（意义：告诉A，这是序号为几的包） 确认号：106（A_SEQ+data_length+1），（意义：告诉A下次从第几字节开始） 在控制区设置FIN：1 在控制区设置ACK：1 选项区：空\nA\u0026gt;B（纯ACK响应） 序号A_SEQ：空 确认号：202（B_SEQ+data_length+1），（意义：告诉B，A想要序号为几的包，上一条FIN本身占一个序号） 在控制区设置ACK：1 选项区：空\nTCP状态变化# 参考# TCP/IP详解-卷1 TCP/IP详解-卷3 关于# 作者：sycki\n阅读：7\n点赞：0\n创建：2020-03-17\n"}]